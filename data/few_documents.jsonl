{"post_id": 39906, "title": "Automate customer interaction using OpenAI Assistants.", "url": "https://www.luminis.eu/blog/automate-customer-interaction-using-openai-assistants/", "updated_at": "2024-03-04T17:46:12", "body": "Almost everybody knows what ChatGPT is. At workshops I give, about 90% of the people have used ChatGPT. Most of them know about the company, but only some know about Assistants. That is a pity; assistants can give you or your users a better experience. \u00a0After reading this blog, you understand what OpenAI Assistants are, how they work and what they can do for you and your users.\n\nDALL E generated coffee bar inspired by the famous Starbucks\nThe use case we use for the demo is a coffee-ordering application. Using the chat application, you talk to the barista, ask for suggestions, and order a nice cup of coffee or something else if you do not like coffee. The demo shows how to work with the different aspects of OpenAI assistants. It shows how to use functions and retrievers. It also shows how to combine it with the hybrid search of Weaviate to find recommended products and verify if the product you want is available in the shop.\nUnderstanding of OpenAI Assistants\nAn assistant is there to help your users interact with a set of tools using natural language. An assistant is configured with instructions and can access an LLM and a set of tools. The provider, OpenAI, provides some of these tools. Other tools are functions that you provide yourself. This might sound abstract. Let\u2019s have a look at an example. One of the provided tools is a code interpreter. The assistant uses this tool to execute generated Python code. Using this tool overcomes one of the well-known problems with doing calculations.\n\nInstructions: You are a personal math tutor. Write and run code to answer math questions.\ntools: code_interpreter\nmodel: gpt-4-turbo-preview\n\nThat is enough to initialise an assistant. You provide access to the assistant using a Thread. Think of a Thread as the chat window. You and the assistant both add messages to the Thread. After adding a message to the Thread, you push the run button to start the interaction with the assistant.\nThe following section introduces the assistant we are creating during this blog post.\nThe coffee-ordering assistant\nI like, or better need, a nice cup of coffee every day, multiple times. I am a black coffee fan. But these hip coffee bars have so many choices. For some people, it is hard to choose the proper coffee. Therefore, we create a coffee assistant that can help you make a choice and assist you during the ordering process.\n\nHave yourself a nice cup of coffee.\nFirst, we give the assistant our instructions.\nYou are a barista in a coffee shop. You help users choose the products the shop has to offer. You have tools available to help you with this task. There are tools to find available products, add products, give suggestions based on ingredients, and finalise the order. You are also allowed to do small talk with the visitors.\nWe provide the assistant with the following tools:\n\nfind_available_products\u200a\u2014\u200aFinds available products based on the given input. The result is an array with valid product names or an empty array if no products are found.\nstart_order\u200a\u2014\u200aStart an order, and the result is ERROR or OK. You can use this to notify the user.\nadd_product_to_order\u200a\u2014\u200aAdd a product to the order. The result is ERROR or OK. You can use this to inform the user\nremove_product_from_order\u200a\u2014\u200aRemove a product from the order. The result is ERROR or OK. You can use this to notify the user\ncheckout_order\u200a\u2014\u200acheck out the order. The result is ERROR or OK. You can use this to notify the user\nsuggest_product\u200a\u2014\u200aSuggests a product based on the input. The result is the name of the product that best matches the input.\n\nThe description of the tool or function is essential. The assistant uses the description to determine what tool to use and when.\nThe video below gives you an impression of what we will build.\n\nThe code\nThe first component for an OpenAI assistant is the Assistant class. I am not rewriting the complete OpenAI documentation here. I do point out the essential parts. The assistant is the component that interacts with the LLM, and it knows the available tools.\nThe assistant can be loaded from OpenAI. No get or load function accepts a name. Therefore, we have a method that loops over the available assistants until it finds the one with the provided name. When creating or loading an assistant, you have to provide the tools_module_name. This is used to locate the tools that the assistant can use. It is essential to keep the tools definitions at the exact location so we can automatically call them. More on this feature when talking about runs.\nWe create the coffee assistant using the code below:\n\r\ndef create_assistant():\r\n  name = \"Coffee Assistant\"\r\n  instructions = (\"You are a barista in a coffee shop. You\"\r\n                  \"help users choose the products the shop\"\r\n                  \"has to offer. You have tools available\"\r\n                  \"to help you with this task. You can\"\r\n                  \"answer questions of visitors, you should\"\r\n                  \"answer with short answers. You can ask\"\r\n                  \"questions to the visitor if you need more\"\r\n                  \"information. more ...\")\r\n\r\n  return Assistant.create_assistant(\r\n      client=client,\r\n      name=name,\r\n      instructions=instructions,\r\n      tools_module_name=\"openai_assistant.coffee.tools\")\r\n\nNotice that we created our own Assistant class, not to confuse it with the OpenAI Assistant class. It is a wrapper for the interactions with the OpenAI assistant class. Below is the method to store function tools in the assistant.\n\r\ndef add_tools_to_assistant(assistant: Assistant):\r\n    assistant.register_functions(\r\n        [\r\n            def_find_available_products, \r\n            def_start_order, \r\n            def_add_product_to_order, \r\n            def_checkout_order,\r\n            def_remove_product_from_order, \r\n            def_suggest_coffee_based_on_description\r\n        ])\r\n\nWe have to create the assistant only once, the next time we can load the assistant to use it for interactions. The next code block shows how to load the assistant.\n\r\ntry:\r\n    assistant = Assistant.load_assistant_by_name(\r\n        client=client, \r\n        name=\"Coffee Assistant\",\r\n        tools_module_name=\"openai_assistant.coffee.tools\")\r\n    logging.info(f\"Tools: {assistant.available_tools}\")\r\nexcept AssistantNotFoundError as exp:\r\n    logging.error(f\"Assistant not found: {exp}\")\r\n    raise exp\r\n\nLook at the complete code for the Assistant class at this location.\nThreads\nThe thread is an interaction between a user and the assistant. Therefore, a Thread object is unique per user. In the application, we use the streamlid session to store the thread_id. Therefore, each new session means a new Thread. The thread is responsible for accepting messages and sending them to the assistant. After a message is sent, a response message is awaited. Each interaction with an assistant is done using a run. The image below presents the flow of the application using these different components.\nOverview of the Assistant flow: First, the user creates a Thread. Next, the user sends a message to the Thread and runs the Thread against the Assistant. The Assistant knows all the available tools and asks the LLM what to do. If a tool needs to be called, the Assistant outputs that request. Our Assistant knows how to call the Tools, but this is the client application. The tool\u2019s output is returned to the LLM, and an answer is generated.\nIt is essential to understand that our Assistant wraps the OpenAI Assistant. Calling the tools is done using our Assistant. Detecting the difference between an output with an answer and an output with the request to call a tool is done using the status of the run. If the status is requires_action, our Assistant finds the tool_calls and calls the tools. This is what happens in the following code block taken from the thread.py.\n\r\ndef __handle_run(self, run: Run) -> Run:\r\n    run = self.__verify_run(run_id=run.id)\r\n\r\n    while run.status == \"requires_action\":\r\n        logger_thread.debug(f\"Run {run.id} requires action\")\r\n        tools_calls = run.required_action.submit_tool_outputs.tool_calls\r\n\r\n        tool_outputs = []\r\n        for tool_call in tools_calls:\r\n            result = self.assistant.call_tool(\r\n                tool_call.function.name, \r\n                json.loads(tool_call.function.arguments))\r\n            logger_thread.debug(f\"Result of call: {result}\")\r\n            tool_outputs.append({\r\n                \"tool_call_id\": tool_call.id,\r\n                \"output\": result\r\n            })\r\n        run = self.client.beta.threads.runs.submit_tool_outputs(\r\n            run_id=run.id,\r\n            thread_id=self.thread_id,\r\n            tool_outputs=tool_outputs\r\n        )\r\n        run = self.__verify_run(run_id=run.id)\r\n\r\n    logger_thread.info(f\"Handle run {run.id} completed.\")\r\n    return run\r\n\r\ndef __verify_run(self, run_id: str):\r\n    \"\"\"\r\n    Verify the status of the run, if it is still in \r\n    progress, wait for a second and try again.\r\n    :param run_id: identifier of the run\r\n    :return: the run\r\n    \"\"\"\r\n    run = self.client.beta.threads.runs.retrieve(\r\n        run_id=run_id, thread_id=self.thread_id)\r\n    logger_thread.debug(f\"Run: {run.id}, status: {run.status}\")\r\n    if run.status not in [\"in_progress\", \"queued\"]:\r\n        return run\r\n    time.sleep(1)\r\n    return self.__verify_run(run_id=run.id)\nNotice how we use the __verify_run function to check the status of the run. If the run is queued or in_progress, we wait for it to finish.\nThe source code for the thread can be found at this location.\nTools\nWe already mentioned the tools that the assistant can use. We have to provide the description of the tool to the Assistant. The following code block shows the specification for one function.\n\r\ndef_suggest_coffee_based_on_description = {\r\n    \"name\": \"suggest_coffee_based_on_description\",\r\n    \"description\": (\"Suggests a product based on the given \"\r\n                    \"ingredients. The result is a valid product \"\r\n                    \"name or an empty string if no products \r\n                    \"are found.\"),\r\n    \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n            \"input\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The coffee to suggest a coffee for\"\r\n            }\r\n        },\r\n        \"required\": [\"input\"]\r\n    }\r\n}\r\n\nIn the code block you see the name, this is important to us. We use the name to call the function. Therefore the name of the function needs to be the same as specified here. The description is really important to the LLM to understand what the tools brings. The parameters are the values provided by the LLM to call the tool with. Again, the description is really important for the LLM to understand what values to provide.\nIn this example we use Weaviate to recommend a drink using the provided text. The next code block shows the implementation.\n\r\ndef suggest_coffee_based_on_description(input: str):\r\n    weaviate = AccessWeaviate(\r\n        url=os.getenv(\"WEAVIATE_URL\"),\r\n        access_key=os.getenv(\"WEAVIATE_ACCESS_KEY\"),\r\n        openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\r\n\r\n    result = weaviate.query_collection(\r\n        question=input, collection_name=\"coffee\")\r\n\r\n    weaviate.close()\r\n\r\n    if len(result.objects) == 0:\r\n        logger_coffee.warning(\"No products found\")\r\n        return \"\"\r\n\r\n    return result.objects[0].properties[\"name\"]\r\n\nConcluding\nThis blog post intends to give you an idea of what it means to work with Assistants. Check the code repository if you want to try it out yourself. The readme file contains the order in which you have to run the different scripts. One is to create the Assistant, one is to load the data into Weaviate, and one is to run the sample application.\nHope you like the post, feel free to comment or get in touch if you have questions.\nReferences\n\nhttps://platform.openai.com/docs/assistants/overview\nhttps://github.com/jettro/ai-assistant\n\n", "tags": ["Assistant", "Generative AI", "OpenAI"], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39930, "title": "Nieuwe lichting IT-toptalenten van Thales, de Belastingdienst en Luminis rondt succesvol Accelerate traject af", "url": "https://www.luminis.eu/blog/nieuwe-lichting-it-toptalenten-van-thales-de-belastingdienst-en-luminis-rondt-succesvol-accelerate-traject-af/", "updated_at": "2024-03-05T14:59:30", "body": "Traditiegetrouw wordt het Accelerate traject afgerond in de hangar op vliegveld Teuge. Tijdens het feestelijke graduation event krijgt een nieuwe lichting deelnemers hun welverdiende Accelerate wings opgespeld.\n\nIn dit nieuwe, versnelde Accelerate traject van tien maanden hebben de Belastingdienst, Thales en Luminis voor de derde keer de handen ineengeslagen. Accelerate is een op maat gemaakt opleidingstraject voor software toptalenten van de drie organisaties. In deze nieuwste versie, Accelerate Craftsmanship, krijgen minder ervaren talenten de kans om een sprong in hun ontwikkeling te maken. Talenten die nog niet direct toe zijn aan een leiderschapspositie maar wel grote stappen in die richting kunnen zetten.\nDe twintig deelnemers en elf coaches ronden dit traject met succes af, en vieren dit op het feestelijke graduation event te midden van collega\u2019s, vrienden en familie.\nTijdens het graduation event krijgen de deelnemers de kans om een afsluitende speech te geven over hun ervaringen tijdens het opleidingstraject. Zo vertelt deelnemer Lennaerd Bakker van de Belastingdienst:\n\u201cIk heb geleerd op zoek te gaan naar datgene wat mij motiveert en waar ik enthousiast van word. Daardoor heb ik een nieuwe passie gevonden, doelen gesteld, doelen behaald, en ben ik positiever. Ik wil iedereen meegeven: Ga op zoek naar hetgeen waar jij blij van wordt, houd dat vast, en laat het niet meer los.\u201d\nNaast soms ontroerende speeches van de deelnemers, krijgt ook Bert Ertman, VP Technology bij Luminis en initiatiefnemer van Accelerate, de kans om wat afsluitende woorden te delen. Hij roept de deelnemers op om buiten hun comfortzone te blijven treden: \u201cLeer groter denken, verbind er een doel aan vanuit je persoonlijke waarden, sorteer je acties op \u2018lef\u2019 en \u2018just do it!\u2019\u201d.\nAccelerate Craftsmanship is het derde traject in een succesvolle Accelerate-reeks. Vanuit een initiatief van Thales en Luminis startte in 2020 het eerste traject om tech-toptalent uit de eigen organisatie klaar te stomen voor een toekomst als softwareleiders. Het centrale thema in dit derde traject is Software Craftsmanship, belicht vanuit zowel technisch inhoudelijk vlak maar vooral in combinatie met persoonlijke ontwikkeling.\nHet persoonlijke ontwikkeltraject wordt in samenwerking met How Company gerealiseerd. How Company is sinds het eerste traject partner van Accerelate, en ziet door middel van hun communicatie- en persoonlijk leiderschapstrainingen hoe de deelnemers veranderen. Jeroen Ogier, trainer bij How Company en nauw betrokken bij Accelerate vertelt:\n\u201cDoordat de deelnemers persoonlijke doelen stellen en daar leren actief mee aan de gang te gaan, worden gedachten en idee\u00ebn echt omgezet in concrete resultaten. Dit leidt tot bijzondere ervaringen, groei en ontwikkeling. Het is in \u00e9\u00e9n woord fantastisch om hierin namens How Company een partner te zijn!\u201d\nDat Accelerate het beoogde doel, IT-toptalent klaarstomen voor de top van het tech-landschap, waarmaakt, beaamt Robert van den Breemen, Teamleider Concern IT-Architecten bij de Belastingdienst:\n\u201cHet succes van het Accelerate programma onderstreept het belang van vakmanschap in de ontwikkeling van talentvolle collega\u2019s. De Belastingdienst kiest ervoor om te investeren in het talent en dit te faciliteren met dit waardevolle ontwikkeltraject. Accelerate gaat niet alleen in op de technische vaardigheden maar ook op de persoonlijke ontwikkeling en het bevorderen van een cultuur van continue persoonlijke en professionele groei. Door het aanbieden van dit traject krijgen de talenten de mogelijkheid om te excelleren en te groeien.\u201d\n\nDe voorgaande trajecten, waarin ook werd samengewerkt met de Belastingdienst en Thales, vormden de basis van deze verkorte Accelerate versie (10 maanden in plaats van 18 maanden red.) waarin de meest relevante onderwerpen en sessies uit eerdere trajecten aan bod kwamen. Of het succes van de eerste twee traject ge\u00ebvenaard kon worden in dit verkorte programma was in het begin wel even spannend, vertelt Henk van Steeg, Head Software Engineering bij Thales:\n\u201cDe vraag of de succesformule van Accelerate ook in tien maanden werkt kan ik ondertussen volmondig met \u2018ja\u2019 beantwoorden. In deze tien maanden hebben we veel collega\u2019s een enorme groei zien doormaken waarbij ze nu meer impact maken dan ze zelf voor mogelijk hielden. Als groeiend bedrijf helpen dergelijke trajecten ons enorm om samen met deze collega\u2019s de uitdagingen aan te kunnen.\u201d\nOok Luminis heeft het eerste Accelerate Craftsmanship als succesvol ervaren. Met elk afgerond programma wordt de succesformule van Accelerate alleen maar beter, en het Craftsmanship-programma is een welkome aanvulling op het Accelerate Leadership-traject, zegt ook Jeroen Bouvrie, Director of Operations bij Luminis en stuurgroeplid van Accelerate:\n\u201cAccelerate Craftsmanship heeft laten zien dat de basis van het Accelerate programma zeer geschikt is om de ontwikkeling van deelnemers die nog minder ver in hun carri\u00e8re zijn te versnellen. Door de inmiddels beproefde mix van human skills en technische skills zie je dat de deelnemers echt gegroeid zijn als mens. Ook wanneer je nog niet toe bent aan een meer leadership-achtige rol, laat deze variant van Accelerate zien dat je grote stappen kan maken in je ontwikkeling.\u201d\nNa tien maanden kijken we terug op een bijzonder waardevol ontwikkeltraject. De afgelopen maanden hebben de deelnemers hard gewerkt aan hun skillset, overwonnen ze samen talloze uitdagingen en behaalden ze persoonlijke doelstellingen. We kijken uit naar de impact die deze groep Accelerate-deelnemers gaat hebben op onze organisaties, nu in en de toekomst.\nMeer weten over het Accelerate-programma of IT-trainingen? Bekijk de website van de Luminis Academy of neem contact op met Louis Pouwels, contactpersoon van de Luminis Academy (academy@luminis.eu).\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 39876, "title": "Tech Talks and Tastings: A Recap of Jfokus 2024", "url": "https://www.luminis.eu/blog/tech-talks-and-tastings-a-recap-of-jfokus-2024/", "updated_at": "2024-03-01T10:27:59", "body": "In this blog post, I tell you all about my experience at the Jfokus Conference 2024 in Stockholm, where my colleague Jettro and I gave a workshop about question-answering systems with Retrieval Augmented Generation.\nBack in October 2023, I wrote my first conference experience blogpost: A Devoxx of Firsts. This felt like a good way for me to look back at the conference and what it brought me. Usually, my gained knowledge and experiences from conferences gradually fade away. Writing it down keeps the memories alive and I can relive them when rereading my own story. So I thought I\u2019d do the same for my visit to Jfokus by writing a recap.\nDay 0: Arrival, Vasa and Burgers\nThe conference started on Monday. But since the workshop we had to give was already at 09:00, Jettro and I decided to travel to Stockholm on Sunday. It was an early wake. We wanted to see the city of Stockholm so we took an early flight (08:30) and landed around 10:30. We couldn\u2019t get into our apartment until 16:00, so I decided to leave my small suitcase in a locker at the central train station. Without having to hassle with my luggage we went on our adventure. We had a nice long walk through the beautiful city center. Eventually, we ended up at the Vasa Museum.\n\nFor our workshops, we always pick a relevant data source to work with. Because we were in Stockholm we wanted something relevant for Stockholm. Jettro visited the city in the summer before. At that time he visited the Vasa Museum where the Vasa Warship is displayed. A ship that sank short after it sailed out around 400 years ago. It was preserved very well, so they were able to get it out of the sea about 350 years later and made it a museum piece. Jettro came up with using the history page of the Vasa Warship for our dataset. I agreed because it was a good fit for a question-answering system. However, I did not see the ship with my own eyes like Jettro did. Luckily the people of Jfokus organised a tour at the Vasa Museum for speakers of the conference on our day of arrival, so we joined them there. This made it possible for me to see the ship from a close distance!\n\u00a0\nAfter the Vasa Museum, it was time to head to our apartment and drop off our bags. We took the advice of a friend and former colleague to go eat at Franky\u2019s Burgers. It turned out to be pretty close to our apartment, so we didn\u2019t have to walk that far. Once there, Jettro and I both ordered the Texas BBQ burger (also on advice). While I was there anyway I made it a double. We definitely made a good choice going there. It tasted so good, I can honestly say that it\u2019s one of the best burgers I ever ate at a restaurant.\nAfter we finished our burger we went back to our apartment and did a last revision of our workshop material to make sure everything was complete and working as it should.\n\u00a0\nDay 1: Workshop, Old Town and Dinner\nMonday was the first conference day. A day full of different workshops. This was also the day we had to give our own workshop: Creating a Semantic Search-Based Question-Answering System with LLMs. The start time was 09:00, so we made sure we arrived at the conference center in time (08:15) and had enough time to register, explore the environment, and set up our equipment. We grabbed some sandwiches at the 7/11 for breakfast and went on to the conference. Turned out they had breakfast there as well. Good thing to know for the next few days. The registering and setting up our equipment all went very smoothly. We even got ourselves a nice Jfokus hoodie. Gotta love merchandise.\n\u00a0\nThe Workshop\n\nPeople started dripping in and it became quite packed. Around 33 people joined the workshop, which I think is a large enough group for two trainers. The theoretical parts of the workshop went well. Jettro and I have developed a good synergy presenting together. Unfortunately, we still had some people struggling with setting up the environment for the practical part. We tried to make it as easy as possible to start, but we forgot to mention in the talk description and prerequisites that we used Java 21. This caused some problems with a few.\n\nEventually, we got all but one (due to laptop restrictions) running so they could start on the exercises. Overall we were satisfied with our performance and the course of the workshop. Judging from the response of the crowd they also thought it was good.\nWe got many positive reactions and also some solid feedback for next time. And most importantly: we all had fun!\n\u00a0\nOld Town\nThe workshops that came after ours were not really of our interest, so we decided to do some more exploring of the city. This time we went to Old Town, Stockholm\u2019s oldest district. I really liked the bright colours of the buildings and the architecture. It\u2019s a beautiful city altogether. When having seen the Old Town walking in cold icy wind, we decided it was time to celebrate the success of our workshop. We still had some time to kill before the speaker dinner started, so we found a nice pub. They had some IPAs on draft with good-sounding names and we couldn\u2019t help to taste a few of them. In our awesome Luminis outfit obviously! After tasting a few beers and bites we went to attend the speaker dinner.\n\n\nThe Speaker Dinner\nThe speaker dinner was nice. It started with a short announcement/speech by someone from Oracle, the sponsor of the dinner. I guess he knew everybody was hungry, so he went over everything very quickly. One thing that stuck with me was that Oracle DB now also supports vector search. Then everybody sat down at the dinner tables and the feast began. One of the attendees from our workshop joined at our table. He was a real fan. Even called it his best workshop ever. Nice compliment! The dinner itself was pretty fancy. The main course was fish. Even though I don\u2019t really like fish, I still tried and with success. It tasted pretty good. The rest of the evening I don\u2019t remember very well because of all the champagne and wine we got. Jettro was smart enough to not consume any alcohol after our pub beers.\nDay 2: Tiring Talks\nThe title of this section might imply the talks were boring, but this was definitely not the case. I consumed one too many alcoholic beverage at the speaker dinner, which made this a tiring day for me. Luckily some interesting talks kept me going throughout the day. This time we ate breakfast at the conference, which was really nice with healthy sandwiches. After that, we went into the main room for the keynotes. Jfokus started as a rave party with flashing lights and music created by Sam Aaron with Sonic Pi, which is a live coding environment to synthesise music. After that, the keynotes started.\n\u00a0\nKeynote 1-2\nThe first keynote was an introduction to the conference by the organisers. The second one was Java in 2024: Constant Change, Delivered by Georges Saab. Basically a background story about the Java release cycle and how they changed it from big releases spanning over years to smaller releases per 6 months. Nothing new if you\u2019re a little up to date with Java.\n\u00a0\nKeynote 3\nAfter that was\u00a0The New Super Power in the Developer\u2019s Toolbox keynote by Lize Raes. If you\u2019ve read my Devoxx blog post, you might recognise her name. There she gave, at least for me, the best talk of the conference. Which was about LangChain4j. She\u2019s a good speaker and also this time she put up a good performance. It was all about AI and how it will help us be more productive in our work and daily lives. We don\u2019t have to fear that AI will take over our jobs, we just have to fear that people using AI will replace us. So now is the time to start learning and using it before it\u2019s too late.\nWhen the keynotes ended, it was time to start the rest of the conference. Below is a brief overview of all the presentations I attended on day 2. The titles are links to the YouTube videos.\n\u00a0\nEnhancing LLM Reasoning and Precision Using RAG and ReAct on AWS by Begum Firdousi Abbas\nMy main topic of interest for the last couple of months has been Retrieval Augmented Generation (RAG). Our own workshop was also about RAG. Luminis is very AWS-oriented, so this talk piqued my interest. Most of the talk felt like an AWS services advertisement. But the ReAct prompting part was new to me. An interesting approach to capturing the reasoning of LLMs, why and how they come to a certain answer. Good food for thought.\n\u00a0\nFrom Serverful to Serverless Java by Dennis Kieselhorst & Maximilian Schellhorn\nIn the same room as the previous AWS talk, so we stayed in our seats for this one. Other presentations during this time slot didn\u2019t appeal much to us. There was the LangChain4j talk by Lize Raes (best-rated talk of Jfokus), but we already attended it at Devoxx. So we decided to stick with this one. It was a decent talk, but I personally didn\u2019t learn much. We have some experienced colleagues who blog and share knowledge about this topic. I might have also missed some things due to my tiredness. Luckily there was a well-needed coffee break after this.\n\u00a0\nAI-powered software development from the trenches by Henrik Kniberg\nI\u2019m not really fond of the phrase \u201cfrom the trenches\u201d, because\u00a0 software development work doesn\u2019t feel like being in the trenches for me. Even though I have already seen a few presentations about AI code assistants on Devoxx, I wanted to see a fresh view on this subject. Henrik did a live demo coding with Cursor, a VS Code fork with a code assistant built in. I sometimes use ChatGPT to help me with my coding problems, but it was nice to see again how a code assistant built in your IDE can really boost productivity. Now onto convincing my employer and clients that I should be able to use this.\n\u00a0\nTechnical Neglect by Kevlin Henney\nAn interesting talk from Kevlin Henney about his view on technical debt. It isn\u2019t necessarily a bad thing if you manage it well. But that last part is where it mostly goes wrong. Watch this if you want to know why \u2018neglect\u2019 is sometimes a better term to use than \u2018debt\u2019.\n\u00a0\nAfter the last talk, we had some food and I decided to go to sleep very early, between 20:00 and 21:00. It was a tiresome day for me, so I needed the rest.\n\u00a0\nDay 3: Refreshing Talks and Departure\nUnlike the day before, I started this day fresh and fit. We had to leave a bit early to catch our flight back, so we only had the chance to attend three presentations. But they were definitely not the least three!\n\u00a0\nFive things every developer should know about software architecture by Simon Brown\nThe week before Jfokus I helped my colleague and friend Robbert-Jan facilitate two Accelerate Craftmanship training sessions about architecture. We did an architectural kata and used Simon\u2019s famous C4 model. Very useful and interesting topic to me, so it was an easy choice for me to attend this talk. I was a bit afraid that this was going to be a C4 model commercial, but it turned out to be a solid talk about architecture in general with only a brief mention of C4. Very good start to the day. I would recommend to watch this.\n\u00a0\nHow hacking works by Espen Sande-Larsen\nFor me, this was the surprise of Jfokus. It\u2019s not the kind of talk I usually go to, but it was in the same room (the main room) as the presentation from Simon. We also had some pretty good spots there, so we decided to stay. Luckily we did. It was a fun and interesting talk with a small introduction to hacking and live examples of hacking an application. Espen showed how easy it sometimes is to hack an application, so it\u2019s important to be aware of vulnerabilities in your code. He suggested regularly doing a Capture the Flag (CTF) exercise with your team (sort of a gamified way to pentest) to see if your code is still secure. Espen is a good presenter and has a solid backstory to support his presentation, which made this the most fun and interesting talk of the conference for me. Recommended to watch!\n\u00a0\nBreaking AI: Live coding and hacking applications with Generative AI by Simon Maple\nAlso in the same room. We were in the hacking mood from the previous talk, so we stayed in our seats again. The title might suggest they hacked applications with Generative AI, but it was actually about exploiting vulnerabilities in the code produced by AI code assistants. It became quite clear that the presenters were from Snyk, but I don\u2019t think they overdid the advertising. It was a nice continuation of the previous presentation where they showed that code produced by code assistants isn\u2019t always the most secure code. So please be aware when using any code assistant that you don\u2019t introduce any vulnerabilities in your code. Tools (like Snyk) can help you to analyse your code to catch vulnerabilities early on.\n\u00a0\nAfter these presentations, it was time to head out to the airport for our flight back. The conference center was next to the central station, where I picked up 800 gr of Swedish candy in a Pressbyr\u00e5n to taste at home.\n\u00a0\nOverall Conference Experience\nOverall a very nice conference, with lots of perks for speakers. The organisers are a group of nice and helpful people, always there to assist you or just have a nice conversation. The location was perfect, in the city center next to the central station of Stockholm. All the rooms were good and well equipped, sound was good everywhere too. The food served looked luxurious and was really good. Water was available throughout the conference center, but I couldn\u2019t find a place to get soda drinks (at Devoxx they had fridges everywhere). Although it\u2019s a bit Sweden focussed, I would recommend going to this conference if you have the chance, especially as a speaker.\nRecommended talks to watch back, in order of my most favourite first:\n\nHow hacking works by Espen Sande-Larsen\nThe New Super Power in the Developer\u2019s Toolbox by Lize Raes\nFive things every developer should know about software architecture by Simon Brown\n\n\u00a0\nKey Takeaways\nWe need to embrace AI. It will not replace us, but the people not using it will be replaced by people who do. It can greatly improve productivity. Just be aware it\u2019s not perfect (yet?), so always verify what it produces does not introduce nasty bugs or security risks. I will start using it wherever possible if allowed by company policies.\nNext to that, I came to the conclusion that I should be more aware of security when writing my code. Seeing someone live coding and easily exploiting the vulnerabilities of that code made me really think. I might start introducing CTFs in teams when possible, which could be a fun way to make everyone aware of security (risks) in their codebase.\nAll Jfokus 2024 presentations can be watched on YouTube in this playlist. The workshops, however, are not recorded.\n", "tags": ["ai", "conference", "java", "jfokus"], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 31377, "title": "Production-Ready CDK \u2013 CDK Pipelines", "url": "https://www.luminis.eu/blog/production-ready-cdk-cdk-pipelines/", "updated_at": "2023-01-27T12:55:14", "body": "We initiated our AWS CDK project in the previous chapter and focused on the project structure. Now, we can leverage CI/CD to speed up the technical value stream. Besides, as the project gets bigger, it becomes more challenging to automate; therefore, why not do it initially?\n\nThe most powerful feature of Cloud Development Kits is abstracting complex cloud applications and, as a result, making the cloud more straightforward. AWS CDK does this also for CI/CD pipelines by offering a module called CDK Pipelines.\nCDK Pipelines save us from writing a lot of code, configuration, and wiring. I have used and tried many CI/CD tools, and CDK Pipelines way is one of the most painless ways of implementing CI/CD. For a more detailed introduction, you can check the documentation. We won\u2019t recap it, but we will focus on real-life scenarios in production-like environments.\nIf you tried CDK Pipelines in the past and did it with the old ways, you might disagree with this statement. I should say: I agree with you. And to clarify, there was another construct that was not the most intuitive in the past. But that one is deprecated. So from now on, we only use this CodePipeline construct from the CDK Pipelines library, which I think works as it should be. The code is compact and opinionated in a beautiful way.\nLet\u2019s start by taking a step back and thinking about the positioning before jumping into the code.\n\u00a0\nDon\u2019t use \u2018cdk deploy\u2019 in your pipelines\n\u201ccdk deploy command is convenient, I always use it for my local CDK code, and I can also use it at my CI/CD pipelines. First, I use the CI/CD tool I want and do the CI part, then just deploy with a single command, easy peasy.\u201d\u200a\u2014\u200aprobably someone who doesn\u2019t care about CI/CD\nWell, you can, but should you?\nI see this in many CI/CD pipelines used for CDK projects, from simple PoCs to production environments in enterprises. cdk deploy command is quick and straightforward, but the intended purpose is fast development, not CI/CD pipelines. Why? Because as your application gets more extensive, you will have more CI/CD steps, more CloudFormation stacks, and cross-account or multi-region deployments. It might seem easy, but it is the dirty way in the long run.\nSecond, when using cdk deploy in a CI/CD pipeline, you must give deployment-related IAM permissions. What happens if your CI/CD tool is compromised? They will have access to your AWS account to deploy stacks or, maybe even if you don\u2019t handle permissions right, destroy existing stacks. It sounds improbable, but it is not the well-architected way. We should be reducing permissions continuously.\nI am not arguing that we shouldn\u2019t be using the command at all, only saying it has a specific purpose which is not secure or complex pipeline scenarios.\n\u00a0\nRestricting to the\u00a0Minimum\nOkay, cdk deploy is out of the equation. So what is the right way?\nWe need to give the least privileges for controlling AWS resources from the CI/CD tooling. One way to do that is by providing only an S3 file upload permission to the IAM role used by the CI/CD tool and creating a deployment pipeline on AWS for the deployment step. We can implement more sophisticated controls in this way, like adding more checks or having manual approvals at different stages.\nFurthermore, since only the S3 file upload action can be compromised, it becomes harder to deploy/destroy resources.\nWe can achieve this in two ways:\n\nUse Git to upload artifacts to S3, then do CI and CD on AWS using CDK Pipelines.\nBuild artifacts with a CI tool, then upload artifacts to S3, finally do CD part on AWS using CDK Pipelines.\n\nLet\u2019s see what the first one looks like:\nWay 1: CDK Pipeline for both Continuous Integration and Continuous Deployment\n\u00a0\nIf you are not starting your software development from scratch for yourself or your company, you should already have at least one CI/CD tooling in use. As a result, you will have CI steps already figured out and implemented before. Good news and this takes us to the second way. We can use the CI tooling and still use CDK Pipelines for CD. This way is the way I use most at my projects:\nWay 2: CDK Pipeline for only Continuous Deployment\n\u00a0\nIntegrating Github and\u00a0AWS\nWe discussed keeping things at a minimum with S3 upload permission. For simplicity, we will skip the CI and go similar to the first way. We will push changes from Git to AWS directly and then deploy them as CloudFormation Stacks.\nLuckily, we can simplify more and skip having an S3 Bucket part. Since we use Github, we can utilize the Github-AWS integration, namely CodeStar Connection. In this way, we use only CodeStar permission to deploy stacks (instead of S3 upload permission). It looks like this at the end:\nWay 3: Git-CDK Pipeline integration using CodeStar Connections\nYou can do it in a minute by using the API for it. Or even easier, you can go to one of the Developer Tooling Services of AWS like CodePipeline, click Settings, click Connections, and finally click Create Connection. Follow the steps and give the permissions you need.\n\n\u00a0\nVoil\u00e0, ready for the CDK Pipeline!\n\u00a0\nAdding CDK Pipeline to the\u00a0Project\nIf the stack from the previous article still exists, you should start over by destroying it using cdk destroy or npx projen destroy as I explained before.\nLet\u2019s start by separating the Lambda Stack from src/main.ts. We create a new file with the path src/lambda-stack.ts:\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\n\r\n// example cdk app stack\r\nexport class LambdaStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    new lambda.Function(this, 'ExampleFunction', {\r\n      functionName: 'example-lambda',\r\n      code: lambda.Code.fromAsset('lambda'),\r\n      handler: 'hello.handler',\r\n      runtime: lambda.Runtime.NODEJS_14_X,\r\n    });\r\n  }\r\n}\r\n\r\n\nThen implement the pipeline at a new file with the path src/cdk-pipeline-stack.ts.\nimport { Stack, StackProps, Stage } from 'aws-cdk-lib';\r\nimport { CodePipeline, CodePipelineSource, ShellStep } from 'aws-cdk-lib/pipelines';\r\nimport { Construct } from 'constructs';\r\nimport { LambdaStack } from './lambda-stack';\r\n\r\n// 3a. We define a Lambda Stage that deploys the Lambda Stack. \r\nexport class LambdaStage extends Stage {\r\n  constructor(scope: Construct, id: string) {\r\n    super(scope, id);\r\n    new LambdaStack(this, 'LambdaStack');\r\n  }\r\n}\r\n\r\nexport class CdkPipelineStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    // 1. We import the CodeStar Connection for Github-CDK Pipeline integration. Therefore, \r\n    // you only need to provide the ARN of the Connection.\r\n    const codePipelineSource = CodePipelineSource.connection('cagingulsen/prod-ready-cdk','main', { \r\n      connectionArn: 'arn:aws:codestar-connections:eu-west-1:YOUR_ACCOUNTI_D:connection/YOUR_CONNECTION_ID'\r\n      },\r\n    );\r\n\r\n    // 2. We define the CDK Pipeline using the source from the first step and \r\n    // use three commands for the synth step. We install dependencies from the yarn.lock file \r\n    // with yarn install --frozen-lockfile command to have deterministic, fast, and repeatable builds. \r\n    // The following two lines, we already know.\r\n    const cdkPipeline = new CodePipeline(this, 'CdkPipeline', {\r\n      pipelineName: 'lambda-stack-cdk-pipeline',\r\n      synth: new ShellStep('Synth', {\r\n        input: codePipelineSource,\r\n        commands: [\r\n          'yarn install --frozen-lockfile',\r\n          'npx projen build',\r\n          'npx projen synth',\r\n        ],\r\n      }),\r\n    });\r\n\r\n    // 3b. Then we add this to the CDK Pipeline as a pipeline stage.\r\n    cdkPipeline.addStage(new LambdaStage(this, 'dev'));\r\n  }\r\n}\r\n\nHere we see three things happening; please check the comments in the code above.\nThen, of course, we also need to change the src/main.ts, because we moved the Lambda Stack to a separate file, and the starting stack of the CDK App is from now on the CDK Pipeline Stack.\nimport { App } from 'aws-cdk-lib';\r\nimport { CdkPipelineStack } from './cdk-pipeline-stack';\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nnew CdkPipelineStack(app, 'CdkPipelineStack', { env: devEnv });\r\n\r\napp.synth();\r\n\r\n\nAnd finally, we need to update the only test by renaming main.test.ts to lambda-stack.test.ts without changing the test. But again, we are testing if our Lambda Stack has exactly one Lambda Function.\nimport * as cdk from 'aws-cdk-lib';\r\nimport { Template } from 'aws-cdk-lib/assertions';\r\nimport { LambdaStack } from '../src/lambda-stack';\r\n\r\ntest('Lambda created', () => {\r\n  const app = new cdk.App();\r\n  const stack = new LambdaStack(app, 'LambdaStack');\r\n  const template = Template.fromStack(stack);\r\n\r\n  template.resourceCountIs('AWS::Lambda::Function', 1);\r\n});\r\n\r\n\nWe need to run cdk deploy or npx projen deploy only once to deploy our stacks. Then, it will deploy the CDK Pipeline, and we can see the pipeline at the CodePipeline service. From now on, for every commit you have on the main branch, the CDK pipeline will pick it up. No more deploy commands. We only push to the main branch to deploy.\nNeat, isn\u2019t it?\n\u00a0\nCDK Pipeline deployed and working\nHere is the code with the CDK Pipeline.\n\u00a0\nOther Cool\u00a0Features\nAs you saw, we only used the most basic way to use CDK Pipelines. We can always configure it more by:\n\nAdding more stacks. We could have a different stack like API Gateway Stack and deploy it in the same pipeline. Or use Lambda Stack again but deploy another version of it with a different configuration.\n\ncdkPipeline.addStage(new LambdaStage(this, 'dev'));\r\ncdkPipeline.addStage(new APIGatewayStage(this, 'dev'));\r\n\nor\ncdkPipeline.addStage(new APIGatewayStage(this, 'dev'));\r\ncdkPipeline.addStage(new APIGatewayStage(this, 'acceptance'));\r\n\n\u00a0\n\nDeploying stacks to multiple regions or accounts:\n\nexport class LambdaStage extends Stage {\r\n  constructor(scope: Construct, id: string, appRegion: string) {\r\n    super(scope, id);\r\n    new LambdaStack(this, 'LambdaStack', {\r\n      env: {\r\n        account: process.env.CDK_DEFAULT_ACCOUNT,\r\n        region: appRegion,\r\n      },\r\n    });\r\n  }}\r\ncdkPipeline.addStage(new LambdaStage(this, 'dev1', 'eu-west-1'));\r\ncdkPipeline.addStage(new LambdaStage(this, 'dev2', 'us-east-1'));\r\n\n\u00a0\n\nAdding other types of stages, like a ShellStep or CodeBuildStep:\n\ndeclare const cdkPipeline: pipelines.CodePipeline; \r\nconst preprod = new APIGatewayStage(this, 'PreProd');\r\ncdkPipeline.addStage(preprod, {   \r\n  post: [     \r\n    new pipelines.ShellStep('Validate Endpoint', {       \r\n      commands: ['curl -Ssf https://my.webservice.com/'],     \r\n    }),\r\n   ],\r\n });\r\n\n\u00a0\n\nRunning pipeline stages in parallel using Waves.\n\ndeclare const cdkPipeline: pipelines.CodePipeline;\r\n  \r\nconst wave = cdkPipeline.addWave('MyWave'); \r\nwave.addStage(new APIGatewayStage(this, 'Stage1')); \r\nwave.addStage(new APIGatewayStage(this, 'Stage2'));\r\n\n\u00a0\n\nAdding manual approvals between pipeline stages:\n\ndeclare const cdkPipeline: pipelines.CodePipeline; \r\nconst preprod = new APIGatewayStage(this, 'PreProd'); \r\nconst prod = new APIGatewayStage(this, 'Prod');\r\ncdkPipeline.addStage(preprod, {   \r\n  post: [     \r\n    new pipelines.ShellStep('Validate Endpoint', {       \r\n      commands: ['curl -Ssf https://my.webservice.com/'],     \r\n    }),\r\n   ],\r\n });\r\ncdkPipeline.addStage(prod, {\r\n   pre: [\r\n     new pipelines.ManualApprovalStep('PromoteToProd'),\r\n   ],\r\n });\r\n\n\u00a0\n\nUsing the (default) self mutation feature. If you add new application stages in the source code or new stacks to LambdaStage, the pipeline will automatically reconfigure itself to deploy those new stages and stacks.\n\n\u00a0\n\n\n\n\nAwesome!\nWe will use some of the features we mentioned here in the following chapters and improve our pipeline.\n\n\nIn this blog, we continued building our project by adding a CI/CD pipeline using the CDK Pipelines module of AWS CDK. The next topics are Bootstrapping and Aspects. We will tackle a few problems we see when we try to use AWS CDK in AWS platforms. See you soon in the next one, cheers!\n\n\n", "tags": ["aws", "aws cdk", "cdk pipelines", "CI/CD", "cloud", "devops"], "categories": ["Blog", "Cloud"]}
{"post_id": 31138, "title": "Production-Ready CDK \u2013 Project Structure", "url": "https://www.luminis.eu/blog/cloud-en/production-ready-cdk-project-structure/", "updated_at": "2023-01-27T12:55:00", "body": "In my last post, I announced I am starting a new blog series on Cloud Development Kits. You can find more about the purpose & plan here.\nIn the first chapter of this series, we will begin building our cdk project hands-on while explaining the tooling and the decisions used.\n\nThe primary tool of this post is Projen! In the CDK community, it is a popular tool these days. But for others, it might be the first time they hear about it.\nProjen is a project configuration management tool. To make it more concrete, what AWS CDK is to AWS is, Projen is to your Git Project. So as they call it: it is a CDK for software projects. We can manage all project configurations from a simple, single Javascript file and synthesize project files such as package.json, .gitignore, .eslintrc.json.\nThe concept sounds familiar. We have all seen and used other utility tools like Cookiecutter or Yeoman before. The main issue with those tools is they are for templating, only for one use. After weeks/months of using those tools, your projects will look very different, and there is nothing to do about it. Whereas with Projen, we can create the project and keep managing and configuring it actively since it is not one-time use.\nWhy is it popular with the CDK community, and how did the project start? Because Mr. Elad Ben-Israel, the leading creator of AWS CDK, started Projen and showcased it\u00a0at the first CDK Day in 2020. Then the project grew quickly and almost became the new standard for the CDK projects. While writing this, I saw that it even became an AWS project.\nMy personal experience and why I prefer it\nI have observed that after people start using AWS CDK, the number of AWS CDK projects usually increases sharply after some time. I once worked in an environment where we had +50 AWS CDK repositories. The configurations, pipelines, versioning were all over the place. We fixed it and made them look similar, but not all projects were developed or maintained at the same rate. As time passed, we had the same issue, and we didn\u2019t have a clever and consistent way of managing our projects.\nI also tried templating engines, mostly Cookiecutter, years ago. But unfortunately, the template becomes obsolete rapidly, and almost always, people are not on the same page regarding the project configuration. Besides that, I tried the Bedrock Pattern but didn\u2019t find it applicable to my projects.\nPlus, it is hard to a correct and consistent project structure. There are so many things to think about. To make it more concrete, here is the list of files/features we usually need from a Typescript project, which is a lot:\n\nThe heart of the project: package.json\nTypescript Compiler configuration: tsconfig.json\nDependencies\nLinter\nUnit testing & coverage\nVersion bumps & changelog\nCI builds\nAutomated releases\nSecurity patches\nLicense\nNpm workflow scripts\n\nLuckily the opinionated projects that come ready with Projen contain months of experience, trial, and error. For me, Projen solved the problems I mentioned and made our configuration management much more straightforward, thanks to these.\nLastly, although I highly recommend it, I should warn you it might sometimes be challenging to fix errors because it is a pretty new tool, and the community is not at its peak yet. So we need a bit of patience, that\u2019s all.\nImplementation\nEnough with the story; let\u2019s start with the implementation. Here are the prerequisites to be able to use AWS CDK and Projen:\n\nAWS Account & IAM User or Role that you can assume\nAWS CLI\nNode.js: recommend version 16; version 14 should also be fine\nIDE of your choice\nGit\n\nI assume you configured all and AWS CLI & git & npm(from Node.js installation) working as expected. So let\u2019s execute the following commands to create the project:\n$ mkdir prod-ready-cdk && cd prod-ready-cdk\r\n$ git init\r\n$ npx projen new awscdk-app-ts\nProjen file\nNow, we have a project ready to be detailed. First, we will be working with the\u00a0.projenrc.js file to configure the project. It should look like this first:\nconst { AwsCdkTypeScriptApp } = require('projen');\r\nconst project = new AwsCdkTypeScriptApp({\r\n  cdkVersion: '1.95.2',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'prod-ready-cdk',\r\n\r\n  // cdkDependencies: undefined,  /* Which AWS CDK modules (those that start with \"@aws-cdk/\") this app uses. */\r\n  // deps: [],                    /* Runtime dependencies of this module. */\r\n  // description: undefined,      /* The description is just a string that helps people understand the purpose of the package. */\r\n  // devDeps: [],                 /* Build dependencies for this module. */\r\n  // packageName: undefined,      /* The \"name\" in package.json. */\r\n  // release: undefined,          /* Add release management to this project. */\r\n});\r\nproject.synth();\nSee, it comes up with AWS CDK v1. We need to change the CDK version to v2 and provide more fields. (Warning: I needed to change the first two lines as well)\nconst { awscdk } = require('projen');\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n  authorAddress: 'kemal.gulsen@luminis.eu',\r\n  authorName: 'Kemal Cagin Gulsen',\r\n  cdkVersion: '2.8.0',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'prod-ready-cdk',\r\n  description: 'A CDK project for my blog posts',\r\n  repositoryUrl: 'https://github.com/cagingulsen/prod-ready-cdk.git',\r\n  keywords: [\r\n    'AWS CDK',\r\n    'projen',\r\n    'Typescript',\r\n    'Deployment',\r\n  ],\r\n\r\n  // cdkDependencies: undefined,  /* Which AWS CDK modules (those that start with \"@aws-cdk/\") this app uses. */\r\n  // deps: [],                    /* Runtime dependencies of this module. */\r\n  // description: undefined,      /* The description is just a string that helps people understand the purpose of the package. */\r\n  // devDeps: [],                 /* Build dependencies for this module. */\r\n  // packageName: undefined,      /* The \"name\" in package.json. */\r\n  // release: undefined,          /* Add release management to this project. */\r\n});\r\nproject.synth();\nFor changes to take effect, we need to rerun Projen:\n$ npx projen\nYou can see the changes in the\u00a0package.json, mainly for the AWS CDK dependencies. With CDK v2, we don\u2019t need to add dependencies per AWS Service,\u00a0\u201caws-cdk-lib\u201d:\u00a0\u201c^2.8.0\u201d\u00a0is all we need.\nFurthermore, you might need to bootstrap AWS CDK again using the\u00a0cdk bootstrap\u00a0command since CDK v2 uses the modern bootstrap stack. This modern way will help us because the modern bootstrap stack is a prerequisite for CDK Pipelines.\nNext, we synthesize the CDK app using the command:\n$ npx projen synth\ninstead of\u00a0cdk synth. But, we see that it doesn\u2019t work since we changed the CDK version. So, let\u2019s update the dependencies and add a Hello World Lambda while on\u00a0src/main.ts.\nTip: instead of using npx projen \u2026\u00a0every time, we can have an alias like\u00a0alias pj=\u201dnpx projen\u201d to make it shorter.\nAWS CDK App: A Hello World Lambda Function\nLet\u2019s use src/main.ts for our Lambda stack for now and refactor it in the next episodes. After I fix the imports and add the lambda function, it looks like this:\nimport { App, Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\nexport class LambdaStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: StackProps = {}) {\r\n    super(scope, id, props);\r\n\r\n    new lambda.Function(this, 'ExampleFunction', {\r\n      functionName: 'example-lambda',\r\n      code: lambda.Code.fromAsset('lambda'),\r\n      handler: 'hello.handler',\r\n      runtime: lambda.Runtime.NODEJS_14_X,\r\n    });\r\n  }\r\n}\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nnew LambdaStack(app, 'lambda-stack-dev', { env: devEnv });\r\n// new LambdaStack(app, 'lambda-stack-prod', { env: prodEnv });\r\n\r\napp.synth();\nAnd of course, add our Lambda source code,\u00a0lambda/hello.js.\nexports.handler = function(event, context) {\r\n  console.log('Hello, Cloudwatch!');\r\n  context.succeed('Hello, World!');\r\n};\nThen finally, after we\u00a0npx projen synth (or shorter, pj synth), we will have the smallest AWS CDK App ready to be deployed. You know the drill; then we do\u00a0npx projen deploy\u00a0and check the lambda created on AWS.\nTesting\nFinal touches, let\u2019s add our first unit test.\nimport * as cdk from 'aws-cdk-lib';\r\nimport { Template } from 'aws-cdk-lib/assertions';\r\nimport { LambdaStack } from '../src/main';\r\n\r\ntest('Lambda created', () => {\r\n  const app = new cdk.App();\r\n  const stack = new LambdaStack(app, 'LambdaStack');\r\n  const template = Template.fromStack(stack);\r\n\r\n  template.resourceCountIs('AWS::Lambda::Function', 1);\r\n});\nTo run tests, we can use the command\u00a0npx projen test.\nWatch mode\nWatch mode is a feature that can be very handy when writing CDK code. AWS CDK Team introduced this mode with AWS CDK v2. Every time we save a file and change the synthesized cdk output, the watch mode calls cdk deploy command. Therefore, our stacks deployed on AWS reflect our code, and we don\u2019t need to use cdk/projen commands every time we deploy. As a result, we save time, and deployments are faster for the development environment.\nTo enable it, we can use\u00a0npx projen watch.\nFor other commands, you can check\u00a0package.json. And for the list of options, you can check the API Reference. However, these days we have a more good-looking option in the Construct Hub.\nGithub Project\nAfter configuring the Projen file and AWS CDK App, we can create a new repository on Github and push the code to the repository. I have mine: cagingulsen/prod-ready-cdk. Then used the following commands:\n$ git remote add origin https://github.com/cagingulsen/prod-ready-cdk.git\r\n$ git push -u origin main\nThen we can commit the latest changes and push them to Github. You can check the code here.\nWe made the introduction for our CDK journey and mainly focused on Projen. But of course, this is not the final version of our cdk project configuration. We will add more settings and use more Projen features in the future to ramp up. And indeed, we will have more CDK Constructs than just a Lambda Function.\nThank you for your time, and see you in the upcoming post on CDK Pipelines. Cheers!\nReferences:\nhttps://github.com/projen/projen\nhttps://youtu.be/SOWMPzXtTCw\nhttps://aws.amazon.com/blogs/developer/increasing-development-speed-with-cdk-watch/\n\u00a0\n", "tags": ["aws", "aws cdk", "cdk", "cloud", "infrastructure as code", "projen"], "categories": ["Blog", "Cloud"]}
{"post_id": 30362, "title": "Silver Linings, No Silver Bullets: 3 Perspectives on Innovating with Cloud", "url": "https://www.luminis.eu/blog/silver-linings-no-silver-bullets-3-perspectives-on-innovating-with-cloud/", "updated_at": "2023-04-20T13:41:38", "body": "We\u2019ve heard the phrase, seen the cynical stickers: \u201cThere is no cloud; it\u2019s just someone else\u2019s computer.\u201d That might have been true 15 years ago, but the thing we call the cloud has evolved significantly since. We live in times of accelerating change; organizations that can leverage the cloud effectively will survive them, even thrive in them. How well prepared are you?\nTaking great advantage of the cloud requires a shift in thinking: technology no longer belongs to the IT department (if it ever did). To become cloud natives, organizations need to learn to streamline their effort around everything the cloud offers.\nLet\u2019s look at this paradigm shift from several perspectives: operations, development, and leadership. Each of these perspectives starts with describing the ineffective way of working. We use the term cloud naive (as in: inexperienced or lacking knowledge). Or as the State of DevOps report classifies it: the way of the low performers. Then, we cross the chasm and look at it from the cloud-native perspective, as used by companies that use cloud technology to accelerate their business continuously.\n\nOperations: embrace speed to reduce risk\nLow performance: cloud naive operations\nIn simpler times, developers delivered complete software packages to operations people, who in turn installed and ran them on the appropriate infrastructure. Preferably not too often and with as little change needed as possible. A similar rule determined a snail\u2019s pace of change for platforms: standardization before innovation. Why? Because every change introduced risk and operations were held responsible for it, often having to clean up the mess when things went wrong.\nThis model led operations departments to push back hard on changes and reduce the speed of change\u2014developers adjusted by increasing release sizes and time between releases.\nThe result: slow change, little cooperation, suffering users.\nElite performance: cloud-native operations\nThe way out of this standoff is well-known by now but not always well understood or effectively implemented: DevOps. Making agile teams responsible for business outcomes forces down the classical barriers between the business, developers, and operations, leading to better customer outcomes. Cloud platforms have evolved by adding layer upon layer of abstraction onto the classic data center, leading to hundreds of managed services that teams can continuously leverage to decrease their time-to-value. Perhaps counterintuitively, this increased rate of change enables teams to manage risk better than ever.\nThink about it: if time-to-value is the most critical metric for success, change size logically also gets smaller. Minor changes are easier to manage in case of failure, and deploying multiple times a day yields teams totally in control of their technological solutions.\nThis cloud-native way of doing operations yields precisely what these teams aimed for in the first place: less risk, more control.\n\nDevelopment: focus on outcomes, not technology\nLow performance: cloud naive development\nBefore the emergence of the cloud, servers had to be installed by hand, as did the software running on them. Installing and testing releases was a time-consuming process, mostly done by other teams. So, developers made sure their sparsely released work contained as many features as possible. Any infrastructural requirements and other dependencies had to be carefully communicated and planned with IT operations, dictating long lead times between innovative ideas and customer value.\nAs a result, developers preferred working with stable, minimally variable environments simulated on their development machines. Their products were massive monoliths often written using a single programming language, running on single application container instances. Development feedback might have been fast; customer feedback certainly wasn\u2019t. Developers were often kept totally in the dark about the impact of their changes in production until nightly calls from frustrated IT operations, who spoke in an entirely different IT dialect.\nAgain: slow change, little cooperation, suffering users.\nElite performance: cloud-native development\nThe step-by-step virtualization and containerization of servers, platforms, and runtime environments have enabled development teams to master what was previously the sole job of operations. And more importantly, the marriage of Dev and Ops has given birth to beautiful things like infrastructure as code, CI/CD, and SRE. Managing complex systems this way has opened up opportunities for creating massively scalable but well-observable systems. The old monoliths have been split into small, modular services or even single functions that typically communicate asynchronously using a mind-boggling number of events.\nNow the customer wins. Changing, replacing, or radically rewriting a single, small module and testing and deploying it in the blink of an eye has become a reality. Experimenting \u2014 seeing if an idea for a new job to be done holds up in production \u2014 has become cheap as chips using the pay-as-you-go managed services as provided by cloud platforms like AWS, Azure, and GCP.\n\nLeadership: get out of the way of innovation\nLow performance: cloud naive leadership\nDuring less volatile times, markets were predictable, transformations were carefully planned and budgeted, and users were okay with incremental changes every so often. Software, and the hardware it ran on, were complex and expensive beasts, mastered only by tech masochists stuffed away in cost centers called IT departments. Nothing got built until it was very meticulously specified in requirements documents. Even then, the delivered result was often wildly different from what the market specialists and innovators defined months or years before, if not already outdated.\nOnce more: slow change, little cooperation, suffering users.\nElite performance: cloud-native leadership\nSoftware has been eating the world for a while now. Organizations need to adapt or accept a slow and painful decline into irrelevance. Users now expect to get what they need before realizing they do: a new Netflix series, a recommended product, or even an improved driving experience. Understanding users is key to market success and requires continuous experimentation and software development mastery. The cloud is a massive enabler: it enables organizations to focus on user needs by handing the management of undifferentiated technology over to the cloud provider.\nBut while technical skill development is essential, it is just one part of the innovation puzzle. Another is culture: leaders need to focus everyone\u2019s attention on the importance of time-to-value and agility as the metrics for success. To create empowered, self-sufficient, and effective teams, people need to come together and shatter the walls previously separating them. Lastly, leaders need to take the long view and realize that short-term risk aversion can be a major innovation blocker.\n\nTo boldly go, together\nTransforming an organization from cloud naiveness into cloud-native isn\u2019t for the faint of heart, but very necessary nonetheless. Luckily, Luminis has traveled this path many times before and is a very effective guide on the path to cloud-native. For example, we helped OHRA, one of the largest Dutch digital insurers, migrate its complete data center to the cloud. Another example is the digital transformation of The Learning Network (TLN) we helped shape and implement.\nGet in touch and let us help you kickstart your cloud-native transformation.\nRelated post:cloudwhitepaperWhite paper Cloud migrationCloud migration: 5 effective steps towards a successful result. Download our free white paper. Do you want to migrate your organisation\u2019s systems to the cloud? A cloud migration provides speed and efficiency, among many other advantages. This white paper is...\n", "tags": ["cloud", "devops", "innovation"], "categories": ["Blog", "Cloud"]}
