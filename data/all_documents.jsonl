{"post_id": 39906, "title": "Automate customer interaction using OpenAI Assistants.", "url": "https://www.luminis.eu/blog/automate-customer-interaction-using-openai-assistants/", "updated_at": "2024-03-04T17:46:12", "body": "Almost everybody knows what ChatGPT is. At workshops I give, about 90% of the people have used ChatGPT. Most of them know about the company, but only some know about Assistants. That is a pity; assistants can give you or your users a better experience. \u00a0After reading this blog, you understand what OpenAI Assistants are, how they work and what they can do for you and your users.\n\nDALL E generated coffee bar inspired by the famous Starbucks\nThe use case we use for the demo is a coffee-ordering application. Using the chat application, you talk to the barista, ask for suggestions, and order a nice cup of coffee or something else if you do not like coffee. The demo shows how to work with the different aspects of OpenAI assistants. It shows how to use functions and retrievers. It also shows how to combine it with the hybrid search of Weaviate to find recommended products and verify if the product you want is available in the shop.\nUnderstanding of OpenAI Assistants\nAn assistant is there to help your users interact with a set of tools using natural language. An assistant is configured with instructions and can access an LLM and a set of tools. The provider, OpenAI, provides some of these tools. Other tools are functions that you provide yourself. This might sound abstract. Let\u2019s have a look at an example. One of the provided tools is a code interpreter. The assistant uses this tool to execute generated Python code. Using this tool overcomes one of the well-known problems with doing calculations.\n\nInstructions: You are a personal math tutor. Write and run code to answer math questions.\ntools: code_interpreter\nmodel: gpt-4-turbo-preview\n\nThat is enough to initialise an assistant. You provide access to the assistant using a Thread. Think of a Thread as the chat window. You and the assistant both add messages to the Thread. After adding a message to the Thread, you push the run button to start the interaction with the assistant.\nThe following section introduces the assistant we are creating during this blog post.\nThe coffee-ordering assistant\nI like, or better need, a nice cup of coffee every day, multiple times. I am a black coffee fan. But these hip coffee bars have so many choices. For some people, it is hard to choose the proper coffee. Therefore, we create a coffee assistant that can help you make a choice and assist you during the ordering process.\n\nHave yourself a nice cup of coffee.\nFirst, we give the assistant our instructions.\nYou are a barista in a coffee shop. You help users choose the products the shop has to offer. You have tools available to help you with this task. There are tools to find available products, add products, give suggestions based on ingredients, and finalise the order. You are also allowed to do small talk with the visitors.\nWe provide the assistant with the following tools:\n\nfind_available_products\u200a\u2014\u200aFinds available products based on the given input. The result is an array with valid product names or an empty array if no products are found.\nstart_order\u200a\u2014\u200aStart an order, and the result is ERROR or OK. You can use this to notify the user.\nadd_product_to_order\u200a\u2014\u200aAdd a product to the order. The result is ERROR or OK. You can use this to inform the user\nremove_product_from_order\u200a\u2014\u200aRemove a product from the order. The result is ERROR or OK. You can use this to notify the user\ncheckout_order\u200a\u2014\u200acheck out the order. The result is ERROR or OK. You can use this to notify the user\nsuggest_product\u200a\u2014\u200aSuggests a product based on the input. The result is the name of the product that best matches the input.\n\nThe description of the tool or function is essential. The assistant uses the description to determine what tool to use and when.\nThe video below gives you an impression of what we will build.\n\nThe code\nThe first component for an OpenAI assistant is the Assistant class. I am not rewriting the complete OpenAI documentation here. I do point out the essential parts. The assistant is the component that interacts with the LLM, and it knows the available tools.\nThe assistant can be loaded from OpenAI. No get or load function accepts a name. Therefore, we have a method that loops over the available assistants until it finds the one with the provided name. When creating or loading an assistant, you have to provide the tools_module_name. This is used to locate the tools that the assistant can use. It is essential to keep the tools definitions at the exact location so we can automatically call them. More on this feature when talking about runs.\nWe create the coffee assistant using the code below:\n\r\ndef create_assistant():\r\n  name = \"Coffee Assistant\"\r\n  instructions = (\"You are a barista in a coffee shop. You\"\r\n                  \"help users choose the products the shop\"\r\n                  \"has to offer. You have tools available\"\r\n                  \"to help you with this task. You can\"\r\n                  \"answer questions of visitors, you should\"\r\n                  \"answer with short answers. You can ask\"\r\n                  \"questions to the visitor if you need more\"\r\n                  \"information. more ...\")\r\n\r\n  return Assistant.create_assistant(\r\n      client=client,\r\n      name=name,\r\n      instructions=instructions,\r\n      tools_module_name=\"openai_assistant.coffee.tools\")\r\n\nNotice that we created our own Assistant class, not to confuse it with the OpenAI Assistant class. It is a wrapper for the interactions with the OpenAI assistant class. Below is the method to store function tools in the assistant.\n\r\ndef add_tools_to_assistant(assistant: Assistant):\r\n    assistant.register_functions(\r\n        [\r\n            def_find_available_products, \r\n            def_start_order, \r\n            def_add_product_to_order, \r\n            def_checkout_order,\r\n            def_remove_product_from_order, \r\n            def_suggest_coffee_based_on_description\r\n        ])\r\n\nWe have to create the assistant only once, the next time we can load the assistant to use it for interactions. The next code block shows how to load the assistant.\n\r\ntry:\r\n    assistant = Assistant.load_assistant_by_name(\r\n        client=client, \r\n        name=\"Coffee Assistant\",\r\n        tools_module_name=\"openai_assistant.coffee.tools\")\r\n    logging.info(f\"Tools: {assistant.available_tools}\")\r\nexcept AssistantNotFoundError as exp:\r\n    logging.error(f\"Assistant not found: {exp}\")\r\n    raise exp\r\n\nLook at the complete code for the Assistant class at this location.\nThreads\nThe thread is an interaction between a user and the assistant. Therefore, a Thread object is unique per user. In the application, we use the streamlid session to store the thread_id. Therefore, each new session means a new Thread. The thread is responsible for accepting messages and sending them to the assistant. After a message is sent, a response message is awaited. Each interaction with an assistant is done using a run. The image below presents the flow of the application using these different components.\nOverview of the Assistant flow: First, the user creates a Thread. Next, the user sends a message to the Thread and runs the Thread against the Assistant. The Assistant knows all the available tools and asks the LLM what to do. If a tool needs to be called, the Assistant outputs that request. Our Assistant knows how to call the Tools, but this is the client application. The tool\u2019s output is returned to the LLM, and an answer is generated.\nIt is essential to understand that our Assistant wraps the OpenAI Assistant. Calling the tools is done using our Assistant. Detecting the difference between an output with an answer and an output with the request to call a tool is done using the status of the run. If the status is requires_action, our Assistant finds the tool_calls and calls the tools. This is what happens in the following code block taken from the thread.py.\n\r\ndef __handle_run(self, run: Run) -> Run:\r\n    run = self.__verify_run(run_id=run.id)\r\n\r\n    while run.status == \"requires_action\":\r\n        logger_thread.debug(f\"Run {run.id} requires action\")\r\n        tools_calls = run.required_action.submit_tool_outputs.tool_calls\r\n\r\n        tool_outputs = []\r\n        for tool_call in tools_calls:\r\n            result = self.assistant.call_tool(\r\n                tool_call.function.name, \r\n                json.loads(tool_call.function.arguments))\r\n            logger_thread.debug(f\"Result of call: {result}\")\r\n            tool_outputs.append({\r\n                \"tool_call_id\": tool_call.id,\r\n                \"output\": result\r\n            })\r\n        run = self.client.beta.threads.runs.submit_tool_outputs(\r\n            run_id=run.id,\r\n            thread_id=self.thread_id,\r\n            tool_outputs=tool_outputs\r\n        )\r\n        run = self.__verify_run(run_id=run.id)\r\n\r\n    logger_thread.info(f\"Handle run {run.id} completed.\")\r\n    return run\r\n\r\ndef __verify_run(self, run_id: str):\r\n    \"\"\"\r\n    Verify the status of the run, if it is still in \r\n    progress, wait for a second and try again.\r\n    :param run_id: identifier of the run\r\n    :return: the run\r\n    \"\"\"\r\n    run = self.client.beta.threads.runs.retrieve(\r\n        run_id=run_id, thread_id=self.thread_id)\r\n    logger_thread.debug(f\"Run: {run.id}, status: {run.status}\")\r\n    if run.status not in [\"in_progress\", \"queued\"]:\r\n        return run\r\n    time.sleep(1)\r\n    return self.__verify_run(run_id=run.id)\nNotice how we use the __verify_run function to check the status of the run. If the run is queued or in_progress, we wait for it to finish.\nThe source code for the thread can be found at this location.\nTools\nWe already mentioned the tools that the assistant can use. We have to provide the description of the tool to the Assistant. The following code block shows the specification for one function.\n\r\ndef_suggest_coffee_based_on_description = {\r\n    \"name\": \"suggest_coffee_based_on_description\",\r\n    \"description\": (\"Suggests a product based on the given \"\r\n                    \"ingredients. The result is a valid product \"\r\n                    \"name or an empty string if no products \r\n                    \"are found.\"),\r\n    \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n            \"input\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The coffee to suggest a coffee for\"\r\n            }\r\n        },\r\n        \"required\": [\"input\"]\r\n    }\r\n}\r\n\nIn the code block you see the name, this is important to us. We use the name to call the function. Therefore the name of the function needs to be the same as specified here. The description is really important to the LLM to understand what the tools brings. The parameters are the values provided by the LLM to call the tool with. Again, the description is really important for the LLM to understand what values to provide.\nIn this example we use Weaviate to recommend a drink using the provided text. The next code block shows the implementation.\n\r\ndef suggest_coffee_based_on_description(input: str):\r\n    weaviate = AccessWeaviate(\r\n        url=os.getenv(\"WEAVIATE_URL\"),\r\n        access_key=os.getenv(\"WEAVIATE_ACCESS_KEY\"),\r\n        openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\r\n\r\n    result = weaviate.query_collection(\r\n        question=input, collection_name=\"coffee\")\r\n\r\n    weaviate.close()\r\n\r\n    if len(result.objects) == 0:\r\n        logger_coffee.warning(\"No products found\")\r\n        return \"\"\r\n\r\n    return result.objects[0].properties[\"name\"]\r\n\nConcluding\nThis blog post intends to give you an idea of what it means to work with Assistants. Check the code repository if you want to try it out yourself. The readme file contains the order in which you have to run the different scripts. One is to create the Assistant, one is to load the data into Weaviate, and one is to run the sample application.\nHope you like the post, feel free to comment or get in touch if you have questions.\nReferences\n\nhttps://platform.openai.com/docs/assistants/overview\nhttps://github.com/jettro/ai-assistant\n\n", "tags": ["Assistant", "Generative AI", "OpenAI"], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39930, "title": "Nieuwe lichting IT-toptalenten van Thales, de Belastingdienst en Luminis rondt succesvol Accelerate traject af", "url": "https://www.luminis.eu/blog/nieuwe-lichting-it-toptalenten-van-thales-de-belastingdienst-en-luminis-rondt-succesvol-accelerate-traject-af/", "updated_at": "2024-03-05T14:59:30", "body": "Traditiegetrouw wordt het Accelerate traject afgerond in de hangar op vliegveld Teuge. Tijdens het feestelijke graduation event krijgt een nieuwe lichting deelnemers hun welverdiende Accelerate wings opgespeld.\n\nIn dit nieuwe, versnelde Accelerate traject van tien maanden hebben de Belastingdienst, Thales en Luminis voor de derde keer de handen ineengeslagen. Accelerate is een op maat gemaakt opleidingstraject voor software toptalenten van de drie organisaties. In deze nieuwste versie, Accelerate Craftsmanship, krijgen minder ervaren talenten de kans om een sprong in hun ontwikkeling te maken. Talenten die nog niet direct toe zijn aan een leiderschapspositie maar wel grote stappen in die richting kunnen zetten.\nDe twintig deelnemers en elf coaches ronden dit traject met succes af, en vieren dit op het feestelijke graduation event te midden van collega\u2019s, vrienden en familie.\nTijdens het graduation event krijgen de deelnemers de kans om een afsluitende speech te geven over hun ervaringen tijdens het opleidingstraject. Zo vertelt deelnemer Lennaerd Bakker van de Belastingdienst:\n\u201cIk heb geleerd op zoek te gaan naar datgene wat mij motiveert en waar ik enthousiast van word. Daardoor heb ik een nieuwe passie gevonden, doelen gesteld, doelen behaald, en ben ik positiever. Ik wil iedereen meegeven: Ga op zoek naar hetgeen waar jij blij van wordt, houd dat vast, en laat het niet meer los.\u201d\nNaast soms ontroerende speeches van de deelnemers, krijgt ook Bert Ertman, VP Technology bij Luminis en initiatiefnemer van Accelerate, de kans om wat afsluitende woorden te delen. Hij roept de deelnemers op om buiten hun comfortzone te blijven treden: \u201cLeer groter denken, verbind er een doel aan vanuit je persoonlijke waarden, sorteer je acties op \u2018lef\u2019 en \u2018just do it!\u2019\u201d.\nAccelerate Craftsmanship is het derde traject in een succesvolle Accelerate-reeks. Vanuit een initiatief van Thales en Luminis startte in 2020 het eerste traject om tech-toptalent uit de eigen organisatie klaar te stomen voor een toekomst als softwareleiders. Het centrale thema in dit derde traject is Software Craftsmanship, belicht vanuit zowel technisch inhoudelijk vlak maar vooral in combinatie met persoonlijke ontwikkeling.\nHet persoonlijke ontwikkeltraject wordt in samenwerking met How Company gerealiseerd. How Company is sinds het eerste traject partner van Accerelate, en ziet door middel van hun communicatie- en persoonlijk leiderschapstrainingen hoe de deelnemers veranderen. Jeroen Ogier, trainer bij How Company en nauw betrokken bij Accelerate vertelt:\n\u201cDoordat de deelnemers persoonlijke doelen stellen en daar leren actief mee aan de gang te gaan, worden gedachten en idee\u00ebn echt omgezet in concrete resultaten. Dit leidt tot bijzondere ervaringen, groei en ontwikkeling. Het is in \u00e9\u00e9n woord fantastisch om hierin namens How Company een partner te zijn!\u201d\nDat Accelerate het beoogde doel, IT-toptalent klaarstomen voor de top van het tech-landschap, waarmaakt, beaamt Robert van den Breemen, Teamleider Concern IT-Architecten bij de Belastingdienst:\n\u201cHet succes van het Accelerate programma onderstreept het belang van vakmanschap in de ontwikkeling van talentvolle collega\u2019s. De Belastingdienst kiest ervoor om te investeren in het talent en dit te faciliteren met dit waardevolle ontwikkeltraject. Accelerate gaat niet alleen in op de technische vaardigheden maar ook op de persoonlijke ontwikkeling en het bevorderen van een cultuur van continue persoonlijke en professionele groei. Door het aanbieden van dit traject krijgen de talenten de mogelijkheid om te excelleren en te groeien.\u201d\n\nDe voorgaande trajecten, waarin ook werd samengewerkt met de Belastingdienst en Thales, vormden de basis van deze verkorte Accelerate versie (10 maanden in plaats van 18 maanden red.) waarin de meest relevante onderwerpen en sessies uit eerdere trajecten aan bod kwamen. Of het succes van de eerste twee traject ge\u00ebvenaard kon worden in dit verkorte programma was in het begin wel even spannend, vertelt Henk van Steeg, Head Software Engineering bij Thales:\n\u201cDe vraag of de succesformule van Accelerate ook in tien maanden werkt kan ik ondertussen volmondig met \u2018ja\u2019 beantwoorden. In deze tien maanden hebben we veel collega\u2019s een enorme groei zien doormaken waarbij ze nu meer impact maken dan ze zelf voor mogelijk hielden. Als groeiend bedrijf helpen dergelijke trajecten ons enorm om samen met deze collega\u2019s de uitdagingen aan te kunnen.\u201d\nOok Luminis heeft het eerste Accelerate Craftsmanship als succesvol ervaren. Met elk afgerond programma wordt de succesformule van Accelerate alleen maar beter, en het Craftsmanship-programma is een welkome aanvulling op het Accelerate Leadership-traject, zegt ook Jeroen Bouvrie, Director of Operations bij Luminis en stuurgroeplid van Accelerate:\n\u201cAccelerate Craftsmanship heeft laten zien dat de basis van het Accelerate programma zeer geschikt is om de ontwikkeling van deelnemers die nog minder ver in hun carri\u00e8re zijn te versnellen. Door de inmiddels beproefde mix van human skills en technische skills zie je dat de deelnemers echt gegroeid zijn als mens. Ook wanneer je nog niet toe bent aan een meer leadership-achtige rol, laat deze variant van Accelerate zien dat je grote stappen kan maken in je ontwikkeling.\u201d\nNa tien maanden kijken we terug op een bijzonder waardevol ontwikkeltraject. De afgelopen maanden hebben de deelnemers hard gewerkt aan hun skillset, overwonnen ze samen talloze uitdagingen en behaalden ze persoonlijke doelstellingen. We kijken uit naar de impact die deze groep Accelerate-deelnemers gaat hebben op onze organisaties, nu in en de toekomst.\nMeer weten over het Accelerate-programma of IT-trainingen? Bekijk de website van de Luminis Academy of neem contact op met Louis Pouwels, contactpersoon van de Luminis Academy (academy@luminis.eu).\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 39876, "title": "Tech Talks and Tastings: A Recap of Jfokus 2024", "url": "https://www.luminis.eu/blog/tech-talks-and-tastings-a-recap-of-jfokus-2024/", "updated_at": "2024-03-01T10:27:59", "body": "In this blog post, I tell you all about my experience at the Jfokus Conference 2024 in Stockholm, where my colleague Jettro and I gave a workshop about question-answering systems with Retrieval Augmented Generation.\nBack in October 2023, I wrote my first conference experience blogpost: A Devoxx of Firsts. This felt like a good way for me to look back at the conference and what it brought me. Usually, my gained knowledge and experiences from conferences gradually fade away. Writing it down keeps the memories alive and I can relive them when rereading my own story. So I thought I\u2019d do the same for my visit to Jfokus by writing a recap.\nDay 0: Arrival, Vasa and Burgers\nThe conference started on Monday. But since the workshop we had to give was already at 09:00, Jettro and I decided to travel to Stockholm on Sunday. It was an early wake. We wanted to see the city of Stockholm so we took an early flight (08:30) and landed around 10:30. We couldn\u2019t get into our apartment until 16:00, so I decided to leave my small suitcase in a locker at the central train station. Without having to hassle with my luggage we went on our adventure. We had a nice long walk through the beautiful city center. Eventually, we ended up at the Vasa Museum.\n\nFor our workshops, we always pick a relevant data source to work with. Because we were in Stockholm we wanted something relevant for Stockholm. Jettro visited the city in the summer before. At that time he visited the Vasa Museum where the Vasa Warship is displayed. A ship that sank short after it sailed out around 400 years ago. It was preserved very well, so they were able to get it out of the sea about 350 years later and made it a museum piece. Jettro came up with using the history page of the Vasa Warship for our dataset. I agreed because it was a good fit for a question-answering system. However, I did not see the ship with my own eyes like Jettro did. Luckily the people of Jfokus organised a tour at the Vasa Museum for speakers of the conference on our day of arrival, so we joined them there. This made it possible for me to see the ship from a close distance!\n\u00a0\nAfter the Vasa Museum, it was time to head to our apartment and drop off our bags. We took the advice of a friend and former colleague to go eat at Franky\u2019s Burgers. It turned out to be pretty close to our apartment, so we didn\u2019t have to walk that far. Once there, Jettro and I both ordered the Texas BBQ burger (also on advice). While I was there anyway I made it a double. We definitely made a good choice going there. It tasted so good, I can honestly say that it\u2019s one of the best burgers I ever ate at a restaurant.\nAfter we finished our burger we went back to our apartment and did a last revision of our workshop material to make sure everything was complete and working as it should.\n\u00a0\nDay 1: Workshop, Old Town and Dinner\nMonday was the first conference day. A day full of different workshops. This was also the day we had to give our own workshop: Creating a Semantic Search-Based Question-Answering System with LLMs. The start time was 09:00, so we made sure we arrived at the conference center in time (08:15) and had enough time to register, explore the environment, and set up our equipment. We grabbed some sandwiches at the 7/11 for breakfast and went on to the conference. Turned out they had breakfast there as well. Good thing to know for the next few days. The registering and setting up our equipment all went very smoothly. We even got ourselves a nice Jfokus hoodie. Gotta love merchandise.\n\u00a0\nThe Workshop\n\nPeople started dripping in and it became quite packed. Around 33 people joined the workshop, which I think is a large enough group for two trainers. The theoretical parts of the workshop went well. Jettro and I have developed a good synergy presenting together. Unfortunately, we still had some people struggling with setting up the environment for the practical part. We tried to make it as easy as possible to start, but we forgot to mention in the talk description and prerequisites that we used Java 21. This caused some problems with a few.\n\nEventually, we got all but one (due to laptop restrictions) running so they could start on the exercises. Overall we were satisfied with our performance and the course of the workshop. Judging from the response of the crowd they also thought it was good.\nWe got many positive reactions and also some solid feedback for next time. And most importantly: we all had fun!\n\u00a0\nOld Town\nThe workshops that came after ours were not really of our interest, so we decided to do some more exploring of the city. This time we went to Old Town, Stockholm\u2019s oldest district. I really liked the bright colours of the buildings and the architecture. It\u2019s a beautiful city altogether. When having seen the Old Town walking in cold icy wind, we decided it was time to celebrate the success of our workshop. We still had some time to kill before the speaker dinner started, so we found a nice pub. They had some IPAs on draft with good-sounding names and we couldn\u2019t help to taste a few of them. In our awesome Luminis outfit obviously! After tasting a few beers and bites we went to attend the speaker dinner.\n\n\nThe Speaker Dinner\nThe speaker dinner was nice. It started with a short announcement/speech by someone from Oracle, the sponsor of the dinner. I guess he knew everybody was hungry, so he went over everything very quickly. One thing that stuck with me was that Oracle DB now also supports vector search. Then everybody sat down at the dinner tables and the feast began. One of the attendees from our workshop joined at our table. He was a real fan. Even called it his best workshop ever. Nice compliment! The dinner itself was pretty fancy. The main course was fish. Even though I don\u2019t really like fish, I still tried and with success. It tasted pretty good. The rest of the evening I don\u2019t remember very well because of all the champagne and wine we got. Jettro was smart enough to not consume any alcohol after our pub beers.\nDay 2: Tiring Talks\nThe title of this section might imply the talks were boring, but this was definitely not the case. I consumed one too many alcoholic beverage at the speaker dinner, which made this a tiring day for me. Luckily some interesting talks kept me going throughout the day. This time we ate breakfast at the conference, which was really nice with healthy sandwiches. After that, we went into the main room for the keynotes. Jfokus started as a rave party with flashing lights and music created by Sam Aaron with Sonic Pi, which is a live coding environment to synthesise music. After that, the keynotes started.\n\u00a0\nKeynote 1-2\nThe first keynote was an introduction to the conference by the organisers. The second one was Java in 2024: Constant Change, Delivered by Georges Saab. Basically a background story about the Java release cycle and how they changed it from big releases spanning over years to smaller releases per 6 months. Nothing new if you\u2019re a little up to date with Java.\n\u00a0\nKeynote 3\nAfter that was\u00a0The New Super Power in the Developer\u2019s Toolbox keynote by Lize Raes. If you\u2019ve read my Devoxx blog post, you might recognise her name. There she gave, at least for me, the best talk of the conference. Which was about LangChain4j. She\u2019s a good speaker and also this time she put up a good performance. It was all about AI and how it will help us be more productive in our work and daily lives. We don\u2019t have to fear that AI will take over our jobs, we just have to fear that people using AI will replace us. So now is the time to start learning and using it before it\u2019s too late.\nWhen the keynotes ended, it was time to start the rest of the conference. Below is a brief overview of all the presentations I attended on day 2. The titles are links to the YouTube videos.\n\u00a0\nEnhancing LLM Reasoning and Precision Using RAG and ReAct on AWS by Begum Firdousi Abbas\nMy main topic of interest for the last couple of months has been Retrieval Augmented Generation (RAG). Our own workshop was also about RAG. Luminis is very AWS-oriented, so this talk piqued my interest. Most of the talk felt like an AWS services advertisement. But the ReAct prompting part was new to me. An interesting approach to capturing the reasoning of LLMs, why and how they come to a certain answer. Good food for thought.\n\u00a0\nFrom Serverful to Serverless Java by Dennis Kieselhorst & Maximilian Schellhorn\nIn the same room as the previous AWS talk, so we stayed in our seats for this one. Other presentations during this time slot didn\u2019t appeal much to us. There was the LangChain4j talk by Lize Raes (best-rated talk of Jfokus), but we already attended it at Devoxx. So we decided to stick with this one. It was a decent talk, but I personally didn\u2019t learn much. We have some experienced colleagues who blog and share knowledge about this topic. I might have also missed some things due to my tiredness. Luckily there was a well-needed coffee break after this.\n\u00a0\nAI-powered software development from the trenches by Henrik Kniberg\nI\u2019m not really fond of the phrase \u201cfrom the trenches\u201d, because\u00a0 software development work doesn\u2019t feel like being in the trenches for me. Even though I have already seen a few presentations about AI code assistants on Devoxx, I wanted to see a fresh view on this subject. Henrik did a live demo coding with Cursor, a VS Code fork with a code assistant built in. I sometimes use ChatGPT to help me with my coding problems, but it was nice to see again how a code assistant built in your IDE can really boost productivity. Now onto convincing my employer and clients that I should be able to use this.\n\u00a0\nTechnical Neglect by Kevlin Henney\nAn interesting talk from Kevlin Henney about his view on technical debt. It isn\u2019t necessarily a bad thing if you manage it well. But that last part is where it mostly goes wrong. Watch this if you want to know why \u2018neglect\u2019 is sometimes a better term to use than \u2018debt\u2019.\n\u00a0\nAfter the last talk, we had some food and I decided to go to sleep very early, between 20:00 and 21:00. It was a tiresome day for me, so I needed the rest.\n\u00a0\nDay 3: Refreshing Talks and Departure\nUnlike the day before, I started this day fresh and fit. We had to leave a bit early to catch our flight back, so we only had the chance to attend three presentations. But they were definitely not the least three!\n\u00a0\nFive things every developer should know about software architecture by Simon Brown\nThe week before Jfokus I helped my colleague and friend Robbert-Jan facilitate two Accelerate Craftmanship training sessions about architecture. We did an architectural kata and used Simon\u2019s famous C4 model. Very useful and interesting topic to me, so it was an easy choice for me to attend this talk. I was a bit afraid that this was going to be a C4 model commercial, but it turned out to be a solid talk about architecture in general with only a brief mention of C4. Very good start to the day. I would recommend to watch this.\n\u00a0\nHow hacking works by Espen Sande-Larsen\nFor me, this was the surprise of Jfokus. It\u2019s not the kind of talk I usually go to, but it was in the same room (the main room) as the presentation from Simon. We also had some pretty good spots there, so we decided to stay. Luckily we did. It was a fun and interesting talk with a small introduction to hacking and live examples of hacking an application. Espen showed how easy it sometimes is to hack an application, so it\u2019s important to be aware of vulnerabilities in your code. He suggested regularly doing a Capture the Flag (CTF) exercise with your team (sort of a gamified way to pentest) to see if your code is still secure. Espen is a good presenter and has a solid backstory to support his presentation, which made this the most fun and interesting talk of the conference for me. Recommended to watch!\n\u00a0\nBreaking AI: Live coding and hacking applications with Generative AI by Simon Maple\nAlso in the same room. We were in the hacking mood from the previous talk, so we stayed in our seats again. The title might suggest they hacked applications with Generative AI, but it was actually about exploiting vulnerabilities in the code produced by AI code assistants. It became quite clear that the presenters were from Snyk, but I don\u2019t think they overdid the advertising. It was a nice continuation of the previous presentation where they showed that code produced by code assistants isn\u2019t always the most secure code. So please be aware when using any code assistant that you don\u2019t introduce any vulnerabilities in your code. Tools (like Snyk) can help you to analyse your code to catch vulnerabilities early on.\n\u00a0\nAfter these presentations, it was time to head out to the airport for our flight back. The conference center was next to the central station, where I picked up 800 gr of Swedish candy in a Pressbyr\u00e5n to taste at home.\n\u00a0\nOverall Conference Experience\nOverall a very nice conference, with lots of perks for speakers. The organisers are a group of nice and helpful people, always there to assist you or just have a nice conversation. The location was perfect, in the city center next to the central station of Stockholm. All the rooms were good and well equipped, sound was good everywhere too. The food served looked luxurious and was really good. Water was available throughout the conference center, but I couldn\u2019t find a place to get soda drinks (at Devoxx they had fridges everywhere). Although it\u2019s a bit Sweden focussed, I would recommend going to this conference if you have the chance, especially as a speaker.\nRecommended talks to watch back, in order of my most favourite first:\n\nHow hacking works by Espen Sande-Larsen\nThe New Super Power in the Developer\u2019s Toolbox by Lize Raes\nFive things every developer should know about software architecture by Simon Brown\n\n\u00a0\nKey Takeaways\nWe need to embrace AI. It will not replace us, but the people not using it will be replaced by people who do. It can greatly improve productivity. Just be aware it\u2019s not perfect (yet?), so always verify what it produces does not introduce nasty bugs or security risks. I will start using it wherever possible if allowed by company policies.\nNext to that, I came to the conclusion that I should be more aware of security when writing my code. Seeing someone live coding and easily exploiting the vulnerabilities of that code made me really think. I might start introducing CTFs in teams when possible, which could be a fun way to make everyone aware of security (risks) in their codebase.\nAll Jfokus 2024 presentations can be watched on YouTube in this playlist. The workshops, however, are not recorded.\n", "tags": ["ai", "conference", "java", "jfokus"], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 39795, "title": "Analyze and debug Quarkus based AWS Lambda functions with X-Ray", "url": "https://www.luminis.eu/blog/analyze-and-debug-quarkus-based-aws-lambda-functions-with-x-ray/", "updated_at": "2024-02-09T14:09:45", "body": "Serverless architectures, like AWS Lambda, have emerged as a paradigm-shifting approach to building, fast, scalable and cost efficient applications. While Serverless architectures provide unparalleled flexibility, they also introduce new challenges in terms of monitoring and troubleshooting.\nIn this blog post, we explore how Quarkus integrates with AWS X-Ray and how using a Jakarta CDI Interceptor can keep your code clean while adding custom instrumentation.\nQuarkus and AWS Lambda\nQuarkus is a Java based framework tailored for GraalVM and HotSpot, which results in an amazingly fast boot time while having an incredibly low memory footprint. It offers near instant scale up and high density memory utilization, which can be very useful for container orchestration platforms like Kubernetes or Serverless runtimes like AWS Lambda.\nBuilding AWS Lambda Functions can be as easy as starting a Quarkus project, adding the quarkus-amazon-lambda dependency, and defining your AWS Lambda Handler function.\n<dependency>\r\n    <groupId>io.quarkus</groupId>\r\n    <artifactId>quarkus-amazon-lambda</artifactId>\r\n</dependency>\nAn extensive guide on how to develop AWS Lambda Functions with Quarkus can be found in the official Quarkus AWS Lambda Guide.\nEnabling X-Ray for your AWS Lambda functions\nQuarkus provides out of the box support for X-Ray, but you will need to add a dependency to your project and configure some setting to make it work with GraalVM / native compiled Quarkus applications. Let\u2019s first start with adding the quarkus-amazon-lambda-xray dependency.\n\n<dependency>\r\n    <groupId>io.quarkus</groupId>\r\n    <artifactId>quarkus-amazon-lambda-xray</artifactId>\r\n</dependency>\nDon\u2019t forget to enable tracing for your Lambda function otherwise it won\u2019t work. An example of doing that is by setting the tracing argument to active within your AWS CDK code.\nfunction = Function.Builder.create(this, \"feed-parsing-function\")\r\n      ...\r\n      .memorySize(512)\r\n      .tracing(Tracing.ACTIVE)\r\n      .runtime(Runtime.PROVIDED_AL2023)\r\n      .logRetention(RetentionDays.ONE_WEEK)\r\n      .build();\r\n\nAfter the deployment of your function and a function invocation you should be able to see the X-Ray traces from within the Cloudwatch interface. By default it will show you some basic timing information for your function like the initialisation and the invocation duration.\n\nAdding more instrumentation\nNow that the dependencies are in place and tracing is enabled for our function we can enrich the traces in X-Ray by leveraging the X-Ray SDKs TracingIntercepter . For instance for the SQS and DynamoDB client you can explicitly set the intercepter inside the application.properties file.\nquarkus.dynamodb.async-client.type=aws-crt\r\nquarkus.dynamodb.interceptors=com.amazonaws.xray.interceptors.TracingInterceptor\r\nquarkus.sqs.async-client.type=aws-crt\r\nquarkus.sqs.interceptors=com.amazonaws.xray.interceptors.TracingInterceptor\r\n\nAfter putting these properties in place, redeploying and executing the function, the TracingIntercepter will wrap around each API call to SQS and DynamoDB and store the actual trace information along side the trace.\n\nThis is very useful for debugging purposes as it will allow you to validate your code and check for any mistakes. Requests to AWS Services are part of the pricing model, so if you make a mistake in your code and you make too many calls it can become quite costly.\nCustom subsegments\nWith the AWS SDK TracingInterceptor configured we get information about the calls to the AWS APIs. But what if we want to see information about our own code or remote calls to services outside of AWS?\nThe Java SDK for X-Ray supports the concept of adding custom subsegments to your traces. You can add subsegments to a trace by adding a few lines of code to your own business logic. You can see in the following code snippet.\n  public void someMethod(String argument)  {\r\n    // wrap in subsegment\r\n    Subsegment subsegment = AWSXRay.beginSubsegment(\"someMethod\");\r\n    try {\r\n      // Your business logic\r\n    } catch (Exception e) {\r\n      subsegment.addException(e);\r\n      throw e;\r\n    } finally {\r\n      AWSXRay.endSubsegment();\r\n    }\r\n  }\r\n\nAlthough this is trivial to do, it will become quit messy if you have a lot of methods you want to apply tracing to. This isn\u2019t ideal and it would be better of we don\u2019t have to mix our own code with the X-Ray instrumentation.\nQuarkus and Jakarta CDI Interceptors\nThe Quarkus programming model is based on the Lite version of the Jakarta Contexts and Dependency Injection 4.0 specification. Besides dependency injection the specification also describes other features like:\n\nLifecycle Callbacks \u2013 A bean class may declare lifecycle @PostConstruct and @PreDestroy callbacks.\nInterceptors \u2013 used to separate cross-cutting concerns from business logic.\nDecorators \u2013 similar to interceptors, but because they implement interfaces with business semantics, they are able to implement business logic.\nEvents and Observers \u2013 Beans may also produce and consume events to interact in a completely decoupled fashion.\n\nAs mentioned, CDI Interceptors are used to separate cross-cutting concerns from business logic. As tracing is a cross-cutting concern this sounds like a great fit. Let\u2019s take a look at how we can create an interceptor for our AWS X-Ray instrumentation.\nWe start with defining our interceptor binding which we will call XRayTracing. Interceptor bindings are intermediate annotations that may be used to associate interceptors with target beans.\npackage com.jeroenreijn.aws.quarkus.xray;\r\n\r\nimport jakarta.annotation.Priority;\r\nimport jakarta.interceptor.InterceptorBinding;\r\n\r\nimport java.lang.annotation.Retention;\r\n\r\nimport static java.lang.annotation.RetentionPolicy.RUNTIME;\r\n\r\n@InterceptorBinding\r\n@Retention(RUNTIME)\r\n@Priority(0)\r\npublic @interface XRayTracing {\r\n}\r\n\nThe next step is to define the actual Interceptor logic, the code that will add the additional X-Ray instructions for creating the subsegment and wrapping it around our business logic.\npackage com.jeroenreijn.aws.quarkus.xray;\r\n\r\nimport com.amazonaws.xray.AWSXRay;\r\nimport jakarta.interceptor.AroundInvoke;\r\nimport jakarta.interceptor.Interceptor;\r\nimport jakarta.interceptor.InvocationContext;\r\n\r\n@Interceptor\r\n@XRayTracing\r\npublic class XRayTracingInterceptor {\r\n\r\n    @AroundInvoke\r\n    public Object tracingMethod(InvocationContext ctx) throws Exception {\r\n        AWSXRay.beginSubsegment(\"## \" + ctx.getMethod().getName());\r\n        try {\r\n            return ctx.proceed();\r\n        } catch (Exception e) {\r\n            AWSXRay.getCurrentSubsegment().addException(e);\r\n            throw e;\r\n        } finally {\r\n            AWSXRay.endSubsegment();\r\n        }\r\n    }\r\n}\r\n\nAn important part of the interceptor is the @AroundInvoke annotation, which means that this interceptor code will be wrapped around the invocation of our own business logic.\nNow that we\u2019ve defined both our interceptor binding and our interceptor it\u2019s time to start using it. Every method that we want to create a subsegment for, can now be annotated with the @XRayTracing annotation.\n@XRayTracing\r\npublic SyndFeed getLatestFeed() {\r\n    InputStream feedContent = getFeedContent();\r\n    return getSyndFeed(feedContent);\r\n}\r\n\r\n@XRayTracing\r\npublic SyndFeed getSyndFeed(InputStream feedContent) {\r\n    try {\r\n        SyndFeedInput feedInput = new SyndFeedInput();\r\n        return feedInput.build(new XmlReader(feedContent));\r\n    } catch (FeedException | IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}\r\n\nThat\u2019s looks much better. Pretty clean if I say so myself.\nBased on the hierarchy of subsegments for a trace, X-Ray is able to show a nested tree structure with the timing information.\n\nClosing thoughts\nThe integration between Quarkus and X-Ray is quite simple to enable. The developer experience is really good out of the box with defining the interceptors on a per client basis. With the help of CDI interceptors you can keep your code clean without worrying too much about X-Ray specific code inside your business logic.\nAn alternative to building your own Interceptor might be to start using AWS PowerTools for Lambda (Java). Powertools for Java is a great way to boost your developer productivity. However, it can be used for more than X-Ray, so I\u2019ll save it for another post.\n", "tags": ["aws", "java", "observability", "serverless"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 39738, "title": "Deploying SageMaker Pipelines Using AWS CDK", "url": "https://www.luminis.eu/blog/deploying-sagemaker-pipelines-using-aws-cdk/", "updated_at": "2024-02-09T14:00:34", "body": "Introduction\nSageMaker is a loved and feared AWS service. You can do anything with it, from building data pipelines, to training machine learning models, to serving said models to your customers. Because of this, there is a range of approaches for any of these problems, which can often be a source of confusion on how to proceed.\nIn this blog, I clear up one such confusion about the deployment of SageMaker pipelines. I show you how to write your own pipeline definitions and how to deploy them using AWS CDK into your SageMaker domain.\nIf you are not yet working with AWS SageMaker I highly encourage you to try it out before proceeding with this walkthrough, specifically because we will be addressing some quite advanced concepts.\nWhat is SageMaker?\nBefore we delve into the how to of deploying SageMaker Pipelines using AWS CDK, it\u2019s essential to understand what SageMaker is and what it brings to the table.\nAmazon SageMaker is a fully managed machine learning service provided by AWS. It\u2019s a comprehensive service that covers a wide range of machine learning tasks. It assists with data preparation, provides a notebook development environment, handles endpoint deployment, provides tools for model evaluation and much more. In essence, it\u2019s a one-stop-shop for machine learning operations, designed to simplify the process of building, training, and deploying machine learning models.\nHowever, these components, while individually powerful, need a maestro to orchestrate them into a cohesive workflow. That\u2019s where SageMaker Pipelines come in. They bridge the gap between these elements, ensuring they work together seamlessly. This orchestration acts as the connecting piece in your MLOps workflow, reducing the complexity and enhancing the manageability of your projects.\nWhat is SageMaker Pipelines?\nSageMaker Pipelines are a versatile service to orchestrate various tasks within an ML model lifecycle. Each pipeline consists of interconnected steps, each of which can run a configured docker container within SageMaker runtime or call one of the services within SageMaker. A few notable features include, but are not limited to:\n\nAllows using custom docker images from AWS ECR.\nCan seamlessly pass large files or metrics between various steps.\nSupport has a Local Mode for testing the pipelines and containerized steps locally.\nIntegrates with services such as AWS Athena and SageMaker Feature Store gathering the necessary (training) data.\nExecutable from services such as AWS StepFunctions and AWS Lambda using AWS SDK.\n\n\nDeploying SageMaker Pipelines using AWS CDK: a high level overview\nBefore we delve into the specifics, it is beneficial to understand the overall structure of our deployment. The following diagram illustrates the components involved in this blog:\n\nOne important aspect to note is that the SageMaker Pipeline does not directly depend on the SageMaker domain. This is correct, the pipeline is a standalone resource, and can be launched programmatically using the AWS SDK or step functions, which is useful in minimal setups.\nHowever, for manual launches, a SageMaker workspace is required. This is where the SageMaker domain becomes necessary.\nTherefore, to ensure a comprehensive understanding of the process, we will also cover the creation of a SageMaker domain in this blog. This will provide a complete overview of the deployment process, equipping you with the knowledge to effectively manage your machine learning projects.\nSetting Up Your Infrastructure\nIn this section, we will focus on the initial steps required to set up the necessary infrastructure for our project. The first task involves creating a CloudFormation project which will deploy our AWS resources including: SageMaker domain, users, data buckets and optionally the VPC.\nFor those interested in the complete code, it is available on\u00a0Github.\nCreate a VPC (optional)\nIf you\u2019ve already got a VPC up and running, you\u2019re one step ahead. Just update the vpc_name in the cdk.json file and feel free to skip this section. However, if you\u2019re looking around and realizing you\u2019re VPC-less, don\u2019t fret. We\u2019ve got you covered.\nCreating a SageMaker domain requires a VPC. By adding the following snippet to your infrastructure CDK stack, will create one for you.\nNote that this particular VPC comes with a public IP. Be aware that this could incur some running costs.\n\nvpc = ec2.Vpc(  \r\n    self,  \r\n    id=\"VpcConstruct\",  \r\n    ip_addresses=ec2.IpAddresses.cidr(\"10.0.0.0/16\"),  \r\n    vpc_name=f\"{self.prefix}-vpc\",  \r\n    max_azs=3,  \r\n    nat_gateways=1,  \r\n    subnet_configuration=[  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Public\",  \r\n            subnet_type=ec2.SubnetType.PUBLIC,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=23,  \r\n            name=\"Private\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Isolated\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED,  \r\n        ),  \r\n    ],  \r\n)\n\nDeploying SageMaker Domain\nFirst things first, before we get into the details of creating a SageMaker domain, we need to establish a default role that all users will assume. This can be fine-tuned or overridden later, depending on your specific use case. Here\u2019s how you can create an execution role:\nvpc = ec2.Vpc(  \r\n    self,  \r\n    id=\"VpcConstruct\",  \r\n    ip_addresses=ec2.IpAddresses.cidr(\"10.0.0.0/16\"),  \r\n    vpc_name=f\"{self.prefix}-vpc\",  \r\n    max_azs=3,  \r\n    nat_gateways=1,  \r\n    subnet_configuration=[  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Public\",  \r\n            subnet_type=ec2.SubnetType.PUBLIC,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=23,  \r\n            name=\"Private\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS,  \r\n        ),  \r\n        ec2.SubnetConfiguration(  \r\n            cidr_mask=24,  \r\n            name=\"Isolated\",  \r\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED,  \r\n        ),  \r\n    ],  \r\n)\r\nCopy\nNow, let\u2019s talk about storage. In SageMaker, scripts, notebooks, and similar resources are all stored in an S3 bucket. By default, SageMaker creates one centralized storage bucket for code and data when you create it using AWS console.\nWe on the other hand will create a separate source and data buckets with the following settings. Both buckets are configured to be inaccessible to the public for obvious reasons.\nself.sm_sources_bucket = s3.Bucket(  \r\n\tself,  \r\n\tid=\"SourcesBucket\",  \r\n\tbucket_name=f\"{self.prefix}-sm-sources\",  \r\n\tlifecycle_rules=[],  \r\n\tversioned=False,  \r\n\tremoval_policy=cdk.RemovalPolicy.DESTROY,  \r\n\tauto_delete_objects=True,  \r\n\t# Access  \r\n\taccess_control=s3.BucketAccessControl.PRIVATE,  \r\n\tblock_public_access=s3.BlockPublicAccess.BLOCK_ALL,  \r\n\tpublic_read_access=False,  \r\n\tobject_ownership=s3.ObjectOwnership.OBJECT_WRITER,  \r\n\tenforce_ssl=True,  \r\n\t# Encryption  \r\n\tencryption=s3.BucketEncryption.S3_MANAGED,  \r\n)\r\nCopy\nThe pipeline, by default, will assume the user\u2019s role unless specified otherwise. For our purposes, the user, or the pipeline, should have enough permissions to read the code for pipeline execution and write the results to the data bucket. It\u2019s a good practice to keep the code read-only when running the pipeline, both for security reasons and to avoid any issues during runtime.\n# Grant read access to SageMaker execution role  \r\nself.sm_sources_bucket.grant_read(self.sm_execution_role)\r\n# Grant read/write access to SageMaker execution role  \r\nself.sm_data_bucket.grant_read_write(self.sm_execution_role)\r\nCopy\nCreating a SageMaker domain itself is a very straightforward process. You just need to give it a name, attach it to the domain VPC you have from the previous steps, and attach the execution role to the default user config. If you want to specify additional security settings such as \u201cVPC Only\u201d mode, you can do it here as well. Similarly, we set tags so all the resources that start under the specific domain or user will inherit cost allocation tags accordingly.\n# Fetch VPC information  \r\nvpc_name = self.node.try_get_context(\"vpc_name\")  \r\nself.vpc = ec2.Vpc.from_lookup(  \r\n    self, id=\"ImportedVpc\",  \r\n    vpc_name=vpc_name if vpc_name else f\"{self.prefix}-vpc\"  \r\n)  \r\npublic_subnet_ids = [public_subnet.subnet_id for public_subnet in self.vpc.public_subnets]  \r\n  \r\n# Create SageMaker Studio domain  \r\nself.domain = sm.CfnDomain(  \r\n    self, \"SagemakerDomain\",  \r\n    auth_mode='IAM',  \r\n    domain_name=f'{self.prefix}-SG-Project',  \r\n    default_user_settings=sm.CfnDomain.UserSettingsProperty(  \r\n        execution_role=self.sm_execution_role.role_arn  \r\n    ),  \r\n    app_network_access_type='PublicInternetOnly',  \r\n    vpc_id=self.vpc.vpc_id,  \r\n    subnet_ids=public_subnet_ids,  \r\n    tags=[cdk.CfnTag(  \r\n\t    key=\"project\",  \r\n\t    value=\"example-pipelines\"  \r\n\t)],\r\n)\r\nCopy\nFinally, we create a user that will be used for invoking the pipeline when run manually.\n# Create SageMaker Studio default user profile  \r\nself.user = sm.CfnUserProfile(  \r\n    self, 'SageMakerStudioUserProfile',  \r\n    domain_id=self.domain.attr_domain_id,  \r\n    user_profile_name='default-user',  \r\n    user_settings=sm.CfnUserProfile.UserSettingsProperty()  \r\n)\r\nCopy\nRun the deploy command using CDK and there you have it! You\u2019ve successfully deployed a SageMaker domain. You can always tweak and customize your setup to better suit your project\u2019s needs, such as configuring roles, attaching ECR images and git repos for notebooks. In the next section, we\u2019ll dive into deploying a simple pipeline.\ncd ./infrastructure_project\r\n\r\ncdk deploy\r\nCopy\nDeploying a Simple Pipeline\nThe deployment of a SageMaker pipeline is a complicated process that involves two key tasks. First, we need to generate a pipeline definition using the SageMaker SDK. Then, we deploy this definition using CloudFormation. Let\u2019s delve into the details of each task.\n\nThe Pipeline Definition\nThe pipeline definition is a structured JSON document that instructs AWS on the sequence of steps to execute, the location for execution, the code to be run, the resources required, and the interdependencies of these steps. Essentially, it is a detailed execution plan for your machine learning pipeline.\nCreating this JSON document manually can be cumbersome and prone to errors. To mitigate this, the SageMaker SDK provides an abstraction layer that enables the use of Python code constructs to build the pipeline definition. You can start using it by adding it as a python dependency with pip install sagemaker.\nTo streamline the process of pipeline creation, we establish a base class. This class serves as an interface, which will be particularly useful when we integrate our pipeline with the rest of our CDK code. Here, we utilize Pydantic BaseModel class to enable type checking on configuration parameters you might want to pass to the pipeline.\nclass SagemakerPipelineFactory(BaseModel):  \r\n    \"\"\"Base class for all pipeline factories.\"\"\"  \r\n    @abstractmethod  \r\n    def create(  \r\n        self,  \r\n        role: str,  \r\n        pipeline_name: str,  \r\n        sm_session: sagemaker.Session,  \r\n    ) -> Pipeline:  \r\n        raise NotImplementedError\r\nCopy\nWe can now proceed to write the actual pipeline declaration using the SageMaker SDK, and one such configuration parameter (pipeline_config_parameter).\nclass ExamplePipeline(SagemakerPipelineFactory):  \r\n    pipeline_config_parameter: str  \r\n  \r\n    def create(  \r\n        self,  \r\n        role: str,  \r\n        pipeline_name: str,  \r\n        sm_session: sagemaker.Session,  \r\n    ) -> Pipeline:\r\n\t    ...\r\nCopy\nWe proceed by declaring a runtime configurable parameter for the instance type. Then we add ScriptProcessor which defines the environment our script will be running in; including the machine instance count, the IAM execution role and the base image.\n...\r\n# Use the SKLearn image provided by AWS SageMaker  \r\nimage_uri = sagemaker.image_uris.retrieve(  \r\n\tframework=\"sklearn\",  \r\n\tregion=sm_session.boto_region_name,  \r\n\tversion=\"0.23-1\",  \r\n)  \r\n\r\n# Create a ScriptProcessor and add code / run parameters  \r\nprocessor = ScriptProcessor(  \r\n\timage_uri=image_uri,  \r\n\tcommand=[\"python3\"],  \r\n\tinstance_type=instance_type_var,  \r\n\tinstance_count=1,  \r\n\trole=role,  \r\n\tsagemaker_session=sm_session,  \r\n)\r\nCopy\nNext we define our first processing step that will use the defined processor (environment definition) to run our script with given job arguments, as well as, input and output definitions.\nprocessing_step = ProcessingStep(  \r\n\tname=\"processing-example\",  \r\n\tstep_args=processor.run(  \r\n\t\tcode=\"pipelines/sources/example_pipeline/evaluate.py\",  \r\n\r\n\t),  \r\n\tjob_arguments=[  \r\n\t\t\"--config_parameter\", self.pipeline_config_parameter  \r\n\t],\r\n\tinputs=[],\r\n\toutputs=[]\r\n)\r\n\r\n\r\nCopy\nOne single step is already enough to define a pipeline. While defining the pipeline, make sure to list it\u2019s runtime parameters.\nreturn Pipeline(  \r\n\tname=pipeline_name,  \r\n\tsteps=[processing_step],  \r\n\tsagemaker_session=sm_session,  \r\n\tparameters=[instance_type_var],  \r\n)\r\nCopy\nHere is the simple script that our job will be runing. It essentially prints the input job argument.\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--config_parameter\", type=str)\r\nargs = parser.parse_args()\r\n\r\nprint(f\"Hello {args.config_parameter}!\")\r\nCopy\nAbove, we have demonstrated a minimal example for building a machine learning pipeline. If you are interested in a deeper dive of the possibilities, check out the examples in The Official Documentation.\nDeploying the Pipeline Definition\nNow that we have our pipeline definition, the next step is deploying it to your AWS Account. This is where CloudFormation comes into play, as it supports the AWS::SageMaker::Pipeline Resource. Looking at the arguments, we see that the pipeline definition should be embedded as a JSON document within the CloudFormation template. This JSON document, in our case, is emitted by SageMaker SDK, which we call during the synthesis phase of the CloudFormation stack creation.\n# Define the pipeline (this step uploads required code and packages by the pipeline to S3)  \r\npipeline = pipeline_factory.create(  \r\n\tpipeline_name=pipeline_name,  \r\n\trole=sm_execution_role_arn,  \r\n\tsm_session=sagemaker_session,  \r\n)  \r\n\r\npipeline_def_json = json.dumps(json.loads(pipeline.definition()), indent=2, sort_keys=True)\r\nCopy\nNote that a new version of the code is deployed into the source bucket by SageMaker SDK before the CloudFormation stack is applied. This might raise a few eyebrows, but it will not cause issues with existing processes, as it is stored in a folder based on a derived version identifier. This does mean that you may need additional cleanup scripts later down the line.\nOnce we have a pipeline definition JSON, we can declare the CfnPipeline construct.\ndef create_pipeline_resource(  \r\n    self,  \r\n    pipeline_name: str,  \r\n    pipeline_factory: SagemakerPipelineFactory,  \r\n    sources_bucket_name: str,  \r\n    sm_execution_role_arn: str,  \r\n) -> Tuple[sm.CfnPipeline, str]:\r\n\t...\r\n\r\n\t# Define the pipeline (this step uploads required code and packages by the pipeline to S3)  \r\n\t...\r\n\t\r\n\t# Define CloudFormation resource for the pipeline, so it can be deployed to your account  \r\n\tpipeline_cfn = sm.CfnPipeline(  \r\n\t\tself,  \r\n\t\tid=f\"SagemakerPipeline-{pipeline_name}\",  \r\n\t\tpipeline_name=pipeline_name,  \r\n\t\tpipeline_definition={\"PipelineDefinitionBody\": pipeline_def_json},  \r\n\t\trole_arn=sm_execution_role_arn,  \r\n\t)  \r\n\tarn = self.format_arn(  \r\n\t\tservice='sagemaker',  \r\n\t\tresource='pipeline',  \r\n\t\tresource_name=pipeline_cfn.pipeline_name,  \r\n\t)\r\n\treturn pipeline_cfn, arn\r\nCopy\nFinally, we combine all everything together by passing our pipeline factory to pipeline resource creation function along with our source and data buckets.\n# Load infrastructure stack outputs as value parameters (resolved at cdk deploy time)  \r\nsources_bucket_name = ssm.StringParameter.value_from_lookup(  \r\n    self, f\"/{self.prefix}/SourcesBucketName\")  \r\nsm_execution_role_arn = ssm.StringParameter.value_from_lookup(  \r\n    self, f\"/{self.prefix}/SagemakerExecutionRoleArn\")  \r\n  \r\n# Create a configured pipeline  \r\nself.example_pipeline, self.example_pipeline_arn = self.create_pipeline_resource(  \r\n    pipeline_name='example-pipeline',  \r\n    pipeline_factory=ExamplePipeline(  \r\n        pipeline_config_parameter=\"Hello world!\"  \r\n    ),  \r\n    sources_bucket_name=sources_bucket_name,  \r\n    sm_execution_role_arn=sm_execution_role_arn,  \r\n)\r\nCopy\nNow the code is complete, deploy the pipeline using the CDK commands.\ncd ./data_project\r\n\r\ncdk deploy\r\nCopy\nTesting the Result: deploying SageMaker Pipelines using AWS CDK\nAfter deploying both of the stacks, we can view and run our pipeline in SageMaker Studio.\nNavigate to the SageMaker service in the AWS Management Console and click on \u201cDomains.\u201d Ensure that your SageMaker domain, created as part of the infrastructure stack, is visible.\n\nInside the SageMaker domain, click on \u201cLaunch\u201d near your created user and launch the SageMaker Studio.\n\nIn the navigation select \u201cPipelines\u201d to see a list of deployed pipelines. Confirm that your example pipeline is listed.\n\nClick on the specific pipeline (e.g., \u201cexample-pipeline\u201d) to view its details and start an exectution to start and monitor your pipeline.\n\nConclusion on deploying SageMaker Pipelines using AWS CDK\nIn this blog, we have learned how to write a simple SageMaker Pipeline in Python and deploy it in AWS CDK. While doing so, we also have deployed a SageMaker Domain, discussed how the pipeline code is stored in AWS, and shared a few best practices for configuration.\nWe have only scratched the surface of what is possible with SageMaker, there are various topics that are equally important within MLOps projects such as testing your code and pipelines, local development, and automated quality monitoring.\nStay tuned for more, or contact me if you have any questions.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 39685, "title": "AWS Multi-Account GitOps Deployment 3: AWS GitHub Deployment", "url": "https://www.luminis.eu/blog/cloud-en/aws-multi-account-gitops-deployment-3-aws-github-deployment/", "updated_at": "2024-02-08T12:31:30", "body": "Welcome to the final installment of our series on multi-account GitOps deployment on AWS.\u00a0After setting up a multi-account AWS organizational structure and integrating AWS accounts with GitHub Actions with short-lived credentials,\u00a0we\u2019re now ready to dive into the GitHub Actions deployment process.\nThe Importance of CICD in Modern Software Development\nContinuous Integration and Continuous Delivery\u00a0(CICD)\u00a0is one of the most important tools in today\u2019s software development endeavours.\u00a0It allows the developers to build,\u00a0test and see the effect of their addition to the codebase in an automated fashion.\u00a0Within the vast amount of providers available for creating pipelines such as GitHub Actions,\u00a0GitLab CICD,\u00a0AWS CodePipeline etc.,\u00a0we will be opting to use GitHub Actions for this series.\u00a0The reason,\u00a0we are using GitHub Actions is that it is a free tool that integrates perfectly with one of the best code repository GitHub that has a huge number of quality custom steps created and maintained by the community.\nSetting Up the GitHub Repository for Deployment\nIn this tutorial, we will create a simple serverless api that has a single Lambda function behind an API Gateway that responds differently with respect to the environment that the stack is deployed in.\n\nInitialize the Repository: Begin by creating a new directory for your project and initialize it as a git repository.\nmkdir simple-api && cd simple-api\r\ngit init\r\nnpx projen new awscdk-app-ts\nSetting Up AWS CDK and Projen: Update the\u00a0.projenrc.ts\u00a0configuration file similar to following:\nimport { awscdk } from 'projen';\r\nimport { ApprovalLevel } from 'projen/lib/awscdk';\r\n\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\nauthorEmail: 'utku.demir@luminis.eu',\r\nauthorName: 'Utku Demir',\r\ncdkVersion: '2.96.2',\r\ndefaultReleaseBranch: 'main',\r\nname: 'simple-api',\r\ndescription: 'A CDK project for Simple Api GitOps Deployments',\r\ngithub: false,\r\nprojenrcTs: true,\r\nkeywords: [\r\n'AWS CDK',\r\n'projen',\r\n'Typescript',\r\n'Deployment',\r\n],\r\nrequireApproval: ApprovalLevel.NEVER,\r\ngitignore: ['.idea'],\r\nlicense: 'MIT',\r\nlicensed: true,\r\n});\r\nproject.synth();\r\n After updating,\u00a0generate the project:\nyarn projen\nImplementing the Necessary CDK Stack: Create a new file named\u00a0simple_api_stack.ts\u00a0under\u00a0src\u00a0to include the necessary configuration for our stack:\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as apigateway from 'aws-cdk-lib/aws-apigateway';\r\nimport { EndpointType } from 'aws-cdk-lib/aws-apigateway';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\nexport interface SimpleApiStackProps extends StackProps {\r\n  environment: string;\r\n}\r\n\r\nexport class SimpleApiStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: SimpleApiStackProps) {\r\n    super(scope, id, props);\r\n\r\n    // Define the Lambda function\r\n    const helloLambda = new lambda.Function(this, 'HelloLambda', {\r\n      runtime: lambda.Runtime.NODEJS_18_X,\r\n      handler: 'index.handler',\r\n      code: lambda.Code.fromInline(`\r\n                exports.handler = async function(event, context) {\r\n                    return {\r\n                        statusCode: 200,\r\n                        body: JSON.stringify({ message: \"Hello, World from ${props.environment} environment!\" })\r\n                    };\r\n                };\r\n            `),\r\n    });\r\n\r\n    // Define the API Gateway\r\n    new apigateway.LambdaRestApi(this, 'Endpoint', {\r\n      handler: helloLambda,\r\n      proxy: true,\r\n      deploy: true,\r\n      cloudWatchRole: true,\r\n      endpointTypes: [EndpointType.EDGE],\r\n    });\r\n  }\r\n}\nHere,\u00a0we create an edge optimized API Gateway Rest Api with a single Lambda handler that will return the message formatted with the correct environment given a get request to the base url.\nLastly,\u00a0edit the\u00a0main.ts\u00a0under\u00a0src\u00a0to include this stack as:\nimport { App } from 'aws-cdk-lib';\r\nimport { SimpleApiStack } from './simple_api_stack';\r\n\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n   account: process.env.AWS_ACCOUNT,\r\n   region: process.env.AWS_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nconst environment = process.env.ENVIRONMENT || 'dev';\r\n\r\nnew SimpleApiStack(app, `${environment}-simple-api`, {\r\n   env: devEnv,\r\n   environment: environment,\r\n});\r\n\r\napp.synth();\nOne important thing to note here is that we have an environment variable called\u00a0ENVIRONMENT\u00a0that will be injected through the GitHub Actions.\nIntegrating AWS with GitHub Actions\nRepository Environments\u00a0Under settings of the repository, you will find the Environments tab. This will allow you to manage your environments for the application. Create all the environments, namely, dev, test and prod as follows:\n\nEnvironment Secrets and Variables: Set up necessary environment secrets and variables on the environment. These variables will store sensitive information like AWS credentials, ensuring they are securely managed and not hard-coded in your scripts is critical.\n\nHere,\u00a0AWS_ACCOUNT\u00a0is the 12-digit account id corresponding to the environment that the stack will be deployed in.\nEnvironment Protection: Add protection to the environments for test and prod environments. This step is crucial to ensure that changes can only be made to these environments under certain conditions, such as after code reviews or passing automated tests. This works like a manual approval and keep in mind that for private repositories, this is only possible if your organization\u2019s plan is GitHub Enterprise. If this is not the case for your organization, I would suggest taking a look at the amazing custom manual approval step made by trstringer linked in the references.\n\nRepository Secrets: Add the\u00a0DEPLOY_ROLE\u00a0as a repository secret in GitHub. This role is necessary for GitHub Actions to interact with your AWS environment securely, and it will be assumed from the deployment account, hence, it will be same for all the environments.\n\nReplace the censored part with your deployment AWS account id.\nCreating the Deployment Workflow\nHere,\u00a0are the necessary steps to create a GitHub Actions Workflow that will deploy our CDK stack to AWS Cloud.\n\nWorkflow File: Write a GitHub Actions workflow file. This file will define the steps and conditions under which your code is deployed to AWS.\nTriggering Deployments: Set up triggers for deployment. For example, you might trigger deployments on pushing to specific branches or tagging a release.\nDeployment Steps: Detail the steps for deploying your application. This may include building your application, running tests, and using AWS CDK commands to deploy to AWS.\nPost-Deployment Verification: Implement steps to verify the successful deployment of your application. This might include health checks, smoke tests, or rollback procedures in case of failure.\n\nFollowing is the deploy workflow file for our simple api for the dev environment:\n# .github/workflows/deploy.yml\r\nname: deploy\r\n\r\non:\r\n  push:\r\n    branches:\r\n      - main\r\n\r\njobs:\r\n  build:\r\n    runs-on: ubuntu-latest\r\n    defaults:\r\n      run:\r\n        working-directory: ./simple-api\r\n    steps:\r\n      - name: Pull repository\r\n        uses: actions/checkout@v3\r\n\r\n      - name: Setup Nodejs and npm\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n\r\n      - name: Setup yarn\r\n        run: npm install -g yarn\r\n\r\n      - name: Setup Nodejs with yarn caching\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n          cache: yarn\r\n          cache-dependency-path: './simple-api/yarn.lock'\r\n\r\n      - name: Install dependencies\r\n        run: yarn install --frozen-lockfile\r\n\r\n      - name: Build and test the project\r\n        run: npx projen build\r\n\r\n  deploy-dev-simple-api:\r\n    runs-on: ubuntu-latest\r\n    defaults:\r\n      run:\r\n        working-directory: ./simple-api\r\n    needs: build\r\n    environment: dev\r\n    env:\r\n      AWS_ACCOUNT: ${{ secrets.AWS_ACCOUNT }}\r\n      AWS_REGION: ${{ vars.AWS_REGION }}\r\n      ENVIRONMENT: ${{ vars.ENVIRONMENT }}\r\n    permissions:\r\n      id-token: write\r\n      contents: read\r\n    steps:\r\n      - name: Pull repository\r\n        uses: actions/checkout@v3\r\n\r\n      - name: Setup Nodejs and npm\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n\r\n      - name: Setup yarn\r\n        run: npm install -g yarn\r\n\r\n      - name: Setup Nodejs with yarn caching\r\n        uses: actions/setup-node@v3\r\n        with:\r\n          node-version: \"18\"\r\n          cache: yarn\r\n          cache-dependency-path: './simple-api/yarn.lock'\r\n\r\n      - name: Install dependencies\r\n        run: yarn install --frozen-lockfile\r\n\r\n      - name: Assume deploy role\r\n        uses: aws-actions/configure-aws-credentials@v2\r\n        with:\r\n          role-to-assume: ${{ secrets.DEPLOY_ROLE }}\r\n          aws-region: ${{ vars.AWS_REGION }}\r\n\r\n      - name: Deploy the application\r\n        run: npx projen deploy --all\nThis workflow file triggers when a push is made to our main branch. It first builds and tests the stack and then runs the deploy step for the dev\u00a0environment.\u00a0To implement the\u00a0test\u00a0and\u00a0prod\u00a0environment deploy steps,\u00a0just copy the\u00a0dev\u00a0job and replace the\u00a0dev\u00a0with\u00a0test\u00a0and follow a similar a process for the\u00a0prod\u00a0environment.\u00a0It is also crucial to make these steps run sequentially so that the deployment to environments don\u2019t happen all at once.\u00a0As such,\u00a0we would need to make sure\u00a0test\u00a0deployment depends on\u00a0dev\u00a0and the\u00a0prod\u00a0deployment depends on\u00a0test.\u00a0For that,\u00a0change the needs part to the name of the\u00a0dev\u00a0or\u00a0test\u00a0job\u2019s name,\u00a0respectively.\nFor this tutorial,\u00a0I haven\u2019t included the steps necessary for the 4th step for post deployment health checks but in a production environment,\u00a0it is crucial to make sure everything is working as expected.\nThe Deployment of the AWS CDK Stack\nFinally,\u00a0when we push our application to our remote\u00a0main\u00a0branch,\u00a0we should expect to see our workflow start running.\u00a0Make sure the working directories are correctly set within the workflow file to achieve successful execution!\n\nWhen the deployment finishes,\u00a0you should be able to use the api endpoint presented in the outputs of the deployment within the workflow logs.\n\nHere is how the manual approval step of the GitHub Actions is seen in the workflow page:\n\nConclusion on AWS GitHub Deployment\nAs we conclude our series on AWS Multi-Account GitOps Deployment on AWS using GitHub Actions, let\u2019s reflect on the journey we\u2019ve embarked on and the key takeaways from this process.\nEmbracing Automation and Efficiency\nThrough this series,\u00a0we\u2019ve explored how to effectively leverage automation to manage and deploy resources across multiple AWS accounts.\u00a0By integrating AWS with GitHub Actions,\u00a0we\u2019ve demonstrated a powerful combination of cloud infrastructure management and modern CI/CD practices.\nAdvantages of Multi-Account Strategy\nThe multi-account strategy on AWS,\u00a0when combined with GitOps practices,\u00a0offers enhanced security,\u00a0better resource segregation,\u00a0and more granular control over access and billing.\u00a0This approach is particularly beneficial for larger teams and organizations aiming to scale their cloud infrastructure efficiently.\nThe Power of AWS Multi-Account GitHub\nGitHub Actions stands out as a versatile and user-friendly tool for CI/CD,\u00a0providing a seamless integration with GitHub repositories.\u00a0Its ability to handle complex workflows,\u00a0along with the vast community-driven actions available,\u00a0makes it an excellent choice for teams looking to implement GitOps.\nKey Learning Points\n\nSetting Up the Foundation: The importance of establishing a strong organizational structure within AWS to support multi-account deployment.\nIntegrating Tools: How to effectively integrate AWS with GitHub Actions, taking advantage of short-lived credentials for enhanced security.\nDeployment Workflows: Crafting GitHub Actions workflows to automate the deployment process, ensuring consistency and reliability.\nSecurity and Best Practices: Emphasizing the importance of security best practices, particularly in managing secrets and permissions.\n\nEncouraging Continuous Learning\nThe landscape of cloud computing and DevOps is ever-evolving.\u00a0This series,\u00a0while comprehensive,\u00a0is just the beginning.\u00a0I encourage you to keep experimenting,\u00a0learning,\u00a0and adapting to new tools and methodologies.\nYour Next Steps\nWith the foundation set and the knowledge gained, you\u2019re now equipped to extend and customize your GitOps workflows. Experiment with different types of deployments, explore new GitHub Actions, and continue to refine your AWS multi-account management strategy. As you make progress,\u00a0share your learnings and collaborate with the community.\u00a0The collective knowledge and experiences help everyone grow and innovate.\nFinal Thoughts\nThank you for joining me in this series.\u00a0I hope it has been enlightening and empowering.\u00a0As you embark on your journey of cloud engineering,\u00a0remember that the path to mastery is through continuous practice and adaptation.\nReferences\nFor further reading and advanced topics,\u00a0here are some resources:\n\nManual Workflow Approval on GitHub Actions\nAWS Documentation on Multi-Account Strategies\nGitHub Actions Documentation\nProduction-Ready CDK\n\n\nHappy Cloud Engineering and until next time!\n", "tags": ["aws", "aws cdk", "cdk", "CI/CD", "cloud", "cloud security", "devops", "github", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 39643, "title": "Simplifying UI with Web Components", "url": "https://www.luminis.eu/blog/simplifying-ui-with-webcomponents/", "updated_at": "2024-02-07T15:05:52", "body": "In the ever-evolving landscape of web development, the quest for more efficient, scalable, and maintainable front-end solutions is perpetual. In this blog post, we dive into simplifying UI with Web Components.\nUnlike native HTML elements, Web Components are not provided by the browser by default. Instead, they are crafted by developers, empowering them to extend HTML itself, defining new elements that behave as if they were native to the web platform.\nThis approach not only leads to more robust and less error-prone applications but also significantly streamlines the development process. The best thing of all: you can use them in all frameworks without a big learning curve, or without frameworks at all!\nSection 1: the basics of using\u00a0Web Components\nComponents in frameworks\nIf you have used a framework you may have noticed that your components usually come with a lot of imports or framework-specific file extensions. For example, two major frameworks:\n// Angular\r\nimport { Component } from '@angular/core';\r\n@Component({\r\n  selector: 'app-component-overview',\r\n  templateUrl: './component-overview.component.html',\r\n})\r\n\r\n// React\r\nclass Welcome extends React.Component {\r\n  render() {\r\n    return <h1>Hello, {this.props.name}</h1>;\r\n  }\r\n}\r\n\nThese imports or file-extensions mean a lot of (potentially big) dependencies and framework specific knowledge. Web Components are native to the browser and as a result they come without complex patterns. This brings upsides and downsides. For sharing components, it brings mostly downsides to use big frameworks if the other project don\u2019t use the same framework. That\u2019s where Web Components jump in.\nUpsides of native Web Components:\n\nEvery framework supports them without any complexity because they are native\nThey are private using Shadow Dom, they don\u2019t mess up the rest of your page\nYou don\u2019t need a framework to create them, although many support the creation\nThey easily integrate into Storybook\n\nDownsides of native Web Components:\n\nNo out-of-the-box solutions for routing\nNo data management solutions built in\nIDE\u2019s don\u2019t support parameter or css styles in autocomplete features\n\nWeb Components\nWeb Components are a browser-native way to write components that can be used in any framework but even without any framework at all. Let me show you an example of a Web Component that uses no frameworks at all:\nclass HelloWorld extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: 'open' });\r\n    this.shadowRoot.innerHTML = `\r\n      <span class=\"hello\">Hello World!</span>\r\n    `;\r\n  }\r\n}\r\ncustomElements.define('hello-world', HelloWorld);\r\n\nYou can write this in a JavaScript file and simply write your HTML like this:\n<body>\r\n    <hello-world></hello-world>\r\n    <script src=\u201dhello-world.js\u201d></script>\r\n</body>\r\n\nWithout any frameworks or magic, you now have a native look-a-like HTML tag!\n\nShadow DOM\nShadow DOM allows developers to encapsulate a piece of the webpage, so its styles and scripts do not interfere with the rest of the page, and vice versa. Think of it like a small, self-contained bubble within your webpage. Elements inside this bubble can have their own HTML, CSS, and JavaScript, which are separate from the rest of the page.\nIn the following example, let\u2019s assume I used the HelloWorld example written in the previous section. I imported the scripts the right way. The HTML looks like this:\n<hello-world></hello-world>\r\n<span class=\"hi\">Hello World!</span>\r\n\nBoth elements render the exact same If I query a native element, or any Web Component without a shadow DOM, you get to see the structure like this:\nconsole.log(document.querySelector('.hi'));\r\n// <span class=\"hi\">Hello World!</span>\r\n\nWe now query the Web Component with an open Shadow DOM attached to it, and the result might be different from what you expect to see:\nconsole.log(document.querySelector('hello-world'));\r\n// You might expect:\r\n// <hello-world><span class=\"hello\">\r\n//   Hello World!\r\n// </span></hello-world>\r\n// The actual result:\r\n// <hello-world></hello-world>\r\n\nShadow DOM shields the internal structure from the script, which means the component is private. As a result, the browser allows you to interact with the element internals through the Shadow DOM. By default your component is left alone and you don\u2019t have to be afraid of leaking style issues!\n\nSection 2: useful concepts for web components\nRendering HTML and CSS\nHTML is the foundation of everything on the web. If you want to show text on the web you use a <p> tag. If you want to draw, you use <canvas>. If you want your awesome-stuff, you use <awesome-stuff>.\nI think you can guess what the following element does.\nclass AwesomeStuff extends HTMLElement {\r\n    constructor() {\r\n        super();\r\n        this.attachShadow({ mode: 'open' });\r\n        this.render();\r\n    }\r\n\r\n    css() {\r\n        return `\r\n            img {\r\n                width: 150px;\r\n            }\r\n        `;\r\n    }\r\n\r\n    render() {\r\n        this.shadowRoot.innerHTML = `\r\n            <img src=\"https://cataas.com/cat\" />\r\n\r\n            <style>${this.css()}</style>\r\n        `;\r\n    }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\n\nUsing Attributes to insert information\nWeb Components are usually stand-alone components that allow or desire no direct mutations and interactions with the outside world. You can, however, define your own so-to-say \u2018api\u2019 by reading Attributes and emitting Events.\nAttributes can be used to put information into a Web Component:\n<awesome-stuff data-name=\"Randy\"></awesome-stuff>\nThe \u2018data-name\u2019 attribute holds the value \u2018Randy\u2019 and can be read from the custom-element like this:\nclass AwesomeStuff extends HTMLElement {\r\n    static observedAttributes = ['name'];\r\n\r\n    constructor() {\r\n        super();\r\n        this.attachShadow({ mode: 'open' });\r\n        this.name = 'World'; // Default value for the name\r\n        this.render();\r\n    }\r\n\r\n    // This is a native method that gets called by the browser\r\n    attributeChangedCallback(attrName, oldValue, newValue) {\r\n        if (attrName === 'name') {\r\n            this.name = newValue; // Update the name property\r\n            this.render();\r\n        }\r\n    }\r\n\r\n    render() {\r\n        this.shadowRoot.innerHTML = `<h1>Hello ${this.name}!</h1>`;\r\n    }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\nYou can see I used \u2018data-name\u2019 and not simply \u2018name\u2019. They both work, but some frameworks use strict HTML parsing and \u2018data-name\u2019 is a supported property while oficially \u2018name\u2019 is not.\nhttps://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes\n\nUsing events to receive information\nWeb Components can share information by emitting events. The browser can be told to observe these events and trigger functions. The elements inside of the Web Component will also trigger events, but these won\u2019t be accessible outside of the Shadow DOM.\nIn the next example you see a Web Component which holds a button, triggering logic and emitting custom events.\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: \"open\" });\r\n    this.render();\r\n  }\r\n\r\n  connectedCallback() {\r\n    const helloButton = this.shadowRoot.querySelector(\"#hello-button\");\r\n\r\n    helloButton.addEventListener(\"click\", () =>\r\n      this.dispatchEvent(new CustomEvent(\"custom-event-hello\", { detail: \"Data from hello button\" }))\r\n    );\r\n  }\r\n\r\n  render() {\r\n    this.shadowRoot.innerHTML = `\r\n      <button id=\"hello-button\">Hello!</button>\r\n    `;\r\n  }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\nListening to these events requires the use of a little JavaScript:\nfunction setupEventListeners() {\r\n  const awesomeStuffElement = document.querySelector('awesome-stuff');\r\n\r\n  awesomeStuffElement.addEventListener('custom-event-hello', (e) => {\r\n    alert('Custom event triggered:', e.detail);\r\n  });\r\n}\r\n\n\nSlots\nWeb Components are shielded from the outside, and the outside is shielded from the Web Components. But that does not mean they cannot co\u00f6perate! We\u2019ve seen Attributes and Events, which allow data flowing in and out of the components. Next to those, we can use Slots. It does not share data or expose anything but it allows the outside to rent some space within the Web Component. Let\u2019s go back to the basics:\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: 'open' });\r\n    this.shadowRoot.innerHTML = `<span class=\"hello\">Hello World!</span>`;\r\n  }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\r\n<awesome-stuff>I am not yet rendered</awesome-stuff>\r\n\nThis still renders \u2018Hello World!\u2019. We need to add a <slot></slot> to render the text inside of the <awesome-stuff> tags like this:\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: \"open\" });\r\n    this.shadowRoot.innerHTML = `<slot><p>Default content.</p></slot>`;\r\n  }\r\n}\r\ncustomElements.define('awesome-stuff', AwesomeStuff);\r\n\r\n<awesome-stuff>I am now inside of awesome-stuff</awesome-stuff>\r\n\nThe text \u2018Default content.\u2019 is only rendered if nothing, not even whitespace, is between the <awesome-stuff> tags.\n\nSection 3: Creating a complex web component together\nLet\u2019s create some 3D dice. These are so complex, you might want to re-use it instead of rewriting it later in other places or other projects. Good thing we use Web Components to create a Custom Element.\nThe HTML is simple:\n<div id=\"die\">\r\n    <div class=\"face face-1\">1</div>\r\n    <div class=\"face face-2\">2</div>\r\n    <div class=\"face face-3\">3</div>\r\n    <div class=\"face face-4\">4</div>\r\n    <div class=\"face face-5\">5</div>\r\n    <div class=\"face face-6\">6</div>\r\n</div>\r\n\nThe CSS will quickly become complex, so lets do it in steps. First, let the die rotate in an animation so we can see the 3D changes. Then we make sure the fonts are set up and all faces are the same size. We also give the faces an orange background.\n#die {\r\n    /* note these CSS variables and calculations are native CSS! */\r\n    --die-size: 50px;\r\n    --face-offset: calc(var(--die-size) / 2);\r\n    position: relative;\r\n    width: var(--die-size);\r\n    aspect-ratio: 1 / 1;\r\n    font-size: var(--face-offset);\r\n    transform-style: preserve-3d;\r\n    animation: rotate 5s infinite linear;\r\n}\r\n\r\n.face {\r\n    position: absolute;\r\n    width: var(--die-size);\r\n    aspect-ratio: 1 / 1;\r\n    background: orange;\r\n    text-align: center;\r\n    font-size: var(--die-size);\r\n    line-height: var(--die-size);\r\n}\r\n\r\n@keyframes rotate {\r\n    0% { transform: rotateX(0deg) rotateY(0deg); }\r\n    100% { transform: rotateX(360deg) rotateY(360deg); }\r\n}\r\n\nWe now have 6 orange squares in the same absolute position (so you only see one). Now let\u2019s add some magic that makes them a 3D cube!\n.face-1 {\r\n    transform: rotateY(0deg) translateZ(var(--face-offset));\r\n}\r\n.face-2 {\r\n    transform: rotateX(270deg) translateZ(var(--face-offset));\r\n}\r\n.face-3 {\r\n    transform: rotateY(90deg) translateZ(var(--face-offset));\r\n}\r\n.face-4 {\r\n    transform: rotateY(270deg) translateZ(var(--face-offset));\r\n}\r\n.face-5 {\r\n    transform: rotateX(90deg) translateZ(var(--face-offset));\r\n}\r\n.face-6 {\r\n    transform: rotateY(180deg) translateZ(var(--face-offset));\r\n}\r\n\nNow let\u2019s add some logic to let the user decide which side the die should roll to.\n  static get observedAttributes() {\r\n    return [\"value\"];\r\n  }\r\n\r\n  attributeChangedCallback(name, oldValue, newValue) {\r\n    if (name === \"value\") {\r\n      this.updateFace(newValue);\r\n    }\r\n  }\r\n\r\n  updateFace(value) {\r\n    const die = this.shadowRoot.querySelector(\"#die\");\r\n    if (value) {\r\n      die.style.animation = \"none\";\r\n      switch (value) {\r\n        case \"1\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(0deg)\";\r\n        case \"2\":\r\n          return die.style.transform = \"rotateX(90deg) rotateY(0deg)\";\r\n        case \"3\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-90deg)\";\r\n        case \"4\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(90deg)\";\r\n        case \"5\":\r\n          return die.style.transform = \"rotateX(-90deg) rotateY(0deg)\";\r\n        case \"6\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-180deg)\";\r\n      }\r\n    } else {\r\n      die.style.animation = \"rotate 5s infinite linear\";\r\n    }\r\n  }\r\n\nThe resulting Web Component\nclass AwesomeStuff extends HTMLElement {\r\n  constructor() {\r\n    super();\r\n    this.attachShadow({ mode: \"open\" });\r\n    this.render();\r\n  }\r\n\r\n  static get observedAttributes() {\r\n    return [\"value\"];\r\n  }\r\n\r\n  attributeChangedCallback(name, oldValue, newValue) {\r\n    if (name === \"value\") {\r\n      this.updateFace(newValue);\r\n    }\r\n  }\r\n\r\n  updateFace(value) {\r\n    const die = this.shadowRoot.querySelector(\"#die\");\r\n    if (value) {\r\n      switch (value) {\r\n        case \"1\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(0deg)\";\r\n        case \"2\":\r\n          return die.style.transform = \"rotateX(90deg) rotateY(0deg)\";\r\n        case \"3\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-90deg)\";\r\n        case \"4\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(90deg)\";\r\n        case \"5\":\r\n          return die.style.transform = \"rotateX(-90deg) rotateY(0deg)\";\r\n        case \"6\":\r\n          return die.style.transform = \"rotateX(0deg) rotateY(-180deg)\";\r\n      }\r\n    }\r\n  }\r\n\r\n  render() {\r\n    this.shadowRoot.innerHTML = `\r\n    <div id=\"die\">\r\n      <div class=\"face face-1\">1</div>\r\n      <div class=\"face face-2\">2</div>\r\n      <div class=\"face face-3\">3</div>\r\n      <div class=\"face face-4\">4</div>\r\n      <div class=\"face face-5\">5</div>\r\n      <div class=\"face face-6\">6</div>\r\n    </div>\r\n    <style>${this.css()}</style>\r\n  `;\r\n  }\r\n\r\n  css() {\r\n    return `\r\n      #die {\r\n          --die-size: 50px;\r\n          --face-offset: calc(var(--die-size) / 2);\r\n          position: relative;\r\n          width: var(--die-size);\r\n          aspect-ratio: 1 / 1;\r\n          font-size: var(--face-offset);\r\n          transform-style: preserve-3d;\r\n          transition: transform 1s ease-out;\r\n      }\r\n\r\n      .face {\r\n          position: absolute;\r\n          width: var(--die-size);\r\n          aspect-ratio: 1 / 1;\r\n          background: orange;\r\n          text-align: center;\r\n          font-size: var(--die-size);\r\n          line-height: var(--die-size);\r\n      }\r\n\r\n      .face-1 {\r\n          transform: rotateY(0deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-2 {\r\n          transform: rotateX(270deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-3 {\r\n          transform: rotateY(90deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-4 {\r\n          transform: rotateY(270deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-5 {\r\n          transform: rotateX(90deg) translateZ(var(--face-offset));\r\n      }\r\n      .face-6 {\r\n          transform: rotateY(180deg) translateZ(var(--face-offset));\r\n      }\r\n    `;\r\n  }\r\n}\r\ncustomElements.define(\"awesome-stuff\", AwesomeStuff);\r\n\nThis button generates a random value between 1 and 6, and applies it to the component:\n<awesome-stuff id=\"awesome\"></awesome-stuff>\r\n\r\n<button onclick=\"document.getElementById('awesome').setAttribute('value', Math.ceil(Math.random() * 6))\">roll</button>\r\n\n\nSo why are Web Components not mainstream yet?\nI know some big companies use them. A big bank in The Netherlands uses almost nothing else for their entire front-end, including the mobile app. They are production ready, awesome and well structured. The problem I see is that the big frameworks don\u2019t have an easy way to build them and don\u2019t easily steer you towards Web Components (probably because they want you to use their framework \ud83d\ude09). Like I mentioned, they are not a replacement of any framework because of missing features, so instead of using them natively a lot of projects fall back to frameworks.\nConclusion on Web Components\nWeb Components are awesome! You can further optimize this dice component and re-use it everywhere, or build something completely different and still use it everywhere! Web Components are great for your low-level components like custom buttons, form elements and other elements that have specific company-wide layouts. They are also great for isolated components such as wizards that guide users through something, complete forms that appear many times in your web application or even your entire website with the right tooling!\nHowever,\u00a0 I do recommend using a framework if you are going to create an entire web application with Web Components, such as Lit, to make sure you have an easier way to do lifecycle hooks, data management and testability. Lit is super small and very close to the native specification so everything I just explained, will work (better) using Lit and the components are equally well distributable if you do have Lit as a dependency. Check out this next codepen for a nice example of two Web Components, the dice with optional 4, 6, 8, 10, 12 and 20 sides and a dicetray, which are a little more complex.\n\nFurther recommondations:\nI built (better) dice and a dicetray, find it here. Looking for the NPM release of the dice component? Find that here. Want to share your thoughts? Get in touch with me via LinkedIn.\n", "tags": ["component", "front end", "frontend", "native", "ui", "ux", "web", "webcomponent", "webdevelopment"], "categories": ["Blog", "Concepting &amp; UX", "Development"]}
{"post_id": 39710, "title": "AI Ethics: Navigating the Landscape", "url": "https://www.luminis.eu/blog/ai-ethics/", "updated_at": "2024-01-29T08:58:17", "body": "Last year, some of the most influential tech gurus signed an open letter to pause AI development to develop AI ethics. The letter was signed by 20,000 people, including Elon Musk, Andrew Yang, and the philosopher Yuval Noah Harari.\nThe goal was to work together to develop and implement shared safety protocols for advanced AI design and development. These protocols would be rigorously audited and overseen by independent outside experts to ensure the safety of AI technology. What has been done since 12th April 2023? Were nine months enough to achieve that enormous task? Seriously, what is lying underneath all of this?\n\nIn this blog post, we dive into the origins of AI ethics, discuss whether AI can have ethics, and look into what the ethical concerns regarding AI are today.\nThe origins of AI ethics\nThe letter asks for AI systems to be \u201cmore accurate, safer, interpretable, transparent, robust, aligned, trustworthy, and loyal\u201d. Also, it asks for more governmental regulations, independent audits before training AI systems. It also points to the need for more \u201ctracking highly capable AI systems and large pools of computational capability\u201d. And finally more \u201crobust public funding for technical AI safety research\u201d.\nAll this belongs to the ethics domain, a philosophical discipline developed over the centuries, mainly by philosophers. After WWII, during the doctor\u2019s trial in Nuremberg, doctors were judged for using war prisoners for experiments without their consent. All of them were sentenced as guilty. Situations like this founded \u201capplied ethics\u201d which are codes or guidelines that help professionals behave according to what is expected.\nIf we want to specifically trace the origins of ethics in AI, we must talk about Isaac Asimov\u2019s famous Three Laws of Robotics (Runaround, 1942):\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given by human beings except where such orders would conflict with the First Law.\nA robot must protect its existence as long as such protection does not conflict with the First or Second Law.\n\nIf you read Asimov, you know how his stories revolve around how these simple laws can be conflictive. A lot has changed since he wrote this, but the need to regulate machine behavior remains. So, what are the main concerns today? How can companies benefit from applying ethics in AI systems?\nDoes AI have ethics?\nNo matter whether you are trying to provide a better recommendation system or creating a chatbot for hiring new employees. Your objective should be to provide the best service to your customers. But what does better mean? Should it just be faster no matter what? Should it run autonomously and decide what is better for its users?\nOf course, the need for a special ethic in each AI domain is crucial. It is not going to be the same principles while designing autonomous vehicles as when we are dealing with cybersecurity. Each sector will focus on different aspects, but we can also make sure that some of them will be common.\nIf we look at the history of ethics, there is one principle that has been followed since the very beginning, the no harm principle. The Hippocratic oath has been followed by doctors for over two hundred years. And even with updates, not harming directly or indirectly remains a top priority.\nThis aspect is key. We should be careful while we try to do things better, otherwise we could end up harming others and ourselves. Imagine you have a chatbot spreading misinformation. It won\u2019t only hurt people who receive that information but also the company\u2019s reputation.\nAs has been said hundreds of times before, there is no moral in technology itself. We must adapt discoveries to our social values.\nWhat are the main ethical concerns today?\nThere are four major concerns when applying AI technology for use in business.\nBias in AI:\nBiased data has a huge impact on AI training. We know that the system is going to be as good as the data used in its training. Stereotypic data can target certain groups as less trustworthy making it more complicated to access personal credits, etc.\nAs an example, we have face recognition systems that perform better if the user is male and white. This is a well-known problem and has been widely covered. In future posts, we will cover some practices to help you avoid these situations.\nTransparency of AI Systems:\nMany AI systems rely on Machine Learning algorithms that extract patterns from a given dataset during training. On top of that, these patterns can change with every update the system gets, so even if we find an explanation for a pattern, it can change or get outdated when new data arrives. This means that patterns are often hidden from the programmers or, in other words, not transparent. As I mentioned, the biased data on the system can remain hidden from the users and enforce it.\nPrivacy and surveillance:\nThis problem has been well-known for the last decade. Since the term Big Data was everywhere, people started paying attention to the data they were given free on the internet. I\u2019ve seen this on close friends that went from \u201cI have nothing to hide\u201d to \u201cI only turn on my location when I\u2019m using my GPS\u201d. Technology has changed significantly in the last decade, but the regulation responded slowly. The same is going to happen to AI.\n\u201cFree\u201d services like social networks are well known for exploiting the weaknesses of their users (procrastination, addiction, manipulation, etc.) for the extraction of personal data and maintaining their attention. We all know that big companies exploit this situation, sometimes hiding this fact. The occupation of \u201cinfluencer\u201d does not get the name for anything else than that.\nAI increases both the possibilities of intelligent data collection and the possibilities for data analysis. Among other problems, personal privacy and surveillance can be at risk with technologies that include device fingerprinting, face recognition, etc., and can lead to identification and search for individuals way more easily. These systems will often reveal facts about us that we wish to suppress or are not aware of like where have we been by making use of the GPS without notice.\nManipulation of behavior:\nAs I was explaining, AI problems in surveillance go further in the accumulation of data. It is quite common that the collected data is used to retain users\u2019 attention, which not only generates an addiction to their service but also manipulates the decisions that the users make online and offline.\nMisinformation is one of the main concerns that online platform users have. It is well known the case of Cambridge Analytica, which used social media as a propaganda platform to alter the vote of thousands of Americans in 2016. Of course, a lot has changed since then, but so has the sophistication of these systems. Today we deal with deep fake photos, videos, and phone calls so real that we have to include fact-checking as part of our daily routine.\nWhat are governments doing?\nThere are different ways to legislate a technology that is supposed to change everything. On one side we have the US vision, which doesn\u2019t put almost any obstacle in the development. On the other hand, there are countries like France or Germany that share the Europe vision to legislate AI more restrictively. Even the Vatican has its manual.You can check the AI Act, created by the EU, which defines some act guides depending on the risk the system supposes. \u201cUnacceptable risks\u201d like social scoring or cognitive behavioral manipulation of people will be banned, for \u201climited risk\u201d the AI system should comply with minimal transparency, allow the user to make informed decisions, etc. You can read the whole document here.\nIn October 2023, during the G7, Japan presented a different approach named the Hiroshima AI Process. This approach is a point in between the other two, but it\u2019s enough specific to focus on problems like copyright or the revelation of personal data. Some of the defined rules include:\n\nBefore and during deployment: identify, evaluate, and migrate risks.\nMitigate vulnerabilities and patterns of misuse.\nTransparency about limitations and/or inappropriate use.\nShare information responsibly with other organizations.\nImplement measures to protect personal data and intellectual property.\n\nConclusions\nAlthough AI is a relatively new tool, ethics for AI proposes an extension of data management in terms of ethics and legislation. If we do not consider AI systems as moral agents, and we still do not, the treatment of data that we should adhere to for our applications extends to the usage and regulations currently in place for GDPR data management.\nQuestions like who is collecting the data, why it is being collected, how long it will be stored, or with whom it will be shared, must be answered before using it. Let\u2019s say that you are using a chatbot that talks with your customers. If you want to store the conversations your users have with the chatbot, you should know how to handle that data, and maybe treat it as if it\u2019s sensitive content, inform the users, and even rethink why your company needs that data in the first place.\nTrying to set up some general rules or guidance is a complicated task, and as we saw with earlier technologies, general law restrictions come later than expected and sometimes even get outdated when published.\nIn the same way that we do not rely on government legislation to make our workspace safer or better for the environment, we must start doing this also to create better AI systems. Assume that you will be responsible for your AI system, so it will be your responsibility to double-check if your data is biased, collect just the relevant information, and most importantly, not harm its users.\nIf we as humans do not implement ethics within our AI designs, they will certainly not do it on their own, AI doesn\u2019t know how to, if we don\u2019t teach them.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39703, "title": "Easy on The Eyes: UX Design in Extreme Environments", "url": "https://www.luminis.eu/blog/easy-on-the-eyes-ux-design-in-extreme-environments/", "updated_at": "2024-02-13T20:58:46", "body": "If the design of a user interface is executed poorly, it will obviously lead to frustrated users. In some situations, consequences can be more dire, like the false missile warning on Hawaii in 2018. In this blog post, you\u2019ll find out why UX design for extreme environments is important and learn some tips and tricks for your next project.\nA while ago, I got involved as a designer at a company that puts oil pipes on the bottom of the ocean. The weak point of these oil pipes is where two pipes are welded together. These seams are welded several times aboard a ship, and are then scanned with ultrasonic sound. These scans are rendered in an user interface. An expert is checking these raw readouts, looking for air bubbles and other faults. Doing so in long shifts, on a ship at sea.\nObviously, I wanted to be sure that the interface was a fit for this high demanding environment. Besides making sure the experts make the right call each time, other challenges like eye strain and fatigue also need to be overcome. So, how do you limit eye strain and fatigue in task-focussed, high-stakes GUIs?\n\nPreventing eye strain\nActivities that require you to look at various details for a longer time will tire your eyes and its muscles. The result is eye strain: you have trouble to focus your vision, you feel pain in your eyes or get a headache. Unpleasant for sure, but if those details in the interface are critical and reading them correctly matters a lot, like UX design in extreme environments, it becomes problematic.\nTo relieve eye strain, you can do several things:\n\n\nUse legible fonts in a larger size\nLarger font sizes can be read better, even from a distance. It then also takes less effort for your eyes to follow and read lines in a paragraph. Choose a typeface that has a good legibility, like the Atkinson Hyperlegible font, which has a good readability by starkly differentiating similar letterforms. Larger font sizes could mean less content fits on a screen, especially on mobile devices, but that\u2019s not necessarily a problem:\n\nPrevent reading the whole screen\nPrimary details and components that help the person perform a task and make decisions should not be spread throughout the whole screen. Otherwise, this would mean eyes have to jump around the screen and refocus on each element to \u2018read\u2019 the screen. Decide what is important in each moment of the flow of your application, and put those items together.\n\nEnough contrast\nMake sure there is enough contrast between text and icons, and their background (but do not overdo it). If contrast is bad (grey text on lighter grey background for example), it\u2019s hard to read and it makes the user squint and refocus, leading to eye strain. Putting the contrast on par with W3C\u2019s AA-level standards even helps users with colorblindness, since good contrast is colorblind itself. So, take it up as a challenge: create more contrast while your inner graphic designer still loves the end result. There are several tools available for this, like LearnUI and Get Stark.\n\nAway from the screen\nEye strain can also be prevented with how the application is used. Perhaps you do have some control over this. In that case, make sure there are frequent breaks from looking at any screen. Other options are preventing screen glare by reducing overhead lighting, and making sure there\u2019s enough distance between the user and the screen for the correct viewing distance.\n\nDark mode:\nTurning on dark mode could reduce the brightness of a screen while contrast can still be on W3C\u2019s AA-level. So when using a screen for long hours it can definitely be more comfortable to switch to a darker interface, and it will reduce eye strain. Readability or reading errors are pretty similar for both light and dark mode as well. (It\u2019s good to know that dark mode is not more beneficial than light mode during the night, when considering sleep health.)\n\n\u00a0\nReducing cognitive load\nEverything you see, hear, feel and think about while using a product will use some brain power to make sense of it, and to decide what to do next to finish the task. This is called the cognitive load. The higher the load, the more tiresome the task.\nSo, limiting this cognitive load can be beneficial for the application\u2019s usability: a person can work longer with it, stay focused for longer, and make less mistakes.\nBe aware that context has a big impact as well. Consider an application that will be used on a factory floor. The noise and distractions of the environment are adding to the cognitive load, before we even consider the use of the application. You have to design with that context in mind and do user tests in those conditions to verify of your solution is usable on that factory floor.\nThere are several ways to reduce cognitive load:\n\n\nFamiliar and intuitive interactions\nCreate a solution with predictable interactions and layouts. Many typical UI challenges have evolved into design patterns throughout the years. These familiar design patterns make an application obvious and intuitive in use, so less brain power wasted. Be aware that the domain you\u2019re working in can have it own unique design patterns.\n\nReduce visual noise\nConsider each screen a person will see and use to perform a task. Some items are crucial for completing that task while others are supportive or only for edge cases.\nCreate a hierarchy to show that difference (by using visual design principles for example). Take a step further by removing everything that is not important, or by moving it to another screen or slide-in.\n\nSimplify and remove ambiguity\nLike reducing visual noise, you can reduce interactive noise as well. Show less information, reduce interactions and limit navigation steps. Hick\u2019s Law states that the time it takes for a person to make a decision will increase with more (equally probable) options. So, limit those options.\nAnd make sure the navigation is easy and requires little contemplation. Navigation is not where you want to do new and unexpected things in critical environments.\nSimplifying does not have a minimal design. It is about taking away ambiguity. Use shorter and more to the point texts, and use everyday words that fit the domain you\u2019re working in. Sometimes, an illustration or additional information is very helpful.\nIcons can be hard to understand, so add labels. And make them more readable by adding meaning and contrast, so each is recognizable in a glance. A good example provided by MegDraws:\n\n\nOffloading tasks:\nOffloading tasks is taking it one step further. Let the application understand or remember details, or make a decision. Not asking for an input will limit the cognitive load of the person.\n\n\u00a0\nDesign against fatigue\nEye strain and limited cognitive load are specific symptoms of fatigue. A person is tired, cannot concentrate, and starts making mistakes. The right UX design in extreme environments can make huge differences. The suggestions mentioned above will help you to reduce symptoms like fatigue and eye strain. As a result, the user is less likely to make mistakes that are related to the user-experience of applications.\nIf fatigue really is an obstacle, consider solutions that go beyond what\u2019s on the screen. Create more variation in tasks or work, as monotonous work will lead to less focus. Let the person have short breaks so they can go on a walk. Even drinking water and having bright lights helps.\nWhen you work on a project that is more task-focused, in a high-stakes context, consider the suggestions above. Go a bit further, add wiggle room and don\u2019t go for \u201cjust enough\u201d. And while designing and developing this user interface, test it, and test it again, as realistically as possible. Make sure your end-result won\u2019t lead to the project\u2019s equivalent of a false inbound missile alarm.\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 39675, "title": "GenAI, It\u2019s Happening Now", "url": "https://www.luminis.eu/blog/genai-its-happening-now/", "updated_at": "2024-01-17T12:25:00", "body": "AI is all around us, and the rise of Generative AI, also known as GenAI, is increasingly being discussed. Numerous articles have been written about it, but one pressing question remains: Is GenAI already usable within my business, or is it just a fun gadget?\nIn this blog post, we explore the potential of GenAI. Together, we investigate the current possibilities, how we already use GenAI for our clients and the associated risks. The goal is to inspire you to think about the impact of GenAI on your business and your daily activities and to provide you with practical tools. Together, we can explore how you can benefit from GenAI.\n\nDALL E created an advertisement poster that convinces business people to start using Generative AI.\nGenAI, a short introduction\nGenAI is the abbreviation for Generative Artificial Intelligence. AI is a broad field where computers perform tasks we do not want or cannot do due to size, speed, or accuracy. Much of AI is used to find data patterns to make predictions and group or compare data. AI has been applied for decades. Consumers came into contact with it through chess computers, voice assistants, self-driving cars, and much more.\nGenerative Artificial Intelligence\nGenerative AI is a particular area of AI where a system converts questions or commands into new data or content. The general public came into contact with GenAI when the company OpenAI launched the product ChatGPT at the end of 2022. ChatGPT allowed us to generate larger pieces of text and create summaries. It became even more interesting when people started using it to go from one language to another. You could even go from an ordinary language to a programming language. This made it possible to write an SQL query based on natural language.\nReview all invoices from the past month, group them by product categories, and calculate the total revenue and the average revenue per category.\nBesides text and, thereby, programming languages, the next step was the generation of imagery. By utilising the technology of Stable Diffusion, it became possible to create visual material. Tools such as DALL-E and Midjourney can create an image based on your question or command. The image above this article was also generated using a prompt.\nCreate an advertisement poster that convinces business people to start using Generative AI, do NOT use text characters in the image.\nEen deepfake foto van Jettro als een Cowboy\nIn addition to static images, there are possibilities to generate moving images. This generation of imagery is now also known as Deepfakes. I have also experimented with it myself; suddenly, I became a rugged cowboy from the movie The Good, the Bad, and the Ugly.\nAll this technology is also finding its way into the business world. In the next part of this blog, we look at applications and the possible consequences of these applications for your work.\nApplications of GenAI in a Business Context\nWithin the business world, GenAI is received differently. Many companies need a strategy for using tools like ChatGPT. This leads to uncontrolled growth within a company, making it unclear who uses it. Let alone that everyone understands the potential risks, for example, which data is or is not used to train the model better. This poses the risk that private data may become available through the model. Proper use of GenAI tools does provide a performance boost to employees. If we, for now, forget about the risks for a moment and focus primarily on the possibilities, what are the typical applications we encounter?\nCustomer contact with Large Language Models\nConsider the initial contact with customers through a chatbot and answering the phone using voice technology. Those recordings with \u2018press option 1, press option 3, press the hash key\u2019 can be replaced by an AI Bot that listens to the question and converses with a person on the phone. If the bot can\u2019t handle it, or if the person doesn\u2019t want to talk to a bot, the person only needs to indicate this, and they will be transferred to a natural person instead of the bot. For the company, this means flexibility and not spending time developing such an option menu for the phone. This is possible with tools like OpenAI\u2019s Whisper and Elevenlabs.\nBy leveraging the capacity of Large Language Models, you can engage in different forms of interaction with your users. Imagine you are a company offering people an unforgettable vacation experience. Then, you can let your users search and filter by choosing a country, type of vacation, number of people, and many other filter options. Alternatively, you can start a conversation with them through chat. Let them describe the kind of vacation they want. You can create an intermediary step by automatically selecting filters based on the text so the user can adjust.\nI want to vacation with two adults for three weeks sometime in August-September. I want to go to the mountains. I do not need warm weather. Besides the beautiful views, I also want to relax occasionally. I prefer apartments, but sometimes a hotel is also acceptable. I want a maximum of 6 locations to propose my vacation.\nGenAI for developers\nAs a developer, you have to maintain an old application. You program everything in Rust or Python these days, and now you have to work with Java. It\u2019s been a while, and you need help understanding everything. You start your development environment, select a piece of code, and ask what that code does. It\u2019s been explained to you correctly. You need to migrate the code to Java 21, request a suggestion, paste the suggestion into your code, and run all the unit tests again. If you get an error, you ask what this error means and how you can fix it. A developer can do this with a tool like GitHub Copilot and JetBrains AI Assistant.\nWe could go on like this for a while. You will also start to see more and more tools adding suggestions that use AI. Microsoft is also working hard on an assistant that is permanently present in all your Office Tools. Then, you can ask Excel to create a formula in a cell that calculates the average and standard deviation of all columns D to K. Or ask PowerPoint to create a presentation about last year\u2019s figures with the Excel file as input.\nIs it all fantastic, and should everyone dive in headfirst? Are there no risks? We will discuss this in the next part of this blog.\nThe risks of GenAI\n\nUsed from euro parliament document.\nThe use of Generative AI has risks. The EU is busy developing legislation around the use of AI. But what are these risks? The risks exist on several levels. First, I will examine the legislation the EU is working on. Here, it mainly concerns the use, transparency, fairness, and privacy.\nIn the EU AI Act, careful consideration has been given to the risk of AI to society. Four levels of risks have been identified. In the highest layer, the risk is unacceptable. Therefore, it is prohibited by law. An example is the use of real-time facial recognition in public places.\nAnother example is the exploitation of people from vulnerable groups through AI. The next layer is High Risk. In this case, mandatory registration of the system is necessary, along with certification. You must prove that the system meets the established requirements regarding risk management, testing robustness, transparency, and safety. Examples of applications are toys, aircraft, vehicles, and medical devices. The other two layers have fewer requirements.\nThe discussion about the legal definition of AI is interesting. Some find it too broad, thereby hindering innovation, while others believe the opposite.\nRisks of ChatGPT and Co-pilot\nAnother risk I want to discuss is using services like ChatGPT from OpenAI and Co-pilot from Github. These tools work with a Large Language Model (LLM) and a prompt. The prompt contains the question and the context. This context can include data you do not want publicly available. An LLM is a model trained on a lot of data. Especially in the early days of the popularity of these tools, it was unclear whether your data was being reused to improve the model or to train it.\nI looked into the policies of both tools regarding the use of the data sent with them. Your data is either used by default or not, depending on your subscription. But in both cases, you can turn off the use of your data for training their model. The business subscriptions do not use your data by default, and the individual subscriptions allow you to turn it off. Free subscriptions do not give you that option. Therefore, it is essential that you thoroughly investigate each tool to understand how you can use it without sharing your data and without using it to improve the model.\nNow that you have an idea of the possibilities of GenAI for your business and insight into the risks, how can you get started? That is the topic of the next part of this blog.\nStarting with GenAI\nYou can, of course, wait to implement AI, and particularly GenAI, within your company. Wait until the legislation is finalised. Wait until everyone is doing it. By then, you\u2019ll probably be too late. It\u2019s much better to start now. Begin small, and start playing with tools that are already available. Let yourself be inspired by what already exists. Do not dive in headfirst without thinking. Start with a strategy and policy for using GenAI within your company. Help the people within your company to use it safely.\nTips and tricks\nWhen you\u2019re comfortable taking the next step, moving beyond just playing around, investigate where you could make the most impact. Assess the risk of AI for that specific situation. Use the layered model according to European legislation. Implementing GenAI or AI within your organisation requires knowledge. Knowledge about processing and storing data. For AI, too, the principle \u2018Garbage in, Garbage out\u2019 applies.\nFor GenAI, the rule \u2018Garbage in is garbage out\u2019 also applies. Invest in your data platform.\nWhile you are working on establishing your internal rules and policies around GenAI, you can already start with content creation. You can test your steps in setting up your policy right away. You will notice that by using the existing tools, on the one hand, you will use them more and more, and on the other hand, more and more questions about policy will arise.\nGenAI can also be used effectively to inspire internal workshops. For instance, part of a planning meeting could involve discussing ideas generated by GenAI. You can see it as an additional team member who can think creatively, not as a replacement.\nThen, proceed to look for repetitive steps in business processes. Due to the repetition, this is a good entry point where GenAI can take over. We have already been able to implement and set up GenAI successfully.\n", "tags": ["Generative AI"], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39574, "title": "Convert a REST API to AWS Lambda with minimal effort using Quarkus", "url": "https://www.luminis.eu/blog/convert-a-rest-api-to-aws-lambda-with-minimal-effort-using-quarkus/", "updated_at": "2024-01-17T09:31:24", "body": "In this blog post, you learn how to convert a REST API to an aws Lambda without any refactoring. The application is already using the Quarkus framework, but with an API based on RestEASY Reactive. Keep reading to find out why you might need the conversion , how I utilized AWS Lambda with AWS\u2019s http gateway and learn about the benefits of saving costs and development time.\nPlaying darts at the office\nWe have a nice side-project at the office, a simple app that we use when playing dart games. Recently, I took upon myself to add a leaderboard to the app. In short, the back end application would be used to save game data and serve high score lists. Because I already loved the Quarkus framework and wanted to show its power to my colleagues, I decided to use Quarkus for this task.\n\nThe idea was simple: Accept different types of game data (for different game modes that the web app provides) and store it in a DynamoDB table. Have some endpoints to query data for different types of games or for specific players, and there you have it.\nI already wrote a functioning application when a colleague asked: \u201cWhy not make it in the form of Lambdas?\u201d Since the application would only be used sporadically, it would be a waste of money to have an EC2 instance running continuously. Response time is not of much importance in this case, and cold start response times could be mitigated by compiling a native executable using GraalVM native image, so a serverless solution seemed ideal.\nWhich Quarkus extension to use?\nThere are multiple Quarkus extensions available for AWS Lambda development, each with their own advantages and drawbacks:\n\nquarkus-amazon-lambda: the bare-bones Lambda development kit;\nquarkus-funqy-amazon-lambda: Lambda binding for the cloud agnostic funqy library;\nquarkus-amazon-lambda-http: develop Lambdas to deploy behind an aws http gateway;\nquarkus-amazon-lambda-rest: develop Lambdas to deploy behind an aws API gateway.\n\nSince I had already written the application, extensions number 1 and 2 would require lots of refactoring because each function needs their own Quarkus project. The extensions use a shared code imported as a dependency (unless you have no problem with hacking the framework to change this behavior).\nExtensions 3 and 4 allowed me to keep my application code as it was. My resource (or controller) classes looked like any other resource class.\u00a0 However, it would, with a simple command, be deployed as a single lambda, along with a gateway that handles incoming requests. This doesn\u2019t only work for RestEASY Reactive, but for any http framework offered by Quarkus such as Undertow,\u00a0Reactive Routes,\u00a0Funqy-HTTP\u00a0or\u00a0Spring Web API. \u00a0For this case, I chose the quarkus-amazon-lambda-http extension because it didn\u2019t need the extra functionality provided by the API gateway. Additionally, the HTTP gateway comes with lower latency at a lower price point.\nHow to convert a REST API to AWS Lambda?\nWhen converting a REST API to AWS Lambda, I recommend to consider the following methods. This is an example of an API built on RestEASY, which is Quarkus\u2019 default extension for http servers. There are small differences between the classic and the reactive versions of RestEASY. However, the code below is valid for both. If you know Jakarta, this code might also look familiar, because the annotations (as well as the MediaType\u00a0and\u00a0Response\u00a0classes) all come from the\u00a0jakarta.ws.rs\u00a0package.\n// 1\r\n    @GET\r\n    @Path(\"hello\")\r\n    public String hello() {\r\n        return \"hello, world\";\r\n    }\r\n// 2\r\n    @GET\r\n    @Path(\"hello/{input}\")\r\n    public String hello(String input) {\r\n        return \"hello, \" + input;\r\n    }\r\n// 3\r\n    @POST\r\n    @Path(\"hello\")\r\n    public String hello(MyClass myObject) {\r\n        return \"hello, \" + myObject.getValue();\r\n    }\nWhen you include the\u00a0quarkus-amazon-lambda-http\u00a0extension in your POM file and deploy your application as a Lambda, you can just send http requests to your\u00a0gateway url. Then you can use paths and pass json objects like you normally would for any REST API. As a result, all of the methods above are still accessible. For instance, method number 2 can be invoked with a GET request to https://{restapi_id}.execute-api.{region}.amazonaws.com/{stage_name}/hello/Luminis\u00a0and should return \u201chello, Luminis\u201d.\nIf you want to test your Lambda directly via the aws Lambda test console, you should wrap your http request in a Lambda request like so:\n{\r\n  \"body\": \"{ \\\"myValue\\\" : \\\"lambda\\\"}\",\r\n  \"resource\": \"/{proxy+}\",\r\n  \"path\": \"/hello\",\r\n  \"httpMethod\": \"POST\",\r\n  \"isBase64Encoded\": false\r\n}\r\n// Output: \"hello, lambda\"\nThis is how the gateway passes requests to your Lambda. Notice that the value of the body parameter has quotes around it and that the inner quotes escaped. This is because, even though we are including a json object, the body must be provided as a string.\nWait, that\u2019s it? All we did was add a dependency and it just works?\nShort answer: Yes!\nLonger answer: Yes, but you need to have your\u00a0AWS credentials configured correctly\u00a0and you need the\u00a0SAM cli\u00a0to test and deploy your lambda.\nDeploying and testing\nTo build our application, we can use the maven or gradle wrapper that comes with your Quarkus project. However, we\u2019re going to use the Quarkus cli. Run the following command in the root directory of your project:\nquarkus build --native --no-tests -Dquarkus.native.container-build=true\nthe\u00a0--native\u00a0flag tells the cli to build a native image using GraalVM. When you use the\u00a0-Dquarkus.native.container-build=true flag, you don\u2019t need to have GraalVM installed. However, you need to have Docker\u00a0or\u00a0Podman running. Your machine automatically pulls a Docker image and spins up a container, which then builds your Lambda! This is great because GraalVM native image normally only allows you to build binaries for your OS. Instead, the Docker image is always Linux, which is what we need. If you\u2019re on Linux and have GraalVM configured correctly, you can just run quarkus build --native. If you\u2019re wondering why we\u2019ve added an --no-tests\u00a0argument, it\u2019s because the test code is not included in the build. You can have the tests run separately in your pipeline, but that\u2019s a story for another time.\nNext, we can use the SAM cli to deploy on a local environment on docker for testing:\nsam local start-api --template target/sam.native.yaml\nOr we can deploy on AWS Lambda:\nsam deploy -t target/sam.native.yaml -g\nConsiderations\nWhile it\u2019s convient to convert a REST API to AWS Lambda, it\u2019s possibly considered a better practice to split your application into multiple Lambdas for the sake of atomicity and performance. However, for simple applications, the pros might outweigh the cons. Especially because deploying a native image reduces startup time drastically.\nOne more thing to consider is that, apart from getting billed for the lambda\u2019s invocations and computing time, you\u2019ll also get billed for the gateway. Luckily, the pricing for the http gateway is only $1 per million requests for the first 300 million requests. It get even cheaper if you go over that threshold.\nConclusion on how to convert a REST API to AWS Lambda\nAny Quarkus-based http-server can be converted to a single AWS Lambda by simply adding the quarkus-amazon-lambda-rest\u00a0or\u00a0quarkus-amazon-lambda-http\u00a0extension to the POM file. Just make sure you have the required aws tools installed and configured on your machine and build a native image using GraalVM if you don\u2019t want unacceptable startup times. It can save you, your company, or your customer serious money if it means getting rid of a mostly idle EC2 instance and requires no refactoring on your server code at all.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 39528, "title": "AWS Multi-Account GitOps Deployment 2: GitHub Actions Setup", "url": "https://www.luminis.eu/blog/aws-multi-account-gitops-deployment-2-github-actions-setup/", "updated_at": "2024-02-08T12:31:56", "body": "Welcome back to the second installment of our series on multi-account GitOps deployment on AWS. In the first part, we navigated through setting up a multi-account AWS organizational structure. Now, we will focus on integrating GitHub for our deployment processes and ensuring all the accounts are ready for deployments.\nIntegrating AWS Accounts with GitHub\n1. Access Management Setup:\n\nLog in to the Management Account: Navigate to AWS and sign in to your management account.\nGo to IAM Identity Center: Once you are logged in, access the IAM Identity Center.\nAccess AWS Accounts: Under IAM Identity Center, find and click on AWS Accounts.\n\n\n\nAssign Users or Groups:\u00a0Select all the newly created accounts (deployment, dev, test, prod) and click on \u201cAssign Users or Groups\u201d.\nSelect the Main User:\u00a0Choose the main user and click next.\nSelect the Permission Set:\u00a0Now, select the Administrator Access permission set (or any other relevant permission set you have created) and click next.\nSubmission:\u00a0Review your configurations and click submit\n\n\n\nVerify Account Access:\u00a0You should now be able to see all the accounts listed in your AWS Access portal at\u00a0https://lutku.awsapps.com/start#/\n\n\n2. AWS CLI and SSO Configuration:\nFor all the newly created accounts,\u00a0add them to the same session name using AWS CLI as we have done in the first instalment of the series:\naws configure sso\r\n\nFollow the prompts and perform this action for all the accounts\u00a0(deployment,\u00a0dev,\u00a0test,\u00a0prod).\u00a0Example session names could be:\n\nlutku-deployment\nlutku-dev\nlutku-test\nlutku-prod\n\n3. GitHub Actions Setup:\nNow,\u00a0let\u2019s set up our repository for AWS CDK and GitHub Actions:\nmkdir actions-setup && cd actions-setup\r\ngit init\r\nnpx projen new awscdk-app-ts\nUpdate the\u00a0.projenrc.ts\u00a0configuration file similar to following:\n\nimport { awscdk } from 'projen';\r\n\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n   authorEmail: 'utku.demir@luminis.eu',\r\n   authorName: 'Utku Demir',\r\n   cdkVersion: '2.96.2',\r\n   defaultReleaseBranch: 'main',\r\n   name: 'actions-setup',\r\n   description: 'A CDK project for GitOps Deployments',\r\n   github: false,\r\n   projenrcTs: true,\r\n   keywords: [\r\n      'AWS CDK',\r\n      'projen',\r\n      'Typescript',\r\n      'Deployment',\r\n   ],\r\n   gitignore: ['.idea'],\r\n   license: 'MIT',\r\n   licensed: true,\r\n});\r\nproject.synth();\r\n\nAfter updating,\u00a0generate the project:\nyarn projen\nCreate a new file named\u00a0actions_setup_stack.ts\u00a0under\u00a0src\u00a0to include the necessary configuration for our stack:\n\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as iam from 'aws-cdk-lib/aws-iam';\r\nimport { Construct } from 'constructs';\r\n\r\nexport interface ActionsSetupStackProps extends StackProps {\r\n  repositoryOwner: string;\r\n  gitDeployableAccounts: string[];\r\n}\r\n\r\nexport class ActionsSetupStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: ActionsSetupStackProps) {\r\n    super(scope, id, props);\r\n\r\n    const githubOidcProvider = new iam.OpenIdConnectProvider(this, 'github-oidc-provider', {\r\n      url: 'https://token.actions.githubusercontent.com',\r\n      clientIds: ['sts.amazonaws.com'],\r\n      thumbprints: ['6938fd4d98bab03faadb97b34396831e3780aea1'],\r\n    });\r\n\r\n    const webIdentityPrincipal = new iam.WebIdentityPrincipal(githubOidcProvider.openIdConnectProviderArn, {\r\n      StringEquals: {\r\n        'token.actions.githubusercontent.com:aud': 'sts.amazonaws.com',\r\n      },\r\n      StringLike: {\r\n        'token.actions.githubusercontent.com:sub': `repo:${props.repositoryOwner}/*`,\r\n      },\r\n    });\r\n\r\n    new iam.Role(this, 'github-actions-deploy-role', {\r\n      roleName: 'github-actions-deploy-role',\r\n      assumedBy: webIdentityPrincipal,\r\n      inlinePolicies: {\r\n        AllowCrossAccount: new iam.PolicyDocument({\r\n          statements: [\r\n            new iam.PolicyStatement({\r\n              effect: iam.Effect.ALLOW,\r\n              actions: ['sts:AssumeRole'],\r\n              resources: [\r\n                `arn:aws:iam::${this.account}:role/cdk-hnb659fds-*-role-${this.account}-*`,\r\n                ...props.gitDeployableAccounts.map(appAccount => `arn:aws:iam::${appAccount}:role/cdk-hnb659fds-*-role-${appAccount}-*`),\r\n              ],\r\n            }),\r\n          ],\r\n        }),\r\n      },\r\n    },\r\n    );\r\n  }\r\n}\r\n\nThe above stack,\u00a0based on the work of Wojciech Matuszewski on Deploying AWS CDK apps using short-lived credentials and Github Actions,\u00a0creates a web identity principal for the GitHub OIDC provider and a role for this web identity principal to assume to be able to conduct deployments.\nDifferent from the stack that deployed the organization structure,\u00a0I recommend creating a file named\u00a0cdk.context.json\u00a0in the root of the project to store the configuration necessary for this stack.\n\n{\r\n  \"repositoryOwner\": \"<owner_of_the_repository>\",\r\n  \"gitDeployableAccounts\": [\r\n    \"<dev_account_id>\",\r\n    \"<test_account_id>\",\r\n    \"<prod_account_id>\"\r\n  ]\r\n}\r\n\nLastly,\u00a0edit the main.ts under src to include this stack as:\n\nimport { App } from 'aws-cdk-lib';\r\nimport { ActionsSetupStack } from './actions_setup_stack';\r\n\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\nconst app = new App();\r\n\r\nconst repositoryOwner = app.node.tryGetContext('repositoryOwner');\r\nconst gitDeployableAccounts = app.node.tryGetContext('gitDeployableAccounts');\r\n\r\nnew ActionsSetupStack(app, 'actions-setup-stack', {\r\n  env: devEnv,\r\n  repositoryOwner: repositoryOwner,\r\n  gitDeployableAccounts: gitDeployableAccounts,\r\n});\r\n\r\napp.synth();\r\n\n4. Bootstrapping and Deployment:\nNow you\u2019ll need to bootstrap your AWS accounts to prepare them for CDK deployments.\nStart with the deployment account:\n\ncdk bootstrap aws://<deployment_account_id>/eu-west-1 --profile lutku-deployment\r\nyarn deploy --all --profile lutku-deployment\r\n\nNavigate to the AWS Management Console,\u00a0go to the IAM Roles in the deployment account,\u00a0and find the\u00a0github-actions-deploy-role.\u00a0Copy the role\u2019s ARN for the next steps.\n\nFor each environment\u00a0(dev,\u00a0test,\u00a0prod),\u00a0run the following commands:\n\ncdk bootstrap aws://<env_account_id>/eu-west-1 --profile lutku-<env> --trust <deployment_role_arn> --cloudformation-execution-policies 'arn:aws:iam::aws:policy/AdministratorAccess'\r\n\nReplace\u00a0<env_account_id>\u00a0with the respective account ID,\u00a0<env>\u00a0with the environment\u00a0(dev,\u00a0test,\u00a0prod),\u00a0and\u00a0<deployment_role_arn>\u00a0with the role ARN copied earlier.\n5. Verification:\nAfter running the above commands,\u00a0navigate to the CloudFormation console in each environment account\u00a0(dev,\u00a0test,\u00a0prod).\u00a0You should see a new stack created by the CDK bootstrap command.\u00a0This confirms that the accounts are now ready for deployments using GitHub Actions.\n\nConclusion\nAnd that\u2019s it!\u00a0You have successfully set up and integrated your AWS accounts with GitHub for GitOps deployments.\u00a0By following these steps,\u00a0you have created a seamless and efficient workflow for deploying your applications across multiple AWS accounts.\nRemember,\u00a0while this setup provides a robust starting point,\u00a0always continue to explore and adapt to the ever-evolving cloud landscape.\u00a0Stay tuned for the final installment of this series,\u00a0where we will delve deeper into advanced GitOps strategies and best practices.\nReferences\n\nAWS Multi-Account GitOps Deployment: Organizational Setup\nDeploying AWS CDK Apps Using Short-lived Credentials and GitHub Actions\n\nHappy Cloud Engineering and until next time!\n", "tags": ["aws", "aws cdk", "CI/CD", "cloud", "github", "GitHub Actions", "iam", "infrastructure as code", "OIDC"], "categories": ["Blog", "Cloud"]}
{"post_id": 39480, "title": "LLM Series, part 1: A Comprehensive Introduction to Large Language Models", "url": "https://www.luminis.eu/blog/llm-series-part-1-a-comprehensive-introduction-to-large-language-models/", "updated_at": "2023-12-22T09:47:14", "body": "Large language models (LLMs) are all the buzz these days. From big corporations like Microsoft enhancing their office products to Snapchat having an assistant for entertainment, to high schoolers trying to cheat their assignments. Everyone is trying to incorporate LLMs into their products, services, and workflow.\nWith the surge in popularity, there\u2019s a flurry of discussions, blogs, and news articles about fine-tuning these models and their myriad applications. Even if you\u2019ve dipped your toes into the LLM pool, you might find yourself stumbling upon unfamiliar terms and concepts.\nIn this three-part blog series, we\u2019ll map out all the key concepts related to LLMs, so you can finally understand what your Machine Learning (ML) enthusiast colleague is talking about, but also potentially incorporate these powerful models into your projects. So, buckle up, and let\u2019s dive into the fascinating world of Large Language Models!\nPrior Knowledge\nBefore we dive deeper into the key LLM concepts, it\u2019s helpful to cover some foundational background knowledge. This will ensure we\u2019re all on the same page as we explore the intricacies of LLMs.\nTokens\nLet\u2019s start with a simple question: how would you split a sentence into words? Seems straightforward, right? But what if the sentence uses contractions like \u201ccan\u2019t\u201d or \u201cI\u2019m\u201d? And what if we switch to a different language, say, Swedish or Polish? Would you split it the same way?\nThis is where the concept of \u201ctokenization\u201d comes into play. It\u2019s all about splitting text into smaller, discrete units (or \u201ctokens\u201c), preferably, in a reversible way. This provides a neat, organized way for our models to process the text.\nOne of the key properties of tokens is that they belong to a fixed-size set, aptly named the \u201cvocabulary\u201c. This makes them much easier to work with mathematically. Each token can be represented by its unique ID in the set or as a one-hot vector.\n\nThe document is tokenized and one-hot encoded producing a fixed-size matrix of vectors. These vectors are fed through a function that transforms them into embeddings, effectively reducing the dimensionality.\nIn the olden days, tokenizers were quite lossy. It was common to work on stemmed words and only consider a set of the most common words. Modern tokenizers, instead, have evolved to focus on efficiency and losslessness. Instead of encoding whole words, algorithms such as Byte Pair Encoding (BPE) take a compression-like approach by breaking words apart.\nVocabulary construction is done in a purely data-driven manner, resulting in token splits that make sense semantically, such as the common verb ending \u201c-ing\u201d. Words like \u201cworking\u201d, \u201ceating\u201d, and \u201clearning\u201d all share this ending, thus an efficient encoding is to give \u201c-ing\u201d its own token. Some splits don\u2019t make sense, producing semantically dissimilar tokens such as \u201clab-elling\u201d which requires the model to do more work to infer its true meaning.\n\nOpenAI\u2019s `cl100k_base` tokenizer encoding a sentence. In this case, 100k refers to its vocabulary size.\nBut what if a word doesn\u2019t exist in the vocabulary? Like \u201cw0rk1ng\u201d? In this case, the tokenizer breaks it down into smaller chunks, sometimes even character by character.\nNow, it\u2019s tempting to assume that a token corresponds to a word. And while that\u2019s often the case, it\u2019s not a hard and fast rule. For simplicity\u2019s sake, we\u2019ll often use the terms \u201cword\u201d and \u201ctoken\u201d interchangeably in this series. But remember, in the wild world of tokenization, a token could be anything from a whole word to a single character or a common word part.\nFor ballpark estimates of token count, you can multiply your word count by 1.25 (assuming English text). This gives a reasonable approximation of the number of tokens in a given piece of text. However, if you\u2019re looking for a more precise estimate, you can use the OpenAI tokenizer web tool.\nToken Embeddings\nNow that we\u2019ve got our tokens, we need a way to represent them that captures more than just their identity. We\u2019ve seen that tokens can be represented as one-hot vectors, which are great for basic math but not so helpful when it comes to comparing different words together. They don\u2019t capture the nuances of language, like how \u201ccat\u201d and \u201cdog\u201d are more similar to each other than \u201ccat\u201d and \u201ccar\u201d.\nEnter the concept of \u201ctoken embeddings\u201c, also known as \u201cword vectors\u201d. The seminal paper Word2Vec was instrumental in bringing this concept to the mainstream. The authors built on two key assumptions from prior work:\n\nSimilar words occur in the same context (a concept known as Distributional Semantics).\nSimilar words have similar meanings.\n\nIt\u2019s important to note that these two assumptions are distinct. The first is about the context in which words are used, while the second is about the meanings of the words themselves.\n\nDemonstration of Linear Relationships Between Words Visualized in Two Dimensional Space. Image from Google Blog\nAt first glance, \u201csimilarity\u201d might seem like a subjective concept. But what if we think of words as high-dimensional vectors? Suddenly, similarity becomes a very concrete concept. It could be the L2 distance between vectors, the cosine similarity, the dot product similarity, and so on.\nIn the Word2Vec paper, the authors used gradient descent techniques to find embeddings for each word. The goal was to ensure that the above assumptions hold true when comparing these embeddings. In other words, they wanted to find a way to represent words as vectors in a high-dimensional space such that similar words (in terms of context and meaning) are close together in that space.\nThis was a game-changer in the field of natural language processing. Suddenly, we had a way to capture the richness and complexity of language in a mathematical form that machines could understand and work with.\nBut the real beauty of these embeddings is that they can capture relationships between words. For example, the vector difference between \u201cking\u201d and \u201cqueen\u201d is similar to the difference between \u201cman\u201d and \u201cwoman\u201d. This suggests that the embeddings have learned something about the concept of gender.\nHowever, it\u2019s important to remember that these embeddings are not perfect. They are learned from data, and as such, they can reflect and perpetuate the biases present in that data. This is an important consideration when using these embeddings in real-world applications.\nEncoders and Decoders\nWhile methods like Word2Vec are great for creating simple word embeddings, they produce what we call \u201cshallow embeddings\u201d. In this context, shallow means that a matrix of weights is trained directly, and thus can be used like a dictionary. As such, the number of possible embeddings is equal to the number of tokens in your vocabulary. This works fine when you\u2019re dealing with individual words, but it starts to break down when you\u2019re working with sentences or whole documents.\nWhy? Well, if you encode all the tokens in a sentence into embeddings, you lose all the order of the words. And as anyone who\u2019s ever played a game of \u201cMad Libs\u201d knows, word order is crucial when it comes to making sense of a sentence. By losing the order, you also lose the context, which can drastically change the meaning of a word.\n\nThe encoder produces a document embedding by combining the individual word embeddings.\nTo overcome this limitation, we need an additional model, the \u201cEncoder\u201d, which does some neural net math magic to create an embedding that takes both context and order into account.\nOn the other side of the equation, we have the \u201cDecoder\u201d. Its job is to produce a token from the input, which is typically a latent vector.\n\nVisualization of a encoding a sequence of tokens embeddings into a single latent representation, and decoding it into a sequence of token probabilities.\nThe architecture of how Encoders and Decoders work can vary greatly. It can be based on Transformers, LSTMs, or a combination of both. We\u2019ll dive deeper into these architectures in a later blog.\nOne interesting thing to keep in mind is that, since encoders and decoders operate in latent space, their input is not limited to text. They can also take embeddings produced from images, audio, and other modalities. This is thanks to innovations like CLIP, which are trained on multimodal tasks by introducing an encoder for each data type.\n\nNExT-GPT with text, image, audio, and video modalities.\nModeling Methods\nThe architecture of large language models isn\u2019t the only factor that gives them an edge. How they model natural language processing tasks also contributes greatly to their performance. Rather than taking a one-size-fits-all approach, large language models specialize in different modeling methods optimized for certain tasks.\nCausal Language Models (CLM)\nCausal Language Models (CLMs) are trained with an autoregressive objective, which is a fancy way of saying they\u2019re trained to predict the next token in a sequence based solely on the previous tokens.\nCLMs typically work in an unidirectional manner, meaning the next token depends only on the previous tokens. It\u2019s a bit like reading a book \u2013 you don\u2019t know what\u2019s coming next until you\u2019ve read what\u2019s come before. Their architecture reflects this, as CLMs are typically decoder-only.\nBecause of their autoregressive nature, CLMs are great for tasks like text (and code) completion, chat, and story writing. Examples of CLMs include Generalized Pretrained Transformer (GPT), and its derivatives, such as Meta\u2019s Llama.\nMasked Language Models (MLM)\nOn the other hand, Masked Language Models (MLMs) are trained to predict masked tokens in a given input by randomly masking certain tokens during training. Its objective task is to \u201cfill in the blanks\u201d given a sentence, but complementary tasks are also used, like predicting which token has been replaced.\nUnlike CLMs, MLMs typically use a bidirectional architecture, meaning they use the context on both sides of a word. This gives them a broader perspective and generally leads to a better understanding of the relationships between words.\n\nDifferences between attention direction. BERT uses a bi-directional Transformer. OpenAI GPT uses a unidirectional left-to-right Transformer.\nMLMs are particularly suitable for tasks like text classification, sentiment analysis, and text tagging. Semantic search is driven by LLMs, where the mathematical distance between document embeddings us used as distance. However, they don\u2019t add much value for incremental token prediction tasks because of their bidirectional nature. Nor can they fill in an arbitrary amount of words.\n\nComparison between architectures of influential models from different modelling methods. (from left to right) BERT is an MLM, Original Transformer is a Seq2Seq model, and LLaMA is a CLM.\nBERT (Bidirectional Encoder Representations from Transformers) model is highly effective in document embedding. Both BERT and ELMo (Embeddings from Language Models) have been instrumental in advancing the field of natural language processing and continue to be widely used in a variety of applications.\nSequence-to-Sequence Models (Seq2Seq)\nThe Sequence-to-Sequence models (Seq2Seq) aim to transform an input sequence (source) into a new one (target), and both sequences can be of arbitrary lengths. Intuitively, it works like translating a sentence from one language to another \u2013 the input and output sentences don\u2019t have to be the same length, but they do relate to one another.\nSeq2Seq models are typically composed of an encoder-decoder architecture, which can be based on Transformers or Recursive Neural Networks (RNNs). The encoder processes the input sequence and compresses it into a latent representation, and the decoder then generates the output sequence from this representation. It is a common sentiment that RNN-based models, while being more expensive (and poorly parallelizable)32, are better than transformer-only models. Thus, various works such as RWKV try to combine the best of both worlds to create hybrid models.\nThese models can generally generate coherent, much larger output based on input, making them suitable for tasks like summarization, translation, and question answering.\n\nVisualization of encoding and decoding flow of an Seq2Seq model.\nA popular example of a Seq2Seq model is T5 (Text-to-Text Transfer Transformer) which during training frames all NLP tasks (such as translation, classification, summarization, and more) into text-to-text problems. Doing so, allows it to learn patterns useful for a variety of tasks. Another popular example is BART (Bidirectional and Auto-Regressive Transformers) which is pre-trained by corrupting text and forcing it to reconstruct the original, which improves its text comprehension. These models have shown impressive results on a wide range of tasks, with only a fraction of parameters they can outperform CLMs on various tasks.\n\nTransformations for corrupting (noising) the text during pre-training of BART (Lewis et al., 2019).\nThe Current State-of-the-Art\nIn the current landscape of large language models (LLM), transformer-based architectures largely steal the limelight. If you are already wondering what\u2019s working under the hood, we\u2019re planning on taking a deeper dive into their components in the next blog.\nThe ever-growing families of models and variants for architectures like GPT or BERT would cause anyone a headache to keep up. Besides the model architecture and modeling methods we have the label of foundational models to help us further organize our taxonomy. The foundational models are the Swiss army knives of AI models. Unlike conventional AI systems, they are trained broadly to be adapted to a variety of tasks with minimal labeled data. The core idea is that if you need more performance at a specialized task, you can start fine-tuning from a solid basis and not from scratch.\nThere are now many commercial and open-source options available for Causal Language Models (CLMs). Notable commercial CLMs include OpenAI\u2019s GPT, Google\u2019s PaLM and Anthropic\u2019s Claude. GPT-4 is a particularly impressive model, with an ensemble of 8 models, each with 220 billion weights. It amounts to an effective size of 1.7 trillion parameters while providing reasonable latency.\nOn the open-source side, Meta\u2019s Meta\u2019s LLaMA and Mistral have gained significant popularity. LLaMA models are available in a range of sizes, from 7 to 70 billion weights. This gives companies the flexibility to choose the model that best fits their needs or to fine-tune it themselves. The community has also developed many tools and optimizations to facilitate running LLaMA.\nWhen picking your model, one should always consider the use case and the amount of effort you are willing to spend on it. There are various benchmarks such as Huggingface\u2019s Open LLM Leaderboard and Massive Text Embedding Benchmark (MTEB) Leaderboard that evaluate both open source and commercial models performance an various tasks.\nFor open source models, however, it\u2019s worth noting that both LLaMA and Mistral models are trained on English text corpus, potentially impacting their performance on tasks in languages other than English.\nAddressing Large Language Models Limitations\nAs exciting as Large Language Models (LLMs) may be, they\u2019re not a one-size-fits-all solution. Just like you wouldn\u2019t use a hammer to drive a screw, there are many tasks that LLMs are well-suited for, and equally as many that they aren\u2019t. Let\u2019s take a look at some limitations posed with LLMs and what techniques exist to work around.\nRetrieval Augmented Generation\nA common challenge with LLMs is their ability \u2013 or rather, inability \u2013 to accurately recall things from memory. Despite their impressive capacity, these models don\u2019t actually \u201cknow\u201d anything. They generate text based on patterns they\u2019ve learned during training, which can sometimes lead to them making stuff up (a phenomenon referred to as \u201challucination\u201d).\nTo counteract this, we can use techniques like Retrieval Augmented Generation (RAG). This approach involves retrieving documents related to a given prompt and feeding them into the LLM to provide the correct context for answering the question.\nThis retrieval process can be done through semantic or vector searches, and the exciting part is, that it can be applied to your custom data as well as external systems like Google Search, essentially giving the LLM a searchable \u201cknowledge base\u201d to draw from.\n\nRetrieval Augmented Generation workflow. Image from AWS Sagemaker Docs.\nExternal Systems and Multimodality\nLLMs, despite their sophistication, still fall short when it comes to tasks like performing math calculations or executing code. A model won\u2019t be able to solve complex mathematical equations or compile and run a piece of Python code without some external help.\nThis is where \u201cTools\u201d come to the rescue, it is another key concept related to LLMs. This involves connecting the LLM to external programs by exposing their API interface within the input context. LLM can call these tools to perform the specialized tags by writing API calls which are executed as part of the generation process.\nA prime example of this concept in action is ChatGPT plugins, which enhances the capabilities of ChatGPT by allowing it to reach out to a suite of community-made plugins. Similarly, Langchain is a more developer-focused platform, that creates API abstractions and pre-built blocks to incorporate this functionality into your application.\nExtending the reach of LLMs even further is the integration of multiple modalities, such as vision and audio. These components convert inputs like images or sound into latent representations, a universal language that our LLM understands.\nCLIP, a breakthrough technology from OpenAI, revolutionized the way we bridge the gap between text and images. Similarly, GPT-4V(ision) and Large Language and Vision Assistant (LLaVA) expand the capabilities of LLMs to comprehend and reason over images.\nChat and Agents\nWe have all become familiar with LLMs through user-friendly interfaces like ChatGPT. Traditionally, LLMs provide a single answer as a completion to the input provided. However, various variants are fine-tuned or use prompt engineering to respond in a chat format, allowing for these interactive conversations.\nTo address the limitation of LLMs being limited to single-turn conversations, AI agents are designed as systems consisting of multiple LLM agents, each instructed with their specific task. These agents communicate with each other over a chat interface, moderated by AI. By working together, these LLM agents form a collaborative machine that can work towards completing a certain task.\n\nAn example of a conversation flow between a python code execution agent, progamming agent and the user (From AutoGen).\nThis concept is explored in-depth in the article Introduction to Autonomous Agents in AI. This collaborative approach has been implemented in projects like ChatDev, which in true spirit of Conway\u2019s law models agents as a company designed to tackle specific tasks, and Autogen by Microsoft, which provides developer tools to create your agent-based applications.\nYour Own Tasks\nThere may be instances where you find that LLMs are not producing satisfactory results for your specific task. However, there are several strategies you can employ to address this.\nOne simple trick can be to rephrase your task. The choice of phrasing has a significant influence on how the model responds. Similarly, applying prompt engineering techniques, like few-shot prompting by providing some examples, can prove useful, giving the model hints about what kind of output you\u2019re hoping for.\nYou can also experiment with different completion regimens such as introducing human-in-the-loop agents. This approach mixes AI-generated completions with human guidance to ensure the outputs align with your expectations.\nIn Conclusion\nWe\u2019ve covered a lot of ground in this blog series on large language models. By now, you should have a solid grasp of the key concepts underlying LLMs \u2013 their inputs, how to apply them for different tasks, and their capabilities.\nI hope you\u2019ve found this exploration illuminating. If you\u2019re eager to go deeper into any of the concepts we\u2019ve discussed, I\u2019ve included some additional resources below. Feel free to check those out while I work on the next installments.\nIf you have any other questions as you continue your LLM journey, don\u2019t hesitate to reach out. I\u2019m always happy to help explain concepts or provide guidance on applying LLMs in business contexts. Whether you need help building an LLM pipeline from scratch, measuring impact and ROI, scaling them up for production, or determining the best use cases for your needs, I\u2019m here. LLMs are powerful tools, but it takes thoughtful implementation to unlock their full potential.\nResources\n\nStart building RAG systems on AWS:\n\nQuestion Answering with your own data, LLMs and Java: Meet Langchain4j \u2013 Luminis\nImprove LLM responses in RAG use cases by interacting with the user | AWS Machine Learning Blog\n\n\nStart building multimodal applications:\n\nSearching through images using the CLIP model \u2013 Luminis\n\n\nAwesome LLM Tools:\n\nLangChain\nGitHub \u2013 microsoft/semantic-kernel: Integrate cutting-edge LLM technology quickly and easily into your apps\n\n\nStart Building Autonomous LLM Applications:\n\nGitHub \u2013 OpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)\nGitHub \u2013 microsoft/autogen: Building LLM Agent Applications\n\n\nWork on your prompts: Prompt Engineering Guide\n\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 39509, "title": "The Power of a Strategy Map: Aligning Business and Development", "url": "https://www.luminis.eu/blog/the-power-of-a-strategy-map-aligning-business-and-development/", "updated_at": "2024-01-22T13:11:21", "body": "In today\u2019s fast-paced business environment, it is essential for organizations to ensure that a product team\u2019s initiatives align with overall business goals. But connecting product strategy to organization strategy is not as straightforward as it sounds. More than often, there is a gap between \u201cthe business\u201d and \u201cdevelopment\u201d. Symptoms include endless prioritization meetings, a focus on features instead of value, and half-baked projects that end up gathering dust.\nIn this blog, I show how the Strategy Map and the Product Roadmap bridge gaps and achieve alignment. I set out the basics and explain what four benefits these powerful tools provide.\n\nThe Strategy Map\nA Strategy Map is a structured representation of an organization\u2019s strategy. It outlines the key objectives that are critical to achieve success and the metrics to monitor it. A Strategy Map helps organizations orchestrate and communicate the strategy to all stakeholders, aligning all parties to work towards the same goals. It includes the key drivers of performance, how they are interrelated and contribute to the goals. This literally is \u201cthe big picture\u201d that makes it easier to identify potential blind spots, trade-offs, and conflicts between different parts of the strategy.\n\nA fictitious Strategy Map with linked goals in 4 perspectives to support overarching mission, vision and value proposition.\nBalanced Scorecard framework\nYou might already know the Balanced Scorecard framework (BSC), as it has been one of the most influential approaches in analyzing and reporting business performance of the past two decades (Kaplan and Norton, 1992, 1993). BSC identifies four generic perspectives: growth and development (later: learning and growth); internal business processes; the customer; and financial. The Strategy Map (Kaplan & Norton, 2001; 2004) is a concept extending BSC with cause-effect relations between these perspectives. As learning and growth is developed within a company, upward links are made to the internal (business process) perspective. Business processes are in turn linked to customers who, ultimately, influence the financial perspective of the company (Kaplan & Norton, 1996: 31). The relations between the different perspectives add logic and coherence to the strategic goals. This makes the Strategy Map helpful in prioritizing and planning product initiatives on the Product Roadmap.\n\n\u00a0\nThe example above shows a single value creation chain taken from a fictitious Strategy Map. The goal in the Financial Perspective is to \u201cincrease value\u201d. This is achieved by \u201cexcellent customer experience\u201d, which is a listed goal in the Customer Perspective. One way to reach that goal is described in the Internal Perspective, by \u201cContinuous delivery\u201d, as it enables continuous improvement of the customer experience. And lastly, achieving that goal requires a \u201cMove to the Cloud\u201d in the Learning and Growth Perspective. In this example, it\u2019s wise to hold any large product initiatives aimed at improving customer experience, and instead aim resources on moving to the Cloud and implementing CI/CD first.\n\nMoney, money, money\nThe \u201cformal\u201d definition of The Strategy Map framework supports the notion that success is ultimately expressed in financial results, like shareholder value. And, to achieve this in a sustainable way, it needs to be achieved through customer loyalty. While this may be true for most businesses, money is not always the (only) motivation behind organizations. For schools, public services, NGOs and other non-profit organizations, Strategy Maps can be used with other perspectives to model the value creation chain.\n\nThe Product Roadmap\nThe Product Roadmap is a plan that outlines how the development of a product contributes to the organization\u2019s strategy over time. It details the specific initiatives, projects and activities needed to achieve the desired outcome: moving the metrics (I\u2019ll get to those in a minute) towards achieving the strategic goals. When Roadmaps stem from the organization\u2019s strategy, they help ensure that everyone in the organization is on the same page and working towards the same goals. This becomes increasingly important for larger businesses, where aligning multiple teams, products, and departments becomes a challenge.\n\nRoadmap or Backlog?\nA Product Roadmap is different from the product backlog, but they are related. The product backlog is a detailed, ever-evolving list of tasks and features that guides the day-to-day development work. It supports the execution of the Product Roadmap\u2019s strategic vision.\n\nFocus on the outcome\nVolatile environments require plans to be updated frequently. However, a Product Roadmap that\u2019s constantly changing does not instill confidence and can break a product team\u2019s morale. Therefore, it\u2019s better to make the Product Roadmap about value (outcomes) instead of features (output). The desired outcomes are directly tied to the organization\u2019s strategy and goals, and more consistent than a list of most wanted features at any given time. So, instead of planning \u201cadd welcome message for new customers\u201d, better plan the initiative \u201cimprove customer onboarding experience\u201d and measure if the team\u2019s efforts contribute to the desired outcome.\nChoosing metrics\nMetrics are used to clarify what goals mean, to set the level of ambition, and to measure progress. Initiatives on a Product Roadmap are aimed at moving these metrics to contribute to the strategic goals. A goal like \u201cExcellent experience\u201d could mean many things, so it is important to decide what it means to your product, customer, and organization, and express that in metrics. For example, you can use the Net Promotor Score to measure customer experience. You can also choose to look at engagement and conversion rates instead. Choosing what metrics to use is important, because these are the metrics that initiatives on the Product Roadmap are aimed at to get moving. There is a lot more to say about metrics, so I will spend another blog on that topic later. For now, the most important thing to know is that metrics are the connection between the initiatives on the Product Roadmap and the goals on the Strategy Map.\n\nMetrics specify the exact meaning and ambition of a goal. To contribute to the strategy, product initiatives should be aimed at moving these same metrics.\nBenefits of Strategy Maps and Product Roadmaps\nStrategy Maps and Product Roadmaps help ensure that product initiatives are aligned with both product and business goals. But like any tool, they only help if understood and used correctly. And even then, they won\u2019t make decisions by themselves. They do help product teams (or any other team) to make choices aligned with organization strategy. Following are the four main benefits of working with Strategy Maps and Product Roadmaps.\nBenefit #1: Catalyst for making clear choices\nCreating and maintaining a Strategy Map and one or multiple Product Roadmaps forces the organization to make fundamental choices. If there is no clarity and agreement on the overall business goals yet, drawing a Strategy Map makes issues tangible and negotiable. Issues need to be resolved before communicating the strategy, to prevent initiatives that head in a wrong direction. The Strategy Map can (and will) evolve over time. However, too many changes too often break people\u2019s trust and willingness to align their efforts with the strategy. Additionally, a Product Roadmap that is just a to-do list with no clear connection to the strategy doesn\u2019t help to achieve business goals. With a confident strategy and a clear map, product teams (or any other team) understand what is needed and are able to make choices aimed at moving the right metrics.\nBenefit #2: A common understanding\nBy using a Strategy Map and Product Roadmaps, organizations provide greater clarity and improve communication about their product initiatives. It encourages everyone in the organization to understand the goals they are working towards. It also helps achieve a common understanding of how initiatives relate to the organization\u2019s overall business goals. This clarity and communication help get buy-in. More importantly, it is essential to ensure that everyone stays aligned and keeps working towards the same objectives.\nBenefit #3: Track progress and measure success\nA goal represents an ambition: something that the organization wants to achieve. To know when you achieved a goal, each goal should have at least one metric to express the ambition. Sometimes defining metrics for goals is not straightforward. For example, consider the business goal \u201cimprove onboarding\u201d. How do you know when you improved this? And how much has it improved? It is not always hard science. It might take asking the user how they experienced the onboarding prior to and after making changes. Or a combination of indicative metrics, like activation and engagement rates. All initiatives on the Product Roadmap need to move one or more of these metrics for strategic justification. This ensures that all product initiatives are aligned with the organization\u2019s business goals, making it possible to track progress and measure success.\nBenefit #4: Prioritization of initiatives\nA Strategy Map helps prioritize product initiatives by identifying the most critical objectives, as well as necessary initiatives to reach the organization\u2019s overall goals. By focusing on the most critical initiatives first, the organization ensures that it is making the most effective use of its resources. A Product Roadmap helps scheduling those initiatives over time and in more detail. This helps ensure that the most important product initiatives have the highest priority, reducing the risk of wasting resources on less critical initiatives.\nStart mapping today\nSo, if you are fed up with endless prioritization meetings about features instead of value, get your team together and start drawing your Strategy Map. As soon as you have your goals and metrics in place, focus the initiatives on your Product Roadmap on these metrics, starting with the highest priority goals. You will see that Strategy Maps and Product Roadmaps are powerful tools to help you align your product initiatives with your overall business goals, prioritize your product initiatives correctly, and allocate your resources effectively. Would you like expert help? Luminis gets you started and can facilitate the strategic alignment process for your organization. Just let me know if you\u2019re interested!\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
{"post_id": 39220, "title": "An Epic Software Estimation: giving continuous insight in large projects", "url": "https://www.luminis.eu/blog/an-epic-software-estimation-how-to-give-continuous-insight-in-large-projects/", "updated_at": "2023-11-15T10:21:22", "body": "In this blog post, we explore the invaluable practice of continuous insight into delivery capacity, and how it can be a game-changer when it comes to keeping your stakeholders happy. We delve into the why, the how, and offer practical tips to help you and your team master the art of software or project estimation. So, let\u2019s embark on a journey to unlock the secrets of effective project estimation and the impact it can have on your organization\u2019s success.\nIntroduction\nA few months ago, a product owner in our team knocked on my door. He was anxious about an upcoming meeting where he, other product owners, and some C-level stakeholders discuss our roadmap\u2019s current status. In these meetings, having project durations is crucial, and he needed to provide a rough estimate for our project\u2019s duration.\nRecalling that a colleague and dear friend had developed a template for tracking epic durations based on estimations, I reassured our product owner: \u201cLet me assist you with that. I\u2019ll call for a team meeting next week, and by the end of it, I can give you an estimation how long this project will take.\u201d PO: \u201cThat would be awesome.\u201d\nWell\u2026 time to figure out how to actually do this.\nWhy software estimations?\nWhat is an epic? An epic is essentially a feature that is too large to be contained in a single user story. Therefore an epic is cut-up into multiple, more manageable, user stories. The term epic is often also used for grouping many user stories. This is because tools like JIRA refer to an epic as a bunch of tasks, user stories, etc. Just know, in this blog post, I use epics to describe a project that is so big, it takes multiple sprints to complete.\nEnough about definitions. What brings an estimation to us? In the words of Steve McConnell: \u201cAn estimation gives us the ability to commit to a delivery and control the project to meet its targets\u201d. This quote comes from \u201cSoftware Estimation: Demystifying the Black Art\u201d, a book that has greatly impacted this blog and my view on software estimation. In this quote, \u201ccontrol\u201d can be read as the decisions we make based on events that have an impact on the project\u2019s duration and quality. In the context of this blog, estimation revolves around how long it will take to deliver a certain part of functionality. The ability to address the question \u201cWhat do we need to finish this project in the time we would like it to take?\u201d gives a great perspective on the decisions you make.\nWhile the \u201ctarget\u201d of a project is important for the business, that doesn\u2019t mean it is achievable. The target is a description of a desirable business objective. The \u201ccommitment\u201d is a promise by the team to deliver a set of functionality by a certain date. The commitment can be more aggressive or less aggressive than the estimate. An estimate merely helps you get control over a project. It can help you make decisions about the path of the project, and help you get a clear view of how certain events impact your project.\nDon\u2019t be afraid to estimate epics\nMore often than not, we shy away from estimating projects with a large scope. The uncertainty that comes with a large project makes it hard to estimate precisely. This is a wrong way of viewing estimations. As mentioned before, we don\u2019t want a precise calculation of how much a project costs. We want insights so that we can exert control over a project. An estimation is something you work on during the entire duration of the project. A feedback loop in which you keep track of your estimation gives great insight into your project. It gives control over the project because you can see the impact of decisions you make.\nHow to make a software estimation?\nWhen working on a project with a defined set of requirements, I think the best way to start estimating epics is by using relative estimations. Quite possibly, it is not the first epic your team works on (and hopefully not the last!!). That\u2019s great! That gives us a reference point to what the new epic can relate to. So say, for example, you want to estimate epic C after already completing epic A & B.\n\nA common way to start estimating is by relating to the size of the epic to a T-shirt size. This would look something like this:\nSimple for A & B, because they are already done. Epic C can be estimated relative to A and B. This process is pretty straightforward. Knowing that C is larger than B and somewhat smaller than A, we can classify C as a size L.\n\nThe estimation gets even more useful when we look back on how much story points it cost us to complete A and B. Say it was 150 points for A and 50 for B. That gives us a range to roughly calculate the size of epic C. For some projects, roughly knowing the size of your epic and the time you need to spend on it is enough. In that case, you can stop the project estimation at this point.\nAs mentioned before, we are not measuring the exact size of the epic. We are merely estimating how long it takes to complete. When you know the velocity of the team and the rough boundaries of the estimation, you have enough information to make a software estimation. We can communicate the size of the epic with stakeholders and see the impact on the duration of the project based on the decisions we make.\nFrom a rough to a specific software estimation\nHowever, I don\u2019t think you should stop there. Because after meeting with the project team once more, you can assign specific story points to epic C, based on previous experiences of epics A and B, and possible complexity.\n\nThis is the best scenario! Because now, it\u2019s even possible to track the progress of the epic, based on the initial estimation. After completing or assigning story points to a user story, you can subtract those points from the 110 we estimated it would cost us to complete epic C. This is helpful for a number of reasons:\nFirst off, it helps the team focus. If your main goal is to complete epic C, we should see a steady decrease in the amount of points left in the epic. If this is not the case, it is a great moment to ask yourself, the team, and perhaps the product owner: \u201cWhat\u2019s going on? What is preventing us from completing this epic?\u201d So, it gives us a great tool to control the project.\nEstimating an epic is also super helpful for timely communication to a stakeholder. Say, after a few months\u2019 work, epic C is left with 40 points. But the team knows there is still much more work to be done. This information is fed through the feedback loop of adjusting your estimation based on new knowledge, which means you can communicate this new estimation to the stakeholders, who can choose to assert control or not. People tend to show greater understanding when you inform them in advance that a feature will take longer, instead of waiting until the expected delivery date.\nSo, you get a lot by taking a bit more time to estimate your epic and continuously communicating about it.\nTips\nOne of the tools I like is the use of milestones. Sometimes it is hard for your team to estimate a whole epic. Even if you have relative epics to relate to. I like to cut the epic into milestones and estimate those individually. In our project, we work towards a minimal viable product (mvp) which is milestone 1. After completing that, we will integrate it; milestone 2, and so on. It gives the team a better overview of the work they need to do.\nAnother tip is to try not to get too fixed on the estimation or story points you initially gave your project. Projects change, whether it\u2019s features that come up halfway through the project or some complexity you didn\u2019t foresee. Just make sure you review the estimation from time to time. When you feel like the work it takes to complete the project doesn\u2019t align with the plan, estimate again and let stakeholders know.\nOur project\nBack to the beginning of this blog post: the promise I made to my product owner. We had a team meeting, and within an hour or so we had a decent idea of what our estimation for this project was. The milestones gave the product owner a tool to communicate the work that needed to be done and what our estimation was based on, giving them more control over the project. So far, we are still on course for our project, but last sprint we saw we needed to include some more stories relating to the epic to keep on track. It has truly helped tracking our progress and adjust the planning when needed.\nSo, give it a try. Call for a team meeting and estimate your epic! If you have any questions, please feel free to reach out to me.\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
{"post_id": 39103, "title": "AWS Multi-Account GitOps Deployment 1: Organizational Setup", "url": "https://www.luminis.eu/blog/aws-multi-account-gitops-deployment-organizational-setup/", "updated_at": "2024-02-08T12:32:20", "body": "In the world of AWS, scaling your infrastructure often means the need for multiple AWS accounts to segregate resources, manage access and centralize billing. But how do you organize these accounts efficiently? Let\u2019s set up an AWS multi-account organizational structure, and leverage the GitOps methodology for deployments within this blog series.\nSetting the Stage\nIn the first installment of this three-part series, we walk through the foundational setup necessary to deploy a multi-account GitOps strategy on the AWS Cloud. Leveraging the AWS Cloud Development Kit (CDK), we create an organizational setup to harness GitOps deployment to the fullest across different AWS accounts.\nPrerequisites:\n\nAWS Account with appropriate permissions, preferably a fresh one.\nAWS CLI v2, AWS CDK, projen and TypeScript.\n\nStep 1: Account Preparation\nTo roll out our CDK stack for organizational structuring, we start with enabling the IAM Identity Center (an evolution from AWS Single Sign-On). This simplifies access, whether it\u2019s through the command line or console. Given the prevailing trend of centralized user, group, and permission management, IAM Identity Center offers AWS customers a cohesive solution. While it has proprietary user and permission management tools, it also smoothly integrates with popular external solutions, like Microsoft Active Directory. For our tutorial, we\u2019re sticking to the straightforward Identity Center Directory by AWS.\n\nAfter logging into your AWS account as the root user (always make sure to enable mfa in the IAM console for the root user), navigate to the IAM Identity Center page for your preferred region. Throughout our series, we\u2019re going with Ireland (eu-west-1).\nOn this page, select Enable and Create AWS organization. This not only grants access via the AWS access portal, but also establishes an AWS Organization.\nFollowing, I suggest personalizing your AWS Access portal URL and assigning a subdomain within awsapps. For instance, I\u2019ve set mine to https://lutku.awsapps.com/start.\nNow that we have set up the Identity Center Directory and the AWS Organizations, we will create our first user to access the console and the command line. Go to Users within the IAM Identity Center and click Add user.\nOnce you\u2019ve entered the essential user details and chosen your login credentials, navigate to the Permission sets page and Create permission set. Here, we\u2019ll create Administrator Access permissions to assign to our user for the management account. Don\u2019t forget to save the credentials necessary to login to the AWS access portal!\nSelect the AdministratorAccess policy under the predefined permission set and hit next. Create a name for it and preferably extend the session duration to 12 hours, though this should be met with the policies of your organization, and click next. Review the details and create the permission set.\nAfter creating the user and the permission set, we will assign the permission set for the management account, so that we can access the management account using the AWS access portal. Go to AWS Accounts and hit Assign users or groups after selecting the management account:\nGo to the Users tab and select the user created as in and hit next:\nSelect the AdministratorAccess permission set created, and click next. After reviewing, submit the request and assign the permission set to the user.\nFollowing these configurations, you are able to use the AWS Access portal to access your management account. Use your personalized URL, and you should be greeted with a login prompt.\nLogin using the credentials and you see the following page:\n\nStep 2: Infrastructure as Code Preparation\nFor scalability and ease of maintenance of your AWS multi-Account organizational structure, we\u2019ll employ AWS CDK, a favorite among AWS users. It aids in deploying AWS resources using your preferred programming language. To generate our CDK application, we\u2019ll use projen, which helps manage our CDK configuration. My colleague has a comprehensive series that delves deeper into its benefits.\n\nInitialize a new repository:\n\nmkdir organisational-setup && cd organisational-setup\r\ngit init\r\nnpx projen new awscdk-app-ts\n\nUpdate the .projenrc.ts configuration file:\n\nimport { awscdk } from 'projen';\r\n\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n  authorEmail: 'utku.demir@luminis.eu',\r\n  authorName: 'Utku Demir',\r\n  cdkVersion: '2.96.2',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'organisational-setup',\r\n  github: false,\r\n  projenrcTs: true,\r\n  keywords: [\r\n    'AWS CDK',\r\n    'projen',\r\n    'Typescript',\r\n    'Deployment',\r\n  ],\r\n  gitignore: ['.idea'],\r\n  license: 'MIT',\r\n  licensed: true,\r\n\r\n  deps: ['@pepperize/cdk-organizations'],\r\n});\r\nproject.synth();\r\n\nIt is important to add @pepperize/cdk-organizations to the dependencies as it is the community construct library we will use to generate the AWS Organizations resources.\nAfter updating, generate the project:\nyarn projen\n\nCreate a new file named organization_setup_stack.ts under src to include the necessary configuration for our stack:\n\nimport * as orgs from '@pepperize/cdk-organizations';\r\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport { Construct } from 'constructs';\r\n\r\nexport interface OrganizationSetupStackProps extends StackProps {\r\n  environments: string[];\r\n}\r\n\r\nexport class OrganizationSetupStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: OrganizationSetupStackProps) {\r\n    super(scope, id, props);\r\n\r\n    const organization = new orgs.Organization(this, 'organization', {\r\n      featureSet: orgs.FeatureSet.ALL,\r\n    });\r\n\r\n    const deployment = new orgs.OrganizationalUnit(this, 'deployment', {\r\n      organizationalUnitName: 'Deployment',\r\n      parent: organization.root,\r\n    });\r\n\r\n    new orgs.Account(this, 'deployment-account', {\r\n      accountName: 'DeploymentAccount',\r\n      email: 'utku.demir+deployment@luminis.eu',\r\n      roleName: 'OrganizationAccountAccessRole',\r\n      iamUserAccessToBilling: orgs.IamUserAccessToBilling.ALLOW,\r\n      parent: deployment,\r\n    });\r\n\r\n    props.environments.forEach(appEnvironment => {\r\n      const environmentOrganizationalUnit = new orgs.OrganizationalUnit(this, appEnvironment, {\r\n        organizationalUnitName: appEnvironment,\r\n        parent: organization.root,\r\n      });\r\n\r\n      new orgs.Account(this, `${appEnvironment}-account`, {\r\n        accountName: `${appEnvironment}-account`,\r\n        email: `utku.demir+${appEnvironment}env@luminis.eu`,\r\n        roleName: 'OrganizationAccountAccessRole',\r\n        iamUserAccessToBilling: orgs.IamUserAccessToBilling.DENY,\r\n        parent: environmentOrganizationalUnit,\r\n      });\r\n    });\r\n    organization.enablePolicyType(orgs.PolicyType.SERVICE_CONTROL_POLICY);\r\n  }\r\n}\r\n\nThe above stack, based on the work of Matt Lewis on Setting up a AWS Multi-Account environment, creates organizational units and accounts for application deployment and application hosting environments.\n\nEdit the main.ts under src to include this stack as:\n\nimport { App } from 'aws-cdk-lib';\r\nimport { OrganizationSetupStack } from './organization_setup_stack';\r\n\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\nconst environments = ['dev', 'test', 'prod'];\r\n\r\nconst app = new App();\r\n\r\nnew OrganizationSetupStack(app, 'organisational-setup-stack', {\r\n  env: devEnv,\r\n  environments: environments,\r\n});\r\n\r\napp.synth();\r\n\nHere we create three accounts for the three environments for our application: dev, test and prod.\nStep 3: Account Bootstrapping and CDK Stack Deployment\nAs the last step, we will configure our AWS CLI, bootstrap the management account and deploy the stacks.\n\nConfigure AWS CLI for SSO:\n\naws configure sso\n\nFollow the on-screen prompts to associate your SSO with your company:\n\n\n\nBootstrap your AWS account to prepare for CDK deployments (don\u2019t forget to replace 123456789012 with your own account id):\n\ncdk bootstrap aws://123456789012/eu-west-1 --profile lutku-management\n\nDeploy the stack:\n\nyarn deploy --all --profile lutku-management\nCongratulations! Now, you should be able to see the basic organizational structure in place in the AWS Accounts page on IAM Identity Center like:\n\nConclusion\nIn this first installment of our three-part series on multi-account GitOps deployment on AWS, we\u2019ve dived deep into setting up an efficient organizational structure. Using AWS\u2019s tools and services, like the AWS Cloud Development Kit (CDK) and IAM Identity Center, we\u2019ve demonstrated how easy it is to set up multiple AWS accounts and create an organizational structure. By the end of this guide, you should have a basic organizational structure in place that paves the way for more advanced GitOps strategies in the subsequent posts. Whether you\u2019re scaling your infrastructure or optimizing access management, the blend of AWS tools and GitOps methodology offers a robust solution. Stay tuned for the next parts, where we\u2019ll delve further into the intricacies of GitOps and multi-account management on AWS.\nRemember, as you embark on this journey, resources and references provided here are just a starting point. The cloud landscape is vast and ever-evolving, so always be open to exploration and learning.\nHappy Cloud Engineering and until next time!\nReferences:\n\u2013 Production Ready CDK Project Structure\n\u2013 GitHub: CDK Organizations\n\u2013 Setting Up a Multi-Account AWS Environment\n", "tags": ["aws cdk", "AWS re:Invent 2021", "CI/CD", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 39058, "title": "A Devoxx of Firsts", "url": "https://www.luminis.eu/blog/a-devoxx-of-firsts/", "updated_at": "2023-10-13T11:19:56", "body": "In this blog post, I tell you all about my experience of Devoxx Belgium 2023. A Devoxx of Firsts, where I did a lot of things for the first time. Keep reading to hear all about the workshopI gave, the awesome keynote on imposter syndrome, and the workshops and talks of my colleagues\nFor the last 2 years, I focused a lot on self-improvement. I was chosen to participate in Luminis\u2019 leadership program called Accelerate. Want to know more about Accelerate? We have a great video explaining it here (starring me!).\nDuring Accelerate, I faced my fears of presenting. As a result of facing my fears, I presented at a conference last year, which was J-Fall. Although only a byte (or lightning) talk of 15 minutes, it was exciting and fun. I did more presentations after that, but I didn\u2019t go to any conferences. Until my colleague Jettro\u2019s and my workshop proposal for Devoxx got accepted. And so the Devoxx of Firsts started: my first conference for the year, my first workshop at a conference, my first conference in a foreign country, and my very first Devoxx. For my colleague Niels, Devoxx even was the first ever conference he attended. Hence the title of this blog post, a Devoxx of Firsts. Also, my first time blogging about a conference experience!\nDay 1 (Monday): LangChain 4 Workshop\n\nMy colleague Nico was so kind to drive me and Jettro to Antwerp, in his awesome Subaru. The first talk I attended was a deep dive talk about LangChain, a tool we also use in our workshop. It proved to be a really good introduction talk to our workshop. During the break, I approached the speaker and asked if he could mention our workshop, so the attendees of this deep dive could put their new knowledge into practice right away. Funnily I felt a bit anxious to ask, as this was the first time I approached a speaker at a conference and talked to them. But I did ask, and he was kind to do so.\nRight after that talk was our workshop. Somehow, the mention by the previous speaker gave me a boost. For the first time, I was not anxious about doing something like this. I felt excited instead\u2014the power of overcoming your fears. The workshop itself didn\u2019t go as well as we hoped, at least in our experience. A few people struggled to get their environments up and running. We learned something there.\nAt some point during a presentation part, the power went down and the room went dark (see picture below). It made presenting a bit harder, but we didn\u2019t let that bother us, and we just continued until the lights went back on. We had some good discussions at the end of the session with some exciting people, which made me leave with a positive vibe. We ended up with a 3.6/5 rating, which I think is not bad for my first workshop. After this, we had a nice dinner in the center of Antwerp with our Luminis group, and went to bed afterwards.\n\nDay 2 (Tuesday): Workshopping\nOn the second day of the conference, I started with a workshop given by my colleagues Nico, Niels, and Jettro (see photo). Their workshop was about how to translate your Event Storming outcome into code. I had done some (practice) Event Storming sessions before, but never made it to the next step. We had to work in pairs, ping-pong style. It was a good session where I saw the power of TDD and pair programming combined. I\u2019m going to do this more often.\n\nFollowing their workshop, we all attended a workshop about Roblox & Quarkus. We created a back-end service in Java with Quarkus, deployed it in AWS, and called that back-end from a Roblox game. The service was used to provide questions and to verify the answers. With the right answer, a gate opened within the game. It was fun to do. Even got my first experience with Amazon CodeWhisperer. Not fully convinced yet, but I saw the potential of code assistants. If it improves a bit more, I\u2019ll definitely use it. I filled the rest of the day with visiting booths to gather goodies and a talk about Machine Learning in Java.\nWhen the conference ended, I attended my first speaker dinner. One of the perks you get when presenting at a conference. We sat down at a table in a corner, so we weren\u2019t really able to mingle with other speakers. That was a bit unfortunate, but I\u2019ll definitely try to do that next time!\nA lot of talks on Devoxx were about ML, LLMs, and AI. Even though Devoxx is mainly a Java conference, I already saw a lot of Python code, because a lot of ML/LLM/AI work is done with Python. During one of the talks, my colleague Jettro made a reference to Monty Python. I then concluded that I\u2019d never seen any of the movies. So back at the apartment after the speaker dinner, my colleagues showed me Monty Python and the Holy Grail for the first time. I totally laughed my cheeks off.\nDay 3 (Wednesday): A Day of Keynotes and AI\nWednesday started with three keynotes. The first one was about the history of Devoxx, as this was the 20th edition. After that, there was one about Java 21. This was refreshing because, for the last couple of years, I focused a lot on self-improvement and search technologies, and not so much on new Java features. The last keynote was about embracing imposter syndrome by Dom Hodgson. We all start off as an imposter when we do something, and that\u2019s not a bad thing. It was a lot of fun, and, for me, the best talk of the conference. You can watch it here if you are interested.\nThe rest of my day was full of interesting data and AI talks:\n\nAI Unbounded: Multiplying the Collective Intelligence of Humanity: an interesting talk about the potential of AI as a catalyst for exponential human progress and how it can help us reach our goals.\nUnderstanding Probabilistic Data Structures with 112,092 UFO Sightings: A fun take on UFO sightings, while also explaining probabilistic data structures. They use hashes to give you faster and smaller data structures in exchange for precision. If you\u2019ve got a mountain of data to process, this could be useful.\nHow to Build a GPT4All: Introduction to GPT4All, an open-source software ecosystem that allows anyone to train and deploy\u00a0powerful\u00a0and\u00a0customized\u00a0large language models on\u00a0everyday hardware.\nGenerative AI in practice: Concrete LLM use cases in Java, with the PaLM API: Working with LLMs in Java with the use of the PaLM API, provided by Google Cloud\u2019s Vertex AI services.\n\nTo close off the day, we watched Monty Python\u2019s Life of Brian back at the apartment. Or so I tried. I fell asleep halfway because of exciting but long days we\u2019ve had, so I decided to go to bed.\nDay 4 (Thursday): More AI, and some Java\nWhile I attended a lot of talks on AI already the previous days, I still had some coming. I heard the term MLOps a few times before, but didn\u2019t really know what it meant. So I chose to attend an introduction to MLOps as my first talk of the day. Now I know what it takes to deploy Machine Learning models to production. Talking about ML, my second talk of the day was about lessons learned about ML in Java with a home project. I always like this kind of talks, because it\u2019s all about experimenting and making mistakes.\n\nAt the same time as the ML in Java session, my colleague Peter also presented (photo on the left). He talked about the QUIC protocol. Although I didn\u2019t attend it, I watched it later online and want to give him a shout-out on how well he did. You can watch his talk here.\nAfter that, finally, the talk I had been waiting for. A talk about LangChain4j. A Java version of LangChain. In preparation for our workshop, I spent a lot of time experimenting with this framework because we didn\u2019t want to limit our attendees to Python. I also wrote a blog post about it. So I was curious about what this talk would bring me. It was\nan informative and fun presentation/demo by one of the founders. I vote this as the second-best talk of the conference for me.\nAt the end of this talk, I approached the speaker and for the first time, I complemented a speaker in person on their performance. We had a short conversation with the dev-team and they made me keen on contributing to the project. If I will be able to help them, that would be my very first contribution to an open-source project. So let\u2019s see what I can do!\n\nThe day continued with more talks:\n\nSemantic Kernal: A Microsoft competitor of LangChain. Interesting content, but a bit of a boring talk. Probably a combination of AI tiredness and a less enthusiastic speaker.\nBattle AI coding assistants: AI could be a great help in writing better, cleaner, and more secure code. So I wanted to see this talk in which one is best. This battle was between Tabnine, GitHub Co-Pilot, and ChatGPT. Too bad it didn\u2019t include Amazon CodeWhisperer and JetBrains AI.\nJava patterns and practices for serverless applications: AI tiredness kicked in, so I chose to go to a talk about using Java in the cloud. And who said Germans have no sense of humor? Here\u2019s one for you.\n\nAnd not specifically on this day, but in the context of A Devoxx of Firsts: Luminis recently acquired a company and throughout Devoxx I met with a few of my new colleagues for the first time. Good to see we have new enthusiastic colleagues.\nThe End: A Devoxx of Firsts\nOne benefit of organizing a conference in a movie theater is that you can also watch a movie. Every year on the Thursday evening, all Devoxx attendees are invited to watch a new movie together. So my conference experience ended by watching the movie \u201cThe Creator\u201d. It had a very fitting topic: AI. And since this was a new movie, I saw it for the first time ;).\nAll Devoxx presentations can be viewed on the Devoxx YouTube channel. The workshops, however, are not recorded.\n", "tags": [], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 39029, "title": "Omscholen tot Software Developer: Rick\u2019s verhaal", "url": "https://www.luminis.eu/blog/omscholen-tot-software-developer-ricks-verhaal/", "updated_at": "2024-01-23T16:04:55", "body": "Jezelf omscholen naar een nieuw vakgebied is een reis die niet altijd even makkelijk is, maar nu ik er op terugkijk is het de beste keuze geweest in mijn carrie\u0300re. In dit blog wil ik graag mijn persoonlijke reis met jullie delen, de uitdagingen die ik ben tegen gekomen en de waardevolle lessen die ik onderweg heb geleerd tijdens mijn omscholing tot Software Developer.\nDe vonk\nZo\u2019n reis begin je natuurlijk niet zomaar. In mijn geval was het de groeiende fascinatie voor technologie. Ik was werkzaam als Marketeer, maar luister in mijn vrije tijd voornamelijk podcasts over de laatste technologische ontwikkelingen. Ik was in mijn dagelijkse werk veel bezig met branding en online marketing campagnes, maar ik houd me liever bezig met het manipuleren van data. Zelfs Excel bestanden automatiseren met VBA geeft mij meer voldoening dan het verzinnen van marketingcampagnes. Wat ging hier mis? Waarom kost het doen van mijn werk zoveel energie? Dat hoort er een beetje bij toch? De leuke dingen vinden plaats in het weekend\u2026 Of niet?\nDe verkenning\nEn toen begon het te knagen. Is dit wel de baan die het beste bij mij past? Is dit wat ik de komende jaren nog wil gaan doen als werk? En het belangrijkste, word ik hier blij van? Eigenlijk kon ik al die vragen redelijk snel met een overtuigende nee beantwoorden. Maar ja, waar begin je dan? Mijn eerste stap was met mensen praten die werkzaam zijn in de IT branche. Ik vertelde hen dat ik dacht aan een overstap, maar dat ik mij afvroeg of dit wel realistisch is op mijn leeftijd, toen 33 lentes jong. Romantiseer ik het misschien teveel? En waar in hemelsnaam begin je zo\u2019n omscholing?\n\nDe sprong\nNa een aantal gesprekken met Software Developers uit mijn kenniskring werd snel duidelijk dat omscholing grofweg in twee opties opgedeeld kan worden. Een eerste optie is om bij een bedrijf dat omscholingen verzorgt aan te kloppen en je aan te melden voor een omscholing, soms ook wel bootcamps genoemd. Er zijn hier namelijk genoeg van. Echter voelde het voor mij niet als het juiste pad. Na zo een training/bootcamp moet je jezelf namelijk terug gaan verdienen en op zich is dat niet erg, en zelfs logisch, het vervelende is dat je vaak ook minder keuze krijgt over waar je uiteindelijk geplaatst wordt/komt te werken. Dus besloot ik te gaan voor optie 2: omscholen via zelfstudie. Wellicht een lastigere route, maar wel eentje waar ik zelf in controle blijf. Een van de kennissen die ik had gesproken tijdens mijn verkenning wees mij op een gratis online cursus \u2018Harvard University: CS50\u2019s Introduction to Computer Science\u2019. Gelijk een goede peiler om te checken of Software Development wel een goede match is. Let\u2019s go!\nDe learning curve\nEn een match was het zeker. De cursus was een perfecte voorbode dat ik de eerste stap had gezet in de goede richting. Basisprincipes van computer science worden behandeld en je krijgt te maken met programmeertalen als C, Python en Java. Gelijk een goed moment om af te tasten welke taal mij het meeste aanspreekt. Na de cursus met veel plezier afgerond te hebben, had ik besloten mij te focussen op Java. Nu was het zaak vlieguren te gaan maken. In een jaar tijd ben ik mijn Github gaan vullen met hobbyprojecten, heb ik mijn Oracle Certified Associate en Professional gehaald en ben ik een priv\u00e9project gestart met twee vrienden (een backend en frontend developer) om zo vertrouwd te raken met het scrum framework.\nDe uitdagingen\nDeze route klinkt nu alsof ik geen obstakels ben tegen gekomen, maar die waren er natuurlijk wel. Ook al besteed ik met veel plezier mijn uren aan IT/Software Development, die uren moeten wel ergens vandaan komen. In eerste instantie probeerde ik zoveel mogelijk in de avonduren te studeren. Echter loop je hier tegen de uitdaging aan dat je na een werkdag soms best moe thuis komt en dan vraagt het veel energie om nieuwe dingen te leren, zeker in een nieuw vakgebied. Daarom had ik besloten een dag minder te gaan werken om zo naast de avonduren ook een volle werkdag erbij te kunnen pakken. Vaak sneuvelde er in het weekend ook wel een dag aan het studeren, maar het weekend probeerde ik toch zoveel mogelijk te ontzien om tijd met mijn vrouw en kind te kunnen besteden. Deed ik dit niet, dan is mijn zoon (7 jaar) niet de beroerdste om mij erop te wijzen dat het weekend is. Mijn vrouw trouwens ook niet. Uiteindelijk resulteerde dit in grofweg 10 tot 15 uur per week die ik spendeerde aan mijn omscholing.\nDe gesprekken\nToen ik mij zeker genoeg voelde begon ik met solliciteren en mijn profiel bij recruiters neer te leggen. Eerlijk gezegd was ik nog niet zeker genoeg of ik de baan volledig in zou kunnen vullen, maar ik was zo gemotiveerd dat het voor mij de vraag was \u2018wanneer\u2019 ik Software Developer zou worden en niet \u2018of\u2019. En die vastberadenheid sloeg over toen ik in gesprek kwam met Luminis. De toenmalige chef zag het wel zitten, hij voelde namelijk wat ik zelf ook lang voelde: passie voor het vak. En natuurlijk had hij door dat ik wat betreft Software Development nog nat achter de oren was. Maar met een gezond stel hersenen, levens- en werkervaring uit mijn voorgaande banen en een goede dosis enthousiasme durfde hij het wel aan en hebben we elkaar de hand geschud. Sindsdien ben ik met veel plezier werkzaam bij Luminis. De match had in mijn ogen niet beter kunnen zijn. Ik ben gedreven mij te blijven ontwikkelen en Luminis faciliteert dit op verschillende vlakken. Zo is iedereen binnen het bedrijf enorm toegankelijk om vragen aan te stellen, worden er met grote regelmaat sessie gegeven waar collega\u2019s hun kennis delen of een nieuwe techniek willen proberen en zijn er verschillende interne trainingen om jezelf te ontwikkelen.\nEn nu?\nMijn reis van Marketing naar Software Developer heeft mij veranderd op verschillende vlakken. Het heeft mij geleerd dat met toewijding, discipline en de bereidheid te veranderen het mogelijk is om een succesvolle carri\u00e8reswitch te maken. De weg kan uitdagend en intimiderend zijn, de beloningen aan het einde zijn immens.\nZit jij op je plek? Geeft jouw werk je nog energie? Twijfel je ook een carri\u00e8reswitch te maken naar Software Developer? Mijn advies zou zijn, ga op onderzoek uit. Wacht niet langer en ontdek of het een match is. Want als dat zo is, dan staat er een prachtige reis op je te wachten.\nVoel je vooral vrij om contact te zoeken als je vragen hebt of als je gedachten wilt uitwisselen, via LinkedIn of rick.deruiter@luminis.eu.\n", "tags": [], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 39003, "title": "Eleven Software Development and Opserve become part of Luminis", "url": "https://www.luminis.eu/blog/eleven-and-opserve-become-part-of-luminis/", "updated_at": "2023-09-26T11:16:23", "body": "With due pride, Luminis announces today that Eleven Software Development and Opserve from Rijswijk become part of IT consulting company Luminis. Joining Luminis offers Eleven and Opserve the opportunity to further shape their growth ambitions.\nApeldoorn, September 26th, 2023 \u2013 During the first meeting between Luminis and the Rijswijk companies, it was immediately apparent that all parties have a number of important things in common. Values such as craftsmanship and collaboration with clients are important parts of their DNA. Within the organizations Eleven and Opserve, and Luminis, there is lots of room for knowledge sharing and learning from each other, and the offices form a nice home base for all colleagues.\nEleven has been realizing high-quality software for its customers since 2009, developing long-term relationships with them. Opserve provides management and monitoring of Linux servers and arranges migration of applications to the Cloud. Luminis was founded in 2002 and helps organizations innovate with smart solutions in the field of Cloud and data.\nDon Olsthoorn, Commercial Director Eleven and Opserve:\n\u201cWith this collaboration, we gain access to more knowledge and expertise and the synergy between the companies allows us to increase our commercial strength. We\u2019ll be able to respond even better to challenges in our client projects and our broadened perspective enables us to accelerate innovation for our customers.\u201d\nEleven customers are supported by fixed teams that proactively provide value through professionalism and customer knowledge. Not only does Eleven develop software for its customers, it also takes care of server management and migrations of applications to the Cloud through sister organization Opserve. Luminis offers customers a very similar service through Luminis Cloud Services (LCS) and will now be able to strengthen these services thanks to the capabilities of Opserve.\nRonald Voets, Managing Director Luminis:\n\u201cEleven and Opserve are great companies that, like Luminis, strive to add value for its customers as quickly as possible. The management team of Eleven and Opserve was looking to strengthen their organization to enable further growth. There is plenty of room for this within Luminis and together we will realize those growth ambitions.\u201d\nEleven and Opserve employees will continue to work for their customers from their current office in Rijswijk. This collaboration provides an opportunity to analyze the customer base of both Luminis and Eleven and Opserve in order to share the expertise and knowledge of the organizations with current and new customers.\nLuminis is part of Yuma, a new player in the field of digital transformation. Yuma combines a people-centric approach with a hands-on mentality and best-in-class expertise. Since Eleven and Opserve are now part of Luminis, they\u2019re also part of the bigger Yuma group.\nPascal Laffineur, CEO Yuma:\n\u201cEleven and Opserve, through Luminis, will strengthen Yuma\u2019s ambition and strategy to provide best-in-class expertise in digital transformations. By establishing long-term relationships with their clients in fixed teams, they are a great addition to the companies within the Yuma group.\u201d\nEleven and Opserve Board members decided to join Luminis\nAbout Eleven and Opserve\nEleven develops high-quality software and builds long-term relationships with their clients. Since 2009, Eleven realized 500+ projects with 200+ customers, such as DELTA Fiber Nederland, Koninklijke Metaalunie, Royal FloraHolland, D\u00fcmmen Orange, Stigas, Noviflora and Syngenta, and they have grown to a team of 23 professionals. Opserve is growing strong in serving Eleven customers and other companies in Linux server management, and is active as an AWS Consulting Partner to help developers migrate applications to AWS Cloud. www.eleven.nl, www.opserve.nl\nAbout Luminis\nLuminis helps organizations innovate successfully. The world of technology is constantly changing, and the complexity and speed of this change continues to increase. The organization has 150 employees, has offices in Amsterdam, Rotterdam, Arnhem and Apeldoorn, and provides its services to, for example, Alliander, Huuskes, BDR Thermea, bol.com and The Learning Network.\nAbout Yuma\nYuma is a new group in BeNeLux that puts people first in digital transformations. This best-in-class expertise is provided by the combination of companies that make up the group: XPLUS, Total Design, Luminis and BPSOLUTIONS. More companies are expected to join the group. Yuma expects a turnover of about 100 million euros this year and is working with 400 employees spread across the BeNeLux. www.weareyuma.com\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38757, "title": "Question Answering with your own data, LLMs and Java: meet Langchain4j", "url": "https://www.luminis.eu/blog/search-en/question-answering-with-your-own-data-llms-and-java-meet-langchain4j/", "updated_at": "2023-09-12T16:33:58", "body": "\nPython has become the de facto programming language when working on data related tasks. I\u2019ve recently started exploring the world of Machine Learning (ML), Large Language Models (LLMs) and vector databases. See my previous blog post about using LLMs and Generative AI to improve search.\nThis is also when I started ramping up my Python programming skills, because most companies and organizations release very neat Python client libraries for their products and services. But sometimes you want or need to use Java. In this blog post, I will explain how you can easily create a question answering system with Java and Langchain4j.\nAt the time of writing this blog post, I am doing research and preparations for conference talks and workshops with my colleague Jettro. These talks and workshops are all about creating question answering systems with the combination of LLMs and semantic (vector) search. We use a powerful tool there called\u00a0LangChain. This tool makes it very easy to connect all the pieces together. Jettro has written a blog about using that tool with their official client, which is a Python library. While the main programming language in our talks and workshops is also Python, we would also like to give the attendees that are not comfortable with Python the opportunity to work with Java. Meet Langchain4j, a Java port of LangChain. Although not yet as feature rich as the original, it already provides sufficient features to be used. I will show how easy it is to work with.\nEmbedding Model\nFirst, you want to start by defining your embedding model. This model is used to covert your text into embeddings. Embeddings are mathematical representations of your text, to be able to do calculations on them. Like calculating the similarity of pieces of text. If you want to know more about this, see this blogpost of Jettro, or watch out for the upcoming part 2 in my \u201cUsing Machine Learning to Improve Search\u201d blog series.\nLangchain4j supports multiple clients for embedding models, like OpenAI, HuggingFace or even local in process models. These are all really simple to initiate. For instance, the OpenAI version looks like this:\n...\r\n    @Qualifier(\"openaiEmbeddingModel\")\r\n    @Bean\r\n    public EmbeddingModel openaiEmbeddingModel() {\r\n        return OpenAiEmbeddingModel.builder()\r\n                .apiKey(\"your-key\")\r\n                .modelName(TEXT_EMBEDDING_ADA_002)\r\n                .build();\r\n    }\r\n...\nAnd the local in process one looks like this:\n...\r\n    @Qualifier(\"inMemoryModel\")\r\n    @Bean\r\n    public EmbeddingModel inMemoryEmbeddingModel() {\r\n        return new InProcessEmbeddingModel(ALL_MINILM_L6_V2);\r\n    }\r\n...\nThat EmbeddingModel interface holds a couple of easy to use methods you can use to convert text to embeddings. You\u2019ll see that later when we\u2019re going to create an embedding for our question.\nEmbedding Store\nAfter creating the embeddings, they need to be stored in an embedding store. This could be an in memory embeddings store, but Langchain4j also supports a few vector databases, like Weaviate and PineCone. Just like the embedding models, the setup for the stores is also very easy. This is what it looks like for an in memory store:\n...\r\n    @Qualifier(\"inMemoryEmbeddingStore\")\r\n    @Bean\r\n    public EmbeddingStore inMemoryEmbeddingStore() {\r\n        return new InMemoryEmbeddingStore<>();\r\n    }\r\n...\nBut if you want to go with Weaviate, for example, it\u2019s not complex either:\n...\r\n    @Qualifier(\"weaviateEmbeddingStore\")\r\n    @Bean\r\n    public EmbeddingStore weaviateEmbeddingStore() {\r\n        return WeaviateEmbeddingStore.builder()\r\n                .apiKey(\"your-key\")\r\n                .scheme(\"https\")\r\n                .host(\"your.weaviate.host\")\r\n                .build();\r\n    }\r\n...\nThe WeaviateEmbeddingStore builder has a few more methods which you can use, you can explore those in the example section.\nData Ingestion\nWhen you have your embedding model and store ready, you want to ingest your data in the embedding store. This can be done with an EmbeddingStoreIngestor:\n...\r\n    Document document = Document.from(\"text\");\r\n    DocumentSplitter documentSplitter = DocumentSplitters.recursive(300);\r\n    EmbeddingStoreIngestor ingestor = EmbeddingStoreIngestor.builder()\r\n            .documentSplitter(documentSplitter)\r\n            .embeddingModel(embeddingModel)\r\n            .embeddingStore(embeddingStore)\r\n            .build();\r\n    ingestor.ingest(document);\r\n...\nIn this example, I created a Document object from the string \u201ctext\u201d, but in reality you would probably have some larger text there. Langchain4j includes some parsers for PDF or DocX (MS Word) and some other types of files. These parsers also output a Document object which can be used to ingest into the store.\nA DocumentSplitter is needed to cut your long text into smaller chunks (300 characters in this case). The Python LangChain library also supports overlap in chunks, but Langchain4j doesn\u2019t support that (yet). Cutting your text into chunks is an important step in vector search, because embeddings of larger chunks tend to be less accurate. Also, sending large texts results in higher token counts,\u00a0which will increase the cost of your language model if you use OpenAI for example. You should experiment with this thoroughly until you find the best chunk size for your data and use case.\nChat Language Model\nNow we have an embedding model and a store with data in it. Next thing is to set up a chat language model to convert our search results into an actual answer to the question asked. For this part, we need an LLM. Langchain4j currently supports clients for OpenAI and HuggingFace. Here is an example for OpenAI:\n...\r\n    @Qualifier(\"openaiChatLanguageModel\")\r\n    @Bean\r\n    public ChatLanguageModel openaiChatLanguageModel() {\r\n        return OpenAiChatModel.builder()\r\n                .apiKey(\"your-key\")\r\n                .modelName(GPT_3_5_TURBO)\r\n                .temperature(0.8)\r\n                .timeout(ofSeconds(15))\r\n                .maxRetries(3)\r\n                .logResponses(true)\r\n                .logRequests(true)\r\n                .build();\r\n    }\r\n...\nHere you can set timeout, retries, choose what model you want to use and set the sampling temperature. The sampling temperature is a parameter for language models that governs the randomness/creativity of the responses of the model. This should be a value between 0 and 1. Higher values like 0.8 will make the output more random. Lower values like 0.2 will make it more focused and deterministic, meaning you almost always get the same response to a given prompt.\nGetting Answers\nWhen all of the above is done, it\u2019s time to put all the pieces together to query for relevant texts, send those together with the question to the ChatLanguageModel and let it write an answer to your question from the provided information.\n...\r\n    public String askQuestion(String question) {\r\n        Embedding queryEmbedding = embeddingModel.embed(question);\r\n        List<EmbeddingMatch> relevant = embeddingStore.findRelevant(queryEmbedding, 4, 0.8);\r\n\r\n        Map<String, Object> variables = new HashMap<>();\r\n        variables.put(\"question\", question);\r\n        variables.put(\"information\", relevant.stream().map(match -> match.embedded().text()).collect(Collectors.joining(\"\\n\\n\")));\r\n\r\n        PrompTemplate promptTemplate = PromptTemplate.from(\r\n                \"Answer the following question to the best of your abilities: \\\"{{question}}\\\"\\n\\n\" +\r\n                        \"Base your answer on the following information:\\n{{information}}\");\r\n        Prompt prompt = promptTemplate.apply(variables);\r\n\r\n        LOGGER.info(\"Sending following prompt to LLM:\\n{}\", prompt.text());\r\n\r\n        AiMessage aiMessage = chatLanguageModel.sendUserMessage(prompt.toUserMessage());\r\n        return aiMessage.text();\r\n    }\r\n...\nFirst, we use the EmbeddingModel to create an embedding for the question string, so it can be used to find semantically relevant pieces of text from the EmbeddingStore. The two other parameters in the findRelevant method are respectively the max amount of results we want to get back (4) and the minimal score the results should have (0.8). For these values you should experiment and find the sweet spot that works best for your use case and data.\nThe next step is to create a Map of variables which holds the question and the relevant texts (information). This map is then applied to the PromptTemplate where you specify the text (or prompt) that goes to the ChatLanguageModel. Prompt engineering, the art of writing instructions for your language model, is a very important step to get the answers you want in the format you want. I would really advise to spend time on writing the best prompt for your use case and data. Deeplearning.ai has a great free course on this.\nLast but not least we send the prompt to the ChatLanguageModel and get back an answer to your question from the information you provide yourself. This could also mean your data does not contain anything related to your question and the answer is something like \u201cI could not answer this question\u201d, depending on your settings and prompt.\nFinal Words\nAs you could see it doesn\u2019t really take much coding to create an awesome question answering system in Java with the use of Langchain4j. In my GitHub repository you can find all the above code with a couple REST endpoints. One to ingest a PDF created from the Devoxx Belgium conference FAQ page. Another one to ask questions which will use the information of that FAQ page as source. A question like \u201cWhat is the address of the venue?\u201d will result in something like \u201cThe address of the venue is Groenendaallaan 394, 2030 Antwerp, Belgium.\u201d, which is very cool.\nKeep in mind that for a production ready system you need to do a lot of investigation and experimentation. You need to choose or train the right embedding model, pick the ideal chunk size for your data, find the most fitting settings, like temperature for ChatLanguageModel and minimum score for matches. To see if your system is performing well you need to have a set of questions available that could be asked by users and verify if the generated answers are similar to what you expect them to be.\nThat being said, with the information in this post you can already start experimenting!\n", "tags": ["LangChain", "Large Language Models", "search", "weaviate"], "categories": ["Blog", "Search"]}
{"post_id": 38642, "title": "Vector Databases Unveiled: The Heart of Modern Data Processing", "url": "https://www.luminis.eu/blog/vector-databases/", "updated_at": "2023-09-18T13:38:19", "body": "In this blog post, we dive deeper into vector databases. and why they are a hot topic. Data can be represented in many ways. Take an analog calendar as an example, you can see all the days stored on lines that represent the weeks, and those weeks are packed on a month. Group 12 of these packs and you have a full calendar. Add some cat pics, update the data, and you can sell it every January. This is a silly example, but I hope you get the idea behind it.\nWe have stored data since the writing was invented. First writings were not epic poems, but just a list of objects in a warehouse or shiploads. Since the 60s, we have used databases to store and manage data. It is more efficient, more consistent, more secure, more scalable, etc. That said, the way the data is stored on these databases will directly affect the benefits offered. Continuing with the first example, calendars are efficient because they are faster and consistent. It is pragmatic to daily life to know that today is the 7th of September, and not the 250th of 2023.\nAs time passed, the needs and the benefits you can get from databases changed and expanded. Let\u2019s take relational databases as an example. If you store \u201cMath 101\u201d on the table subjects and create a relation with the table subjects, you are able to get all the students enlisted in the course. This \u201crelation\u201d creates a feeling of belonging, something that is not there, is not real, but helps us create meaning of the data, and organize it with a purpose. It helps us recreate the model we have in our minds.\nIntroducing Vector Databases\nThis year, vector databases have been a hot topic. We discuss later why and which specific problems they solve, but at this moment, the important thing is to understand what they store, the vectors.\nVectors are mathematical objects that represent direction and magnitude in space. Us humans have problems if we try to imagine more than 3 dimensions, but computers don\u2019t have that problem, in fact, vectors are highly versatile and almost everything can be represented as a vector.\nLet\u2019s say that you have an image and you want to store it. One way to do this is to split the image into pixels and assign a number based on a scale. By doing this, we will have another way of representing the data: instead of an image, we have a high-dimensional vector.\nImage via pinecone.io\nSo, as you probably already imagined, vector databases are databases made for storing vectors. But weren\u2019t vectors just numbers? Why can we not just store it on an SQL database?\nWell, the short answer is that you can, and (even better news) there are databases you already know that support vectors, like OpenSearch (via \u00a0k-NN), PostgreSQL (via PGVector) or ElasticSearch (via Knn search). That said, you will probably lose all the benefits of using dedicated vector databases.\nVector databases unveiled: the benefits\nImagine you have a basket with fruits. You pick an orange, and you need to choose another fruit similar to the orange. Will you pick an apple? Probably not. Maybe a lemon? Maybe, both are acid, and the skin is similar. Vector databases can help you with this task.\nSemantic search allows you to get values that are more related to others depending on the context. If relational databases establish a relation between the different tables, which creates a sense of belonging, the vector database does the same for similarity. The examples in this picture can help you understand this concept.\nImage via developers.google.com\nSimilarity search is one of the strongest points vector databases have. It\u2019s one of the reasons why databases are a hot topic right now: it allows you to store text (as a vector) and based on this use LLMs like ChatGPT or LLaMA to ask questions that can be answered based on a simulated context or just retrieve similar documents. Some databases that get especially good results are Weaviate, Pinecone and DeepLake. On the other hand, if we want a fast implementation with Python, you could use Chroma.\nThe same logic that applies to similarity can be used to get recommendation engines. These engines provide custom suggestions or recommendations to users based on their preferences, behavior, or characteristics, which in this case will be stored as vectors. Weaviate (they explain how on this post) is a database especially good on this, but other mentioned databases can also fit on this.\nAnother useful case for vector databases is real-time geospatial search and analytics. This involves the instantaneous querying, retrieval, and analysis of location-based data. This is particularly useful in applications where time-sensitive information related to geographic locations needs to be processed and visualized quickly. Optimize routes, tracking vehicles, emergency responses, etc. are some real problems that can benefit from vector databases, and the best option for this scenario is Qdrant.\nConclusion\nThis blog pretends to be an introduction to vector databases. Understanding their fundamentals and some use cases is more important than just using them, because everybody does. That said, we strongly encourage you to use them when working with LLMs, as it is the standard right now, but make sure you pick the right option before you start developing.\nIf you check out the top tending repos on github, you will see that most of them are trying to create an AGI (Artificial General Intelligence), like Auto-GPT, babyagi or jarvis (Microsoft). These tools make use of LLMs and vector databases because they allow them to create long-term memory, storing the prompts as vectors and using these prompts themselves to generate even more content with their own context.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 38716, "title": "Decoding Similarity Search with FAISS: A Practical Approach", "url": "https://www.luminis.eu/blog/search-en/decoding-similarity-search-with-faiss-a-practical-approach/", "updated_at": "2023-08-29T13:30:09", "body": "I am preparing for a series of conference talks and workshops on Retrieval Augmented Generation. With my strong information retrieval (search) background, I have been working on vector-based technologies. I found some excellent resources on using FAISS from James Briggs.\nIn this blog post, you read about FAISS and your options for implementing similarity search using FAISS. I decided to use the Python Notebook approach to enable you to tag along.\nSemantic Search\nBefore we dive into FAISS, let us spend some time on Semantic Search. What is Semantic Search? I assume you are used to search engines on websites and most likely Google. Most of the site search engines use search engines like Elasticsearch, OpenSearch, and Solr. Most websites may not even be. A lot of sites still use SQL queries to perform searches. Most of these engines use some form of Lexical search or keyword matching. As a user, you must type precisely the terms known in the data. More advanced engines use mechanisms to overcome typos and synonyms. These engines need help understanding what you are talking about. They need to get the semantic meaning of the words you use in the query.\nWe have all been trained by using these websites only to enter a few words in a search bar. If you, as a company, are lucky, your users will try other words when you do not present them with the right results. Suppose you better understand the semantic meaning of the user and match concepts rather than keywords. In that case, you can present much better results, positively impacting your user experience and business outcome.\nWe need a way to match concepts, but how do we determine this meaning or similarity between concepts in pieces of text? One answer is transforming text into mathematical representations, typically vectors. Once our data is represented as vectors in a multi-dimensional space, finding similar items becomes a matter of calculating distances between these vectors. Here\u2019s where algorithms like k-nearest neighbors (kNN) come into play.\nSimilarity search using\u00a0kNN\nCalculation is more straightforward with numbers than with text. The moment we started talking about calculating distances, we stepped over many concepts. First, we need to transform text into a vector of numbers. Second, we need to calculate distances between these vectors of numbers. We discuss converting text into numbers in the section about embeddings.\nFor now, imagine we have the vectors. In the image below, we simplified them into a two-dimensional space. However, in reality, this is a lot higher. The vectors we work with have 1536 dimensions. More about that later in the embeddings section.\nkNN diagram for BrickHeadz titles\nThere are different formulas to calculate the similarity between vectors. Examples are l2, cosine similarity, and inner product.\nTo get an intuition on the distance metrics, below you get an idea for calculating the similarity between the vector for voetbal and Frodo & Gollem.\nCalculate L2 distance for two vectors, the query voetbal and frodo\nNow, we can compare two vectors and calculate how similar they are. We can use brute force and exact calculations to find the most similar vectors. This is okay if you have less than 100k documents. Approximate Nearest Neighbours is available for speed. It will be less accurate. You trade in accuracy for speed. For most models, you can tune this balance. We discuss ANN, HNSW, IVF in another section.\nThe next sections give more background information on the topics discussed here. Together with the Notebook, these sections give you a fair understanding of working with vector similarity.\nEmbeddings\nDid you look at the Notebook? In one block, we create a method called create_embedding. The method uses the OpenAI API. Through this API, we use the model text-embedding-ada-002. This model accepts text and outputs a vector of dimension 1536.\nTraining such a model is easier than training Large Language Models. However, taking an existing embedding model is usually easier with a larger corpus of texts. Training your custom model can improve results if you have a corpus with many non-familiar words. Techniques like word2vec that train on guessing missing words or next words are well documented. I like this tutorial from Tensorflow.\nDistance Metrics\nThe similarity between two vectors is calculated using a distance metric. You can choose different metrics. L2 is a good first choice. Similarity is used a lot as well. The inner product is another good choice. I am not going into all the details. Choosing the best model is a matter of testing them.\nL2 or Euclidean distance is also called the straight line distance between two vectors. Values must be normalized. The metric performs poorly if the dimensions increase to tens of thousands.\nCosine similarity is a good second choice that only measures the angle between vectors. It does not consider the lengths of vectors.\nThe inner product, or dot product, is a specialization of the cosine similarity. Faiss uses this next to L2 as a standard metric. IP performs better on higher-dimension vectors.\nThe Weaviate documentation has a nice overview of distance metrics.\nFAISS Indexes\nThe beginning of this blog post shows how to work with the Flat index of FAISS. This index is called exhaustive, and the results are exact. They are exact, as we determine the distance for all available vectors to the query\u2019s vector. You can imagine that the performance for such a query becomes a problem with too many vectors. If speed is important, we need another index. Meet the approximate indexes. These indexes are not exact anymore, but they are faster. They have a way of limiting the scope of the search to perform. Now, the trick is to find the best balance between performance and quality of results.\nHNSW\u200a-\u200aHierarchical Navigable Small\u00a0World\nAn approach that uses a graph to find the vector that best matches the query vector. The graph vertices have several friends. The graph traverses the friend closest to our query if it is closer to our query than the current vector. Combining multiple graphs that become more accurate per layer creates a hierarchy of graphs. The highest layer is the entry point. When we reach the closest distance, we go to the connected lower layer connected to the vertex in the top layer.\nWe can configure the amount of friends each vertex has. More friends mean higher accuracy, but lower performance. Imagine you are searching for a kitchen. You ask five companies for an offer and choose the lowest. If you ask 100 companies for their price, you can get a better price. It will take a lot of effort, though.\nYou can read more about HNSW and Faiss in this post from Pinecone.\nIVF\u200a-\u200aInverted File System or\u00a0Index\nIVF is an interesting clustering technique to select a subset of the vectors to calculate the best matches from. It uses a Voronoi diagram to select cells with vectors to consider. The idea is to have several centroids in cells with vectors closer to that centroid than any other centroid. Next, we find the most similar centroid to the query and select that cell to find the most similar vectors.\nSometimes, we miss a similar vector right at a cell\u2019s border. This hurts the accuracy of the index. Therefore, we can select multiple cells to limit the chance of missing a good vector. That does increase the latency of the index. So again, we have to find the right balance.\nFinal words\nWorking with vector stores like Weaviate, Amazon OpenSearch Service, and Pinecone gives you much power. Without the knowledge of the technology supporting these data stores, you cannot create the best-performing solution. You must understand the trade-off between performance and accuracy\u200a-\u200a the difference between exhaustive similarity calculation and exact results with approximate similarity.\nTherefore, when working with one of those vector stores, read the documentation for the right parameters to find the optimal balance of accuracy and performance for your specific application.\nReferences\nPinecone has a nice series of posts bundled in Faiss: The Missing Manual.\nJames Briggs has created some incredible YouTube videos. His series about similarity search accompanies the posts from the link above.\nNice overview of vector search with Weaviate that explains most concepts in the Weaviate context.\nExtensive documentation for the kNN plugin in OpenSearch.\n", "tags": ["faiss", "knn", "relevance", "search", "similarity"], "categories": ["Blog", "Search"]}
{"post_id": 38593, "title": "Hands-On with Observability: Installing Elasticsearch and Kibana", "url": "https://www.luminis.eu/blog/hands-on-with-observability-installing-elasticsearch-and-kibana/", "updated_at": "2023-08-18T10:23:56", "body": "Following our previous discussion on the fundamentals of Elastic Stack and observability while using Elastic Stack, we\u2019re set to take our exploration to the next level. In this chapter, we shift our focus from theory to practice, diving deep into the setup and utilization of Elasticsearch and Kibana via Docker.\nBefore we begin, ensure you have Docker installed and running on your machine. If not, you can download it here.\nDiving into Observability: The Docker-Facilitated Elasticsearch and Kibana Setup\nAs we embark on this journey to enhance application observability with Elasticsearch, our first task involves creating an Elasticsearch cluster tailored to store and process our specific data. This is generally achieved through a series of steps, including the installation of Elasticsearch, the configuration of indices and mappings, as well as the establishment of data ingestion pipelines to collect and manipulate data from a variety of sources. Today, however, our focus is centered on the initial setup \u2014 the installation of Elasticsearch and Kibana facilitated by Docker.\nDeploying Elasticsearch Using Docker CLI\nYour journey into application observability with Elasticsearch starts with setting up an Elasticsearch cluster. To do this, pull the Docker image with the following command:\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:8.7.0\nNow that you have the image, you can run the following commands to start a single-node Elasticsearch cluster for development:\n# create a new docket network for Elasticsearch and Kibana\r\ndocker network create elastic\r\n\r\n# start Elasticsearch in Docker. Generates credentials --> Save it somewhere!\r\ndocker run --name es01 --net elastic -p 9200:9200 -it docker.elastic.co/elasticsearch/elasticsearch:8.7.0\r\n\r\n# copy the security certificate from Docker to local\r\ndocker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .\r\n\r\n# open a new terminal and verify that you can connect to your cluster\r\ncurl --cacert http_ca.crt -u elastic https://localhost:9200\nMake sure you copy the generated password and enrollment token and save them in a secure location. These values are shown only when you start Elasticsearch for the first time. You\u2019ll use these to enroll Kibana with your Elasticsearch cluster and log in.\nOnce the Elasticsearch cluster is set up and configured, developers can use tools like Kibana to create dashboards and charts to visualize the data and identify trends and issues. Kibana is a source-available data visualization dashboard software for Elasticsearch.\nDeploying Kibana Using Docker CLI\nWith the Elasticsearch cluster now set up, we\u2019ll turn our attention to Kibana, a powerful tool for visualizing Elasticsearch data:\n# start Kibana in Docker and connect it to existing Elasticsearch cluster\r\ndocker run --name kib-01 --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.7.0\nThis will start Kibana in docker and will output the next to the terminal when done:\n\nKibana has not been configured.\nGo to\u00a0http://0.0.0.0:5601/?code=579012\u00a0to get started.\n\nWhen you click on the link you will see this screen:\n\nEnter your enrollment token and generated password from before and you will get access to Kibana.\n\nRolling Up Our Sleeves: Diving Into Practical Application\nAs we pivot from theory to practice, our narrative unfolds around two main applications: an E-commerce Website (Application A) and an Inventory Management Service (Application B).\n\nE-commerce Website (Application A): it could include features like user registration, product browsing, adding to cart, and purchase transactions.\nInventory Management Service (Application B): This could be a back-end microservice that manages the inventory for the e-commerce website. It could have features like adding new stock, updating existing stock, marking stock as expired, etc.\n\n\n\u00a0\nIntroducing Dev Tools: Our Gateway to Observability in Elasticsearch\nEager to start communicating directly with Elasticsearch? You\u2019re in luck! By navigating to the Management \u2192 Dev Tools in Kibana, you\u2019ll gain direct access. This powerful tool enables us to execute HTTP requests against the Elasticsearch REST API, facilitating operations like creating indices, adding documents, and running queries. We\u2019ll now create two indices to accommodate our different logs: \u2018ecommerce_app_logs\u2019 for the E-commerce application and \u2018inventory_management_logs\u2019 for the Inventory Management service.\nDeploying Indices: Laying the Groundwork\nInitiate your journey with index creation using the following commands in Dev Tools:\nPUT /ecommerce_app_logs\r\n{\r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"application-name\": {\r\n        \"type\": \"text\"\r\n      },\r\n      \"timestamp\": {\r\n        \"type\": \"date\"\r\n      },\r\n      \"log_level\": {\r\n        \"type\": \"keyword\"\r\n      },\r\n      \"message\": {\r\n        \"type\": \"text\"\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /inventory_management_logs\r\n{\r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"application-name\": {\r\n        \"type\": \"text\"\r\n      },\r\n      \"timestamp\": {\r\n        \"type\": \"date\"\r\n      },\r\n      \"log_level\": {\r\n        \"type\": \"keyword\"\r\n      },\r\n      \"message\": {\r\n        \"type\": \"text\"\r\n      }\r\n    }\r\n  }\r\n}\nThese commands generate the necessary environment for data ingestion, defining the properties of each document that will be inserted into these indices.\nHowever, in production environments, it\u2019s often more scalable and maintainable to utilize index templates. Index templates provide a way to automatically set up mappings, settings, and aliases as new indices are created. By adopting index templates, you can ensure that every new index conforms to a pre-defined structure, making your data ingestion process more streamlined and consistent. This not only reduces the risk of manual configuration errors but also simplifies operations when dealing with a multitude of similar indices. If you\u2019re eager to learn about index templates, you can explore this subject further in this blog from Luminis colleague Jettro Coenradie. Don\u2019t worry if you prefer to stay tuned here, as index templates will also be covered later in this series.\nFeeding Data: Populating Our Indices\nNow that we made these indices we can start adding documents/logs to them.\nPOST /ecommerce_app_logs/_doc\r\n{\r\n  \"application-name\": \"ecommerce_app\",\r\n  \"timestamp\": \"2023-07-24T14:00:23\",\r\n  \"log_level\": \"INFO\",\r\n  \"message\": \"User 'john_doe' successfully logged in\"\r\n}\r\n\r\nPOST /inventory_management_logs/_doc\r\n{\r\n  \"application-name\": \"inventory_management\",\r\n  \"timestamp\": \"2023-07-24T10:15:00\",\r\n  \"log_level\": \"INFO\",\r\n  \"message\": \"New shipment of 'Samsung Galaxy S20' arrived. Quantity: 100\"\r\n}\nAs we progress, we\u2019ll explore automated ways of populating these indices, particularly through tools such as Filebeat and Logstash.\nPeeking at Our Data: Exploring the Discover Tab\nCongratulations on taking your first steps toward indexing and data ingestion. To view your logs, head over to the Analytics \u2192 Discover tab in Kibana. All your logs appear here, but should you need to focus on a specific application, simply add a filter. This not only organizes your data but also sets the stage for more advanced data manipulation techniques we\u2019ll delve into in the future.\n\n\nAssessing Our Initial Journey Into Observability\nHow did your journey into observability start? Was your path clear and straightforward, or did you grapple with unexpected obstacles? Either way, remember that every hurdle overcome is a stepping stone toward mastery. We\u2019d love to hear your stories, your triumphs, and even your challenges. So, feel free to share your experiences in the comments below!\nAnd don\u2019t forget: our exploration into the world of Elasticsearch-driven observability is just beginning. In our next post, we\u2019ll expand our knowledge, shifting from the ingestion of data to its management. We\u2019ll delve deeper into managing the index life-cycle and ensuring our data remains organized and accessible, even as it grows. So, stay tuned for more hands-on guidance in our observability series. Together, let\u2019s unlock the power of data and transform how we see our applications.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 38528, "title": "Data Quality Series, part 3: Overview of Data Lineage", "url": "https://www.luminis.eu/blog/data-quality-series-part-3-overview-of-data-lineage/", "updated_at": "2023-08-13T12:24:28", "body": "In this article, we delve into the often overlooked, but crucial aspect of data quality \u2013 data lineage. Data lineage records the flow of data and all the transformations throughout its life-cycle, from source to destination. Understanding this is vital for maintaining data integrity and transparency in data processes, making it an essential component of the data quality workflow.\nWe previously explored the significance of data quality in the blog post, \u201cIntroduction to Data Quality\u201d, which emphasizes the importance of clean and standardized data for accurate analysis and decision-making. If you are interested in getting more hands-on experience with data quality testing, read the second blog post in this series: \u201cData Quality Testing with Deequ in Spark\u201d.\nNow, we take a closer look at data lineage, its benefits, and how it contributes to maintaining data reliability. As a whole, we aim to compile a comprehensive overview of important concepts to guide a user who is considering implementing data lineage within their organization.\nThe rest of the blog is structured as follows:\n\nWhat is Data Lineage?\nRequirements for Effective Data Lineage\nBenefits of Data Lineage\nTools for Data Lineage\n\nWhat is it?\nTraditionally, the data resided in a data warehouse with only a few connections to external systems. Today, as the demand has grown, the data flows between a multitude of systems, teams, and (external) organizations. Consequently, it is easy to overlook the impact of a single change somewhere in the life-cycle of the data.\nData lineage refers to the steps a dataset took to reach its current state. It encompasses the entire life-cycle of data, from its creation or ingestion to its consumption and usage in various processes and applications. By understanding data lineage, organizations gain visibility into how data is transformed and manipulated as it moves through different systems, processes, and transformations. It is an important tool for data engineers to debug potential issues in the data flow processes.\nThere are two primary types of data lineage: table-level lineage and field-level lineage. Table-level lineage provides an overview of the tables or datasets involved in the data flow, whereas field-level lineage goes deeper, tracking the lineage of individual fields or columns within those tables.\nRequirements for Data Lineage\nData lineage works like documentation: when done right, it should not put an additional burden on your development workflow, it should only enhance o. To harness the full potential, there are some general guidelines that should be satisfied, as described in the Data Quality Fundamentals book by Moses, et al.:\n\nFast Time to Value: Abstracting the relationships between data objects down to the field level is crucial for quick remediation. Simply tracking at the table level may be too broad and insufficient for understanding the impact of changes and identifying specific issues. (split point)\nSecure by Design: Data lineage shouldn\u2019t directly access the data. Instead, it should rely on metadata, logs, and queries to gather information about the data flow. This simplifies the design, as well as ensures that no potentially private business data leaks into your documentation.\nAutomation: Manual maintenance becomes increasingly challenging and error-prone as data pipelines become more complex. Investing in an automated data lineage generation approach saves time and reduces the risk of human error.\nIntegration with Popular Data Tools: A data project typically orchestrates data flow between multiple tools. The lineage tracking should seamlessly integrate with these technologies to create a unified view of your business, rather than dictating your workflow.\n\nThe benefits\nImplementing robust data lineage practices offers several benefits to organizations:\n\nCommunication and Transparency: It acts as a communication channel between data producers and data consumers, helping to bridge the gap between different teams by providing a clear understanding of the impact of broken or changed data on downstream consumers.\nImproved Data Quality and Trust: Data lineage allows organizations to build trust in their data assets. By providing visibility into the data\u2019s journey and transformation, it enhances data quality, reliability, and accuracy. This, in turn, promotes better decision-making based on trustworthy information.\nCompliance and Auditability: Data lineage supports compliance efforts by enabling organizations to demonstrate adherence to regulations, such as the General Data Protection Regulation (GDPR). It provides an audit trail of data usage and ensures transparency in data management practices.\n\nSome practical applications of data lineage in use include:\n\nDebugging: When issues arise in data analysis or reporting, data lineage can be invaluable for root cause analysis. By tracing the lineage of problematic data, analysts can identify where the issue originated and take corrective action more efficiently.\nReducing Technical Debt: Data lineage helps identify columns or fields that are no longer in use or have been deprecated. By marking and propagating these changes downstream, organizations can reduce technical debt and streamline their data pipelines.\nGovernance: With privacy regulations and data governance becoming increasingly important, data lineage provides a way to track how personally identifiable information (PII) is used within an organization. It enables organizations to understand who has access to sensitive data, how it is utilized, and ensures compliance with data protection regulations.\n\nTools\nNow, let\u2019s explore some powerful tools that can help you establish and maintain a seamless data lineage process.\nOpenLineage\nOpenLineage is an emerging industry standard for data lineage tracking that is gaining traction. It is supported by the Linux Foundation, Atronomer, Collibra. It aims to establish a unified framework for capturing, managing, and sharing metadata across various tools and platforms. OpenLineage provides a consistent way to represent data lineage, making it easier to integrate with different systems and tools. You can easily incorporate it with any tool by submitting events to its API endpoint.\nOne exciting integration with OpenLineage is the combination with Marquez, a metadata service that tracks data workflows and lineage, open-sourced by WeWork. Together, they offer a simple, yet powerful solution to maintain a comprehensive and standardized view of data lineage. With this integration, you can easily trace data transformations, dependencies, and the origin of data through various data pipelines.\nMicrosoft Purview\nMicrosoft Purview is a comprehensive data governance and data cataloging solution that also offers data lineage capabilities. Purview is part of the Microsoft Azure ecosystem and integrates well with other Azure services. It allows organizations to discover, classify, and understand their data assets, making it easier to implement robust data lineage practices.\nOne notable feature of Purview is its integration with Azure Data Factory (ADF). While ADF provides some level of data lineage tracking through job dependencies, Purview enhances this functionality by offering a more unified and visual representation across the data ecosystem.\n\u00a0\nData Lineage in Microsoft Purview\nDatahub\nDatahub is a versatile data platform that provides robust data lineage capabilities, among other features. It offers extensive integration support, making it suitable for various data environments. While it is open source, the installation is heavy and requires both Kafka and Elasticsearch to operate, making it a tough choice for small projects.\nDatahub can handle large-scale data lineage requirements. Data engineers and data analysts can rely on Datahub to trace data paths, identify data inconsistencies, and ensure data quality across their pipelines, making it a one-stop shop data quality tool.\nDataset Lineage overview in DataHub\nSpline\nIf your organization mainly uses Apache Spark for data processing, Spline is an excellent tool to consider for data lineage tracking. Spline offers the ability to join lineage across multiple datasets, providing a comprehensive view of how data transformations take place.\nOne notable advantage of Spline is its compatibility with OpenLineage (currently as POC). This allows you to leverage OpenLineage\u2019s ecosystem to combine lineage across environments for visualization.\nDataset High Level Data Lineage overview in Spline UI\nDBT (Data Build Tool) and Dagster\nDBT and Dagster are two powerful data tools that emphasize data-first practices and can significantly contribute to your data lineage efforts.\nThe first one mentioned, DBT, is a popular data transformation tool that enables data engineers and analysts to model, transform, and organize data in a structured manner. By leveraging DBT\u2019s features, you can ensure that your data lineage accurately reflects data transformations and helps maintain data integrity.\nOn the other hand, Dagster is a data orchestration tool designed to facilitate the development and management of data workflows. With Dagster, you can build robust data pipelines that capture data lineage effectively, making it easier to identify and resolve issues in your data processes.\nData Graph in Dagster Combining FiveTran, DBT and Tensorflow Assets\nApache Airflow\nApache Airflow is a workflow management platform that, while not a strict data lineage tool, supports data lineage indirectly through its connectors and integrations. By utilizing these connectors, you can associate data pipelines with metadata about the data sources, dependencies, and transformations.\nWhile Airflow\u2019s data lineage capabilities might not be as sophisticated as some dedicated data lineage tools, it can still play a significant role in providing visibility into your data workflows and their impact on downstream processes.\nConclusion\nIn conclusion, data lineage is a vital aspect of data quality, providing transparency in processes and transformations. Building your lineage with best practices in mind, such as automation and the correct level of abstraction, brings a multitude of benefits like improved communication, enhanced data quality, and compliance support.\nPowerful tools are available for establishing and maintaining data lineage, offering unified frameworks for metadata management and comprehensive tracking across workflows.\nEmbracing data lineage and leveraging these tools empowers everyone within the organization to make better decisions, ensure data reliability, and build trust in their data.\n", "tags": ["Data Engineering", "Data Lineage", "Data Quality", "DataOps"], "categories": ["Blog", "Data"]}
{"post_id": 38492, "title": "How to improve Observability using the Elastic Stack", "url": "https://www.luminis.eu/blog/how-to-improve-observability-using-the-elastic-stack/", "updated_at": "2023-08-14T14:05:41", "body": "In the fast-paced world of modern software applications, ensuring a smooth and reliable user experience is paramount. Currently, I am working on improving the observability of the applications of a customer, using the Elastic stack. In this blog, I take you along with me on that journey.\nBy improving observability, the customer hopes to identify and resolve application issues more quickly, ensuring a smooth and reliable user experience for their customers. In this blog, you read about what observability is and how we can achieve it by implementing the Elastic Stack. This blog post is part of a series in which we explore a range of use cases, each complemented by an appropriate solution.\nNext in the series on Observability using Elastic Stack:\nThroughout this series, I spotlight various use cases and challenges to demonstrate how you can effectively leverage Elasticsearch within your business for better observability. Key topics that I dive into include:\n\nGetting started with the ELK Stack and Docker\nManaging the index life cycle\nCreating Kibana dashboards\nImplementing alerting systems\nEmploying monitoring techniques with Application Performance Monitoring (APM)\nSecuring Your Elastic Stack: Tips for Optimal Security\nLeveraging Machine Learning Features in the Elastic Stack\n\nObservability: What Does It Mean?\nThe ability to monitor and understand the behavior of a system through the collection and analysis of metrics, logs, and tracing data. That is a general definition of observability. Usually, observability is divided into three pillars. These pillars of observability refer to the three critical aspects of monitoring and understanding the behavior of a system: metrics, logging, and tracing.\n\n\nMetrics are numerical values that describe the performance and behavior of a system. They can be used to monitor the health and capacity of a system, as well as identify trends and patterns over time.\nLogging is the process of recording events and messages generated by a system. Logs can track a system\u2019s behavior, identify issues, and provide context for debugging and troubleshooting.\nTracing is following a request or operation flow through a distributed system. Tracing can help identify bottlenecks and performance issues, as well as provide a complete view of how a system is functioning.\n\nTogether, these three pillars of observability provide a comprehensive understanding of the behavior and performance of a system. They are essential for ensuring the reliability and stability of modern software applications. The Elastic Stack is a powerful tool that can help with application observability by providing a central location for storing, analyzing, and visualizing various data types. Now, let\u2019s dive deeper into understanding this powerful resource.\nThe Elastic Stack: A Closer Look\nThe Elastic Stack, formerly known as the ELK Stack, is a robust suite of open-source software tools designed to take data from any source, in any format. It enables users to search, analyze, and visualize that data in real time. The Elastic Stack is composed of four main components: Elasticsearch, Logstash, Kibana, and Beats.\n\n\nElasticsearch is the heart of the Elastic Stack. It is a distributed, RESTful search and analytics engine capable of handling a wide variety of data types, including textual, numerical, geospatial, structured, and unstructured. You can use Elasticsearch for log and event data storage, but also for full-text search, distributed search, and analytics.\nLogstash is a server-side data processing pipeline that accepts data from multiple sources simultaneously, transforms it, and then sends it to a \u201cstash\u201d like Elasticsearch. It\u2019s extremely useful in gathering logging data and other event data from different sources and provides filters to transform the data.\nKibana is the visualization layer of the Elastic Stack. It allows you to explore your Elasticsearch log data through a web interface, and build dashboards that highlight the relationships between your data over time. Kibana also allows for the management of Elasticsearch indices and the manipulation of the data contained within.\nBeats is a platform for single-purpose data shippers. They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch. Each Beat is designed for a specific data type, such as system metrics, network metrics, or log files.\n\nTogether, these components provide a flexible, scalable, and effective way to collect, process, store, search, analyze, and visualize data in real time. Therefore, they are invaluable in improving observability within systems.\nNext Up: Hands-On with Elastic\nThe Elastic Stack offers a powerful suite of tools that provide valuable insights into the behavior and performance of your systems, enhancing observability and consequently, improving operational efficiency. But the true power of these tools lies not only in understanding them conceptually, but also in employing them hands-on. So, what\u2019s the next step in our journey? It\u2019s time to roll up our sleeves and get practical. We explore how to implement each component and leverage their combined capabilities for effective system observability. Get ready for an insightful and dynamic journey \u2014 you don\u2019t want to miss it!\nIf you\u2019re an experienced reader not planning to follow this series to the end, we\u2019d love to hear from you as well! How have you harnessed the power of the Elastic Stack to enhance observability in your projects? Share your insights and stories in the comments below. Your experience could be the inspiration or solution someone else is seeking!\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 38432, "title": "Accelerate Graduation: deelnemers vieren afsluiting met een vlucht en feestelijke middag", "url": "https://www.luminis.eu/blog/accelerate-graduation-deelnemers-vieren-afsluiting-met-een-vlucht-en-feestelijke-middag/", "updated_at": "2023-07-26T09:12:08", "body": "Op vliegveld Teuge werd vrijdag 7 juli het tweede Accelerate-traject afgesloten met een feestelijk graduation event. Dit opleidingstraject, waarvoor Thales, de Belastingdienst, Bosch en Luminis de handen ineenslaan, is een op maat gesneden traject voor toptalenten van de vier organisaties. De 23 deelnemers rondden het traject van achttien maanden succesvol af en kregen hun Accelerate wings opgespeld.\u00a0\nHet afronden van het programma werd op een bijzondere manier gevierd, de deelnemers stapten namelijk in een Cessna voor een korte vlucht. Vervolgens deelden ze persoonlijke speeches op het podium en namen ze complimenten en hun felbegeerde wings in ontvangst van de stuurgroep en coaches.\n\nBert Ertman, VP Technology bij Luminis en initiatiefnemer van het Accelerate concept, riep de geslaagden op uit hun comfortzone te blijven stappen: \u201cDe sky was de afgelopen 18 maanden de limit, die hebben jullie nu bereikt maar kijk eens hoe veel verder je nog kunt gaan. Blijf in beweging en blijf anderen inspireren.\u201d\nHet meest indrukwekkend zijn de verhalen van deelnemers over de impact van Accelerate op zowel hun carri\u00e8re als priv\u00e9leven. Met name de persoonlijk leiderschap-trainingen van How Company hebben veel teweeggebracht. Meerdere deelnemers spraken over een onomkeerbare groei in hun zelfvertrouwen en eigenwaarde, wat ze niet alleen een betere collega maakt, maar ook een betere ouder, partner of vriend. Of zoals een deelnemer zelf zei: \u201cDankzij het Accelerate-netwerk leer je obstakels overwinnen. De kracht van gemeenschap en de kracht van kwetsbaarheid daarin was duidelijk. Je leert niet alleen van elkaar, maar juist ook met elkaar.\u201d\nTijdens het traject werden de deelnemers begeleid door coaches die de deelnemers waar nodig een duwtje in de rug gaven. Robert en Erik, coaches van Accelerate, lieten zien dat het coach-zijn hen ook veel heeft gebracht: \u201cCoaching is als een spiegel voor je zelfbewustzijn. Wat je uitdraagt, moet je ook zelf toepassen. Dit heeft ervoor gezorgd dat wij nu zelf ook beter weten waar onze krachten liggen en wat onze uitdagingen zijn.\u201d\nAl met al blikken we terug op een succesvolle afsluiting van een bijzonder traject. Achttien maanden lang werkten deelnemers aan hun skill set, overwonnen ze samen talloze uitdagingen en behaalden ze persoonlijke doelstellingen. We kijken uit naar de impact die deze groep Accelerate-deelnemers gaan hebben op onze organisaties, nu in en de toekomst.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38415, "title": "Searching through images using the CLIP model", "url": "https://www.luminis.eu/blog/search-en/searching-through-images-using-the-clip-model/", "updated_at": "2023-07-12T08:53:32", "body": "Screenshot Streamlit app for image search\nA picture says more than a thousand words. But can you use words to find images? I do not mean going through the meta-data of images but searching for the actual image. That is what we are going to see in this blog post. An app that is searching through images using the CLIP model\nI took a few small images from different websites, stored them in a vector store, and sent textual queries to find matches. The screenshot shows the Streamlit app that interacts with the vector store. This blog post shows how many lines of code you need to create your image search.\nHow it works \u2013 the CLIP model\nCLIP \u2013 Contrastive Language\u2013Image Pre-training is a model that originates from OpenAI. The main idea is to add Natural Language Supervision to a model learning from images. The idea is to create one vector space containing text and images. The idea of a vector space is that similar concepts are close to each other in that vector space. The smaller the distance between two concepts in a vector space, the more likely they talk about the same concept. If we have a combined vector store for text and images, we can compare the concept from text with the concept from an image. Creating such a model is also called a multi-modal model. A good starting point to learn more about CLIP is the webpage from OpenAI.\nCLIP is the base for a lot of other models. Huggingface has some excellent models, and a lot of them are free to use. In the next section, you can see how easy it is to use these models with Weaviate, the vector store we use that comes with many extras. The model that I have chosen is sentence-transformers/clip-ViT-B-32-multilingual-v1. I choose this model for its multilingual capabilities. To be honest, the support for the Dutch language is not great. I was surprised by the quality of English requests. The video at the end of this blog gives an idea of the quality.\nRunning the sample\nYou can find the code for this sample in my GitHub repository. You want to look at the file run_weaviate_clip.py. You can check the readme.md for instructions on how to run the sample. You can also configure PyCharm to run Streamlit. The following image shows you how to do that.\npycharm run configuration for a streamlt app\nSetting up Weaviate\nYou can use docker-compose to set up a local running Weaviate instance. The project contains a docker-compose file named docker-compose-weaviate-clip.yml. The file is in the infra folder. You can start docker using the following command. Beware, it does download the complete CLIP model of around 5 Gb. So you need space, internet, and patience. Notice the two services that we deploy, weaviate and the multi2vec-clip module with the specified model.\n---\r\nversion: '3.4'\r\nservices:\r\n  weaviate:\r\n  command:\r\n    - --host\r\n    - 0.0.0.0\r\n    - --port\r\n    - '8080'\r\n    - --scheme\r\n    - HTTP\r\n    image: semitechnologies/weaviate:1.19.11\r\n    ports:\r\n    - 8080:8080\r\n    restart: on-failure:0\r\n    environment:\r\n      CLIP_INFERENCE_API: 'http://multi2vec-clip:8080'\r\n      QUERY_DEFAULTS_LIMIT: 25\r\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\r\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\r\n      DEFAULT_VECTORIZER_MODULE: 'multi2vec-clip'\r\n      ENABLE_MODULES: 'multi2vec-clip'\r\n      CLUSTER_HOSTNAME: 'node1'\r\n    volumes:\r\n    - ./data_clip:/var/lib/weaviate\r\n  multi2vec-clip:\r\n    image: semitechnologies/multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1\r\n    environment:\r\n      ENABLE_CUDA: '0'\r\n...\nConfiguring the Schema\nWeaviate can work without a schema, but providing the schema makes it explicit what you want. Within the schema, we also configure the multi2vec-clip module. Below you can find the schema and the code to load the schema.\n{\r\n  \"class\": \"Toys\",\r\n  \"description\": \"A sample to search for toys\",\r\n  \"moduleConfig\": {\r\n    \"multi2vec-clip\": {\r\n      \"imageFields\": [\r\n        \"image\"\r\n      ]\r\n    }\r\n  },\r\n  \"vectorIndexType\": \"hnsw\",\r\n  \"vectorizer\": \"multi2vec-clip\",\r\n  \"properties\": [\r\n    {\r\n      \"dataType\": [\r\n        \"string\"\r\n      ],\r\n      \"name\": \"filename\"\r\n    },\r\n    {\r\n      \"dataType\": [\r\n        \"blob\"\r\n      ],\r\n      \"name\": \"image\"\r\n    }\r\n  ]\r\n}\r\n\ndef load_weaviate_schema(client: WeaviateClient, schema_path: str) -> None:\r\n    if client.does_class_exist(WEAVIATE_CLASS):\r\n        client.delete_class(WEAVIATE_CLASS)\r\n        run_logging.info(\"Removed the existing Weaviate schema.\")\r\n\r\n    client.create_classes(path_to_schema=schema_path)\r\n    run_logging.info(\"New schema loaded for class '%s'.\", WEAVIATE_CLASS)\r\n\nNotice that we throw away the schema if it already exists\nStoring the images\nNext, we store the images that are provided in the data_sources folder. First, we encode the image as a base64 byte array. Next, we decode the bite array into a utf-8 string. The string is added to weaviate in the field image of type datablob.\ndef store_images(client: WeaviateClient) -> None:\r\n    with client.client.batch(batch_size=5) as batch:\r\n        for file_name in os.listdir(IMG_PATH):\r\n            if file_name.endswith(\".jpg\"):\r\n                with open(IMG_PATH + file_name, \"rb\") as img_file:\r\n                    b64_string = base64.b64encode(img_file.read())\r\n\r\n                data_obj = {\"filename\": file_name, \"image\": b64_string.decode('utf-8')}\r\n                batch.add_data_object(data_obj, WEAVIATE_CLASS)\r\n                run_logging.info(\"Stored file: %s\", file_name)\r\n\nQuerying the images\nNow we can import all the images. The Streamlit app has a button to do exactly that. With all the images in the store, we can finally execute queries. Weaviate makes it easy to execute a near_text query. The next code block executes the query and extracts the required data from the response.\ndef query(client: WeaviateClient, query_text: str, the_limit: int = 3):\r\n    run_logging.info(\"Executing the query '%s'\", query_text)\r\n    near_text = {\"concepts\": [query_text]}\r\n\r\n    response = (client.client.query\r\n                   .get(WEAVIATE_CLASS, [\"filename\"])\r\n                   .with_near_text(near_text)\r\n                   .with_limit(the_limit)\r\n                   .with_additional(properties=[\"certainty\", \"distance\"])\r\n                   .do())\r\n    if response[\"data\"][\"Get\"][\"Toys\"]:\r\n        found_picts = response[\"data\"][\"Get\"][\"Toys\"]\r\n        return [{\"filename\": pict[\"filename\"], \"certainty\": pict[\"_additional\"][\"certainty\"]} for pict in found_picts]\r\n\r\n    return []\r\n\r\n\nDemo time\nYou can look at the video below for the demo of searching through images using the CLIP model. Pay attention to the search terms and the results. In the beginning, I highlight the value of certainty. A higher value means the model is more certain it found a good match. I like that you can search for wood and colors. Even those terms return reasonable results.\n\nhttps://www.luminis.eu/wp-content/uploads/2023/07/streamlit-run_weaviate_clip-2023-07-11-12-07-54.webm\nResults that do not look like a match\nYou might have thought there were many bad results if you watched the demo. One thing that is different in a vector search from the well-known index-based search. With index-based search, whether there is a match depends on the terms you are searching for and the terms in the index. With vector search, we look at the closest match. Therefore, you get the six closest results when asking for six results. That does not mean they are always a good match, but they are the closest match we could find.\nYou can use certainty to limit results to a specific score, but it is hard to predict which score works for you. That depends on the model, your data, and your query.\nConcluding\nI hope you like my blog post and you learned something. I also hope you are now convinced that image search has changed dramatically. If you need help, please get in touch with me. I am sure we can figure something out.\n", "tags": [], "categories": ["Blog", "Search"]}
{"post_id": 38404, "title": "Tips on assuring your happiness job wise", "url": "https://www.luminis.eu/blog/tips-on-assuring-your-happiness-job-wise/", "updated_at": "2023-07-12T15:57:07", "body": "Have you ever wondered, at any point in your career, if you have made the right decision job wise? It could be that you are not satisfied anymore with your current employer, the customer you are working for, or that you want to trade your job as a lecturer to become a software developer.\nThe most important thing to learn from turning your career upside down is the feeling you will wake up with every day that you have made the right decision. It is your happiness that counts above anything else!\nInspiration\nThe reason I decided to write this blog post is that there has not been much written about this topic. This year, I read a book called \u2018Met je voeten in het stopcontact,\u2019 which translates to \u2018Putting your feet in a power outlet,\u2019 by Gwen van Poorten.\nIn this book, Gwen talks about how she became her own best friend, and created a life she feels excited about. She shows a vulnerable side of herself, talking about having a burnout aged 22 and how she managed to turn her life around and become the best version of herself. Also, the current media occasionally drops a post about this topic, but it is still something that does not get covered enough. Even though I never let it come that far, this book and online posts relate to my blog post. I hope the tips I describe in my post will help you reflect and ensure happiness in your career.\nI have a BSc in Computer Science, but I do not see myself working as a back-end developer, what should I do?\nIt is important to create a work environment that you are pleased with. For example, you get excited from working at a marketing firm as a social media director. In this case, you have created an environment that you want to be in. I did not have that energized feeling even before I started my first job. But what did I want to do after I got my bachelor\u2019s degree in computer science? I had no idea what the possibilities were, only that I had to work as a back-end developer.\nThat first job excitement\nLuckily, a miracle happened. A former colleague asked me to stay at the university I studied at. That removed my first barrier of not having to work as a back-end developer. I did not have to work behind a desk all day and write back-end code. I could become a teacher!\n\nWhen I worked as a lecturer, I really liked the fact that there was no certain schedule or task you had to do for eight hours straight. It was a truly diverse environment where you had to prepare classes, help in labs, or give a lecture to students on a study related topic. I thought it was only possible to have this diversity in tasks for jobs in education or in a similar sector. But it was not until I changed jobs that I found out you can also have that type of diversity in a job as front-end developer.\nHow can I afford life if I radically change my profession?\nAfter three years working at the university, I felt like it was time for a change. I did not feel that same spark I had in the beginning. I still liked teaching, but the atmosphere at the university had changed negatively, which meant I did not want to stay there. On top of that, since I started a masters degree in Interaction Design, I became more interested in front-end development in combination with user interface design.\nTime for a change\nIn early 2020, I decided to apply for jobs in front-end development and got hired by Luminis as a junior front-end developer! I agreed to work a 32h week, so I had one day off for my masters degree. It all happened during the COVID-19 lockdowns, when the housing prices were sky high. So, how could it be possible to start a new job, continue my masters degree, and move to a new city, which was not even close to where I used to live? And how wouldn\u2019t afford everything?\nAfter I found out I got the job at Luminis, I had to find a place to live. It was difficult to find something, but I was never worried I wouldn\u2019t afford it. Also, I did not easily pick an apartment just so I had a place to live in. I have my needs, and the apartment needed to fit those needs. Eventually, I found the apartment that checked off all my boxes. Even though renting is expensive, I knew I made the right decision, because I would be excited to live there and enjoy my job as well.\nIs it possible to study, while also having a part-time job (32h)?\nShort answer: YES! But I guess you would like a more thorough explanation. I will start at the beginning, because I had to get a master\u2019s degree for my previous lecturer job. The university gave me two paid days to study. Halfway through my studies, I decided to\nchange jobs. How would that work? Because I would not get two paid days off for that job. I decided to work part-time, a 32h workweek. It meant I had one less day to work on my studies.\nMy solution:\nMake a PLAN! For the study, I knew what courses I had to take during a semester, which means I could easily plan out my semester. In my case, Google Classroom showed which deadlines I had on which days. I wrote down everything I had to do to meet that specific deadline. Each small task I divided over every Friday per deadline. I knew exactly what I had to do each Friday to stay on route towards meeting my deadlines.\nWorking at Luminis\nLuminis has always supported me from the beginning. In my application process, I let them know that I was studying for a master\u2019s and wanted to work a 32h workweek. Because there was clear communication, it was never a problem.\nIt takes effort to create a life that you are satisfied with, but if you do not take that leap of faith, you will not know if that one job opportunity will create the life you want to live! It worked out for me. I recently graduated with a master\u2019s degree in Interaction Design, and I have a job that I love that I can change to my satisfaction. You do not need to listen to what society wants you to do. If you did that, you would live the life society wants you to live instead of the life you want to create!\nTo end this blog post, I would like to shake my top three tips that can help you on your path to happiness job\u00a0wise:\n#1: Be excited about going to your workplace.\n#2: Money \u2260 Happiness. Take a leap of faith to become the person you want to become and have the life you want to live. Do not let money hold you back.\n#3: You are never too old to study. If you want to go back to school and change or upgrade your career, do it. It is never too late. Communicate, plan, and make it work the way you want it to work!\n", "tags": [], "categories": ["Blog", "Working at Luminis"]}
{"post_id": 38388, "title": "Develop and Test your Github Actions workflow locally with \u201cact\u201d", "url": "https://www.luminis.eu/blog/develop-and-test-your-github-actions-workflow-locally-with-act/", "updated_at": "2023-07-05T12:21:30", "body": "At work, I regularly train people on the subject of Continuous Integration and Continuous Delivery, where I predominantly utilize GitHub Actions for the workshop assignments. This choice is motivated by GitHub\u2019s extensive adoption within the developer community and the generous offering of approximately 2000 minutes or 33 hours of free build time per month.\nDuring one of my recent workshops, a participant raised a question regarding the possibility of locally testing workflows before pushing them to GitHub. They pointed out the inconvenience of waiting for a runner to pick up their pipeline or workflow, which negatively impacts the developer experience. At that time, I was unaware of any local options for GitHub Actions. However, I have since come across a solution called \u201cact\u201d that addresses this issue.\nWhat is \u201cact\u201d?\n\u201cact\u201d is a command-line utility that emulates a Github Actions environment and allows you to test your Github Actions workflows on your developer laptop instead of in a Github Actions environment. You can install \u201cact\u201d by using for instance brew on the Mac.\n$ brew install act\nRunning Workflows Locally\n\u201cact\u201d enables you to execute and debug GitHub Actions workflows locally, providing a faster feedback loop during development. Running the \u201cact\u201d command line will pick up the workflows in your .github/workflows folder and try to execute them. Using \u201cact\u201d can be as simple as:\n$ act\n\u201cact\u201d uses Docker to create an isolated environment that closely resembles the GitHub Actions execution environment. This ensures consistency in the execution of actions and workflows. If you don\u2019t have Docker installed you can use Docker Desktop or use Colima, an easy way to run container runtimes on macOS.\nRunners\nWhen defining the workflow you can specify a runner based on a specific virtual machine/environment when performing your steps.\njobs:\r\n  Build:\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n      ...\r\n\nBy default, \u201cact\u201d has a mapping to a specific docker image when you specify the ubuntu-latest runner. When running \u201cact\u201d for the first time it will ask you to pick a default image for ubuntu-latest. You can choose from 3 types of base images that can be mapped to ubuntu-latest:\n\nMicro Docker Image (node:16-buster-slim)\nMedium Docker Image (catthehacker/ubuntu:act-latest)\nLarge Docker Image (catthehacker/ubuntu:full-latest)\n\nDon\u2019t worry if you\u2019re not happy with the one you selected, you can always change the default selection by changing the following file in your users home directory ~/.actrc.\nThe large docker image is around 18GB!!, so I initially picked the medium-sized image as it should contain most of the commonly used system dependencies. I soon learned that it contains quite some libraries, but when I tried to run a Java + Maven-based project I learned that it did not contain Apache Maven, while the normal ubuntu-latest on GitHub does have that.\n[CI/Build] \u2b50 Run Main Build\r\n[CI/Build]   \ud83d\udc33  docker exec cmd=[bash --noprofile --norc -e -o pipefail /var/run/act/workflow/2] user= workdir=\r\n| /var/run/act/workflow/2: line 2: mvn: command not found\r\n[CI/Build]   \u274c  Failure - Main Build\r\n[CI/Build] exitcode '127': command not found, please refer to https://github.com/nektos/act/issues/107 for more information\r\n\nI didn\u2019t want to switch to an 18GB docker image to be able to just run Maven, so I ended up finding an existing image by Jamez Perkins. It simply takes the original \u201cact\u201d image ghcr.io/catthehacker/ubuntu:act-latest and adds Maven version 3.x to it. You can easily specify running your workflow with custom images by providing the platform parameter.\n$ act -P ubuntu-latest=quay.io/jamezp/act-maven\nAfter using that image my workflow ran without any errors.\nWorking with multiple jobs/stages\nYour GitHub actions workflow usually consists of one or more jobs that separate different stages of your workflow. You might for instance have a Build, Test and Deploy stage.\n\nUsually, you build your application in the build job and use the resulting artifact in the deploy job. Jobs can run on different runners, so in a GitHub Actions environment, you will probably be using the upload/download artifact action which will use centralized storage for sharing the artifacts between different runners. When using \u201cact\u201d and sharing artifacts you will need to be specific about where the artifacts need to be stored. You can do so by providing a specific parameter named --artifact-server-path.\n$ act -P ubuntu-latest=quay.io/jamezp/act-maven \\\r\n  --artifact-server-path /tmp/act-artifacts\r\n\nWorking with secrets\nIt\u2019s a best practice to always separate your secrets from your workflow definition and only reference them from a specific secret store. When using GitHub Actions you can store your secrets in the built-in secret management functionality.\nTo provide an action with a secret, you can use the secrets context to access secrets you\u2019ve created in your repository.\njobs:\r\n  staticanalysis:\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n    - uses: actions/checkout@v3\r\n      with:\r\n        # Disabling shallow clone is recommended for improving relevancy of reporting\r\n        fetch-depth: 0\r\n    - name: SonarQube Scan\r\n      uses: sonarsource/sonarqube-scan-action@master\r\n      env:\r\n        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}\r\n        SONAR_HOST_URL: ${{ secrets.SONAR_URL }}\r\n\n\u201cact\u201d does not have a UI in which you can specify secrets, so you will need to provide those values explicitly from the command line or store them in a .env formatted file when testing your workflow. If you only have a few secrets you can easily add them by just providing the secret from the command line by using the -s option.\n$ act -s SONAR_TOKEN=somevalue\r\n$ act --secret-file my.secrets\r\n\nWorking with environment variables\nSimilar to secrets you sometimes make use of environment variables inside your workflow. For a single environment variable you can use --env myenv=foo or if you have a set of environment variables you can create a dotenv file and provide a reference to the file from the CLI by providing the --env-file parameter.\n$ act --env-file my.env\nThe .env file is based on a simple standard file format which contains a set of key-value pairs divided by new lines.\nMY_ENV_VAR=MY_ENV_VAR_VALUE\r\nMY_2ND_ENV_VAR=\"my 2nd env var value\"\r\n\nEvent simulation\nEvents are a fundamental part of workflows. Workflows will start due to some specific event happening within Github like a push, creation of a pull request, etc. With \u201cact\u201d you can simulate such an event to trigger your workflow(s). You can provide the event as an argument.\n$ act pull_request\nEvents are usually more complex than just a simple string so if you want to be specific you can provide a reference to an event payload:\n$ act --eventpath pull_request.json\r\n{\r\n  \"pull_request\": {\r\n    \"head\": {\r\n      \"ref\": \"sample-head-ref\"\r\n    },\r\n    \"base\": {\r\n      \"ref\": \"sample-base-ref\"\r\n    }\r\n  }\r\n}\r\n\nBy providing your events from the command line you can test different scenarios and observe how your workflows respond to those events.\nSummary\nUsing \u201cact\u201d is straightforward and can significantly help in the initial phase of developing your workflow. \u201cact\u201d offers a significant advantage in terms of a swift feedback loop. It enables developers to perform tests locally and iterate rapidly until they achieve the desired outcome, eliminating the need to wait for GitHub\u2019s runners to finish the workflow.\n\u201cact\u201d additionally aids developers in avoiding resource wastage on GitHub\u2019s runners. By conducting local tests, developers can ensure the proper functioning of their workflows before pushing code changes to the repository and initiating a workflow on GitHub\u2019s runners.\nIf you\u2019re working with GitHub Actions I would recommend to asses \u201cact\u201d as a tool for your development team.\n", "tags": ["CI/CD", "git", "github", "testing"], "categories": ["Blog", "Development"]}
{"post_id": 38365, "title": "Yuma, a new digital transformation challenger", "url": "https://www.luminis.eu/blog/yuma-a-new-digital-transformation-challenger/", "updated_at": "2023-06-28T13:52:22", "body": "Yuma is a new digital transformation group in the BeNeLux. A group where digital and human transformation go hand-in-hand as it states there is no digital transformation without human transformation.\nAmsterdam, June 28th, 2023 \u2013 Yuma will combine a human, people-centric approach, with a hands-on mentality and best-in-class expertise. This best-in-class expertise is a result of the combination of XPLUS, Total Design, Luminis and BPSOLUTIONS, and more companies are expected to join the group.\n\nLot van Wegen, CMO Yuma:\n\u201cIn today\u2019s world companies are digitally reinventing themselves continuously. Many digital transformations fail because they don\u2019t pay enough attention to the human side of the transformation. We see the need for a more human approach. That\u2019s why we started Yuma: a combination of best-in-class experts with an empathetic, pragmatic, and people-centric way of working.\u201d\nAs of the 1st of July, Pascal Laffineur will be the CEO of Yuma. Coming from NRB, Altran (CapGemini), SFR, and Alcatel, Pascal has a strong tech and IT background. Building on Pascal\u2019s leadership skills in his previous CEO roles, he brings the necessary knowledge and expertise to position Yuma as the prominent player we aspire to be in Western Europe.\nPascal Laffineur, CEO Yuma:\n\u201cBeing at the start of Yuma is the best challenge I could dream about. I truly believe in a more human and holistic approach to make digital transformation successful. The group is all about teamwork, customer service, thought leadership, technology, and fun!\u201d\nYuma expects a revenue of about 100 million euros this year and works together with 400 employees located across BeNeLux.\nKeep an eye on the website: weareyuma.com\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38323, "title": "Introduction to Autonomous Agents in AI", "url": "https://www.luminis.eu/blog/machine-learning-ai-en/introduction-to-autonomous-agents-in-ai/", "updated_at": "2023-06-27T14:32:51", "body": "Over the past 12 months, AI tools have garnered significant attention and generated considerable hype within the tech industry. Initially, the focus was on the metaverse, which was seen as an extension of our existing reality, offering more immersive experiences. However, the emergence of powerful tools such as chatGPT and DALL-E 2 has marked the beginning of a new paradigm.\nWhile the initial excitement surrounding AI tools may have subsided, one area that continues to grow and gain traction is autonomous agents in AI. Although the term \u2018autonomous agents in AI\u2019 might not immediately ring a bell, you are likely familiar with many related concepts.\nWhat is an agent?\nTo begin with, the word agent has nothing to do with AI. An agent is defined as anything that can perceive its environment and act upon that. This concept has been used in different disciplines, like cognitive science, ethics, or social simulations, but it comes from economics, where a \u201crational agent\u201d is defined as anything with a goal. The Matrix (1999) already used this concept, playing with this double meaning as a govern agent and software agent, which can respond to inputs and operate on given instructions.\nThe Stanford Encyclopaedia of Philosophy defines artificial intelligence as \u201cthe study of agents that receive precepts from the environment and perform actions\u201d. Each such agent implements a function that maps percept sequences to actions, and we cover different ways to represent these functions\u2026\u201d (Russell & Norvig 2009, vii).\nSo if AI is the study of agents, this is an important topic.\nNow you have the idea, where is the hype?\nResearchers from Stanford and Google created an interactive environment with 25 AI agents that simulate human behavior. You can see the full paper here. It is not just a bunch of pixel dolls walking around and having bot conversations; they remember those conversations, join for coffee at a cafe, a walk in the park, etc. These bots were able to respond to the environment by splitting complex tasks and giving priority over time. They could even prepare a Valentine\u2019s Day party by themselves:\n\u201cFor example, starting with only a single user-specified notion that one agent wants to throw a Valentine\u2019s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.\u201d\n\nIt looks like some folks just put some LLMs to talk to each other, but that is just the surface. There are some cool and promising projects trying to implement the best Autonomous Agent in AI possible, like Camel, AutoGPT (from OpenAI), or BabyAGI, and each one has a different approach. Let\u2019s take for instance BabyAGI, which autonomously creates and performs tasks based on an objective, following their Github:\nThe script works by running an infinite loop that does the following steps:\n\nPulls the first task from the task list.\nSends the task to the execution agent, which uses OpenAI\u2019s API to complete the task based on the context.\nEnriches the result and stores it in Chroma/Weaviate.\nCreates new tasks and reprioritizes the task list based on the objective and the result of the previous task.\n\n\nAnother example is this new tool developed by HyperWrite that can connect to the internet and take control of your computer and order a pizza from Domino\u2019s for you:\n\n\nAutoGPT agents like Hyperwrite AI can do everything for you from generating creative poetry to ordering pizza \ud83e\udd2f  \nDiscover the AI way to interact with the internet!! pic.twitter.com/n9XrS9JZvA\n\u2014 Shubham Saboo (@Saboo_Shubham_) April 14, 2023\n\nWhat is next?\nSo, as you can see, autonomous agents in artificial intelligence are powerful tools able to perform tasks and make decisions independently, without human intervention. The uses of these agents have potential to revolutionize various industries and domains, like healthcare, transportation, finance and customer service. However, with their increasing integration into society, it becomes crucial to examine the implications of this technology. Several topics need to be discussed.\nEthical Concerns:\nAs autonomous agents become more sophisticated and capable, ethical considerations become paramount. Questions arise regarding the decision-making process of these agents and the accountability for their actions. Who should be held responsible if an autonomous agent makes a mistake or causes harm? Establishing ethical guidelines and legal frameworks is crucial to ensure that agents operate in a responsible and accountable manner.\nJob Displacement:\nSince this technology is designed to perform tasks traditionally done by humans, it raises questions about the future of employment. While some argue that AI will create new job opportunities, others fear that certain industries and professions may become obsolete. Preparing for this potential shift in the job market and finding ways to re-skill and up-skill the workforce are critical considerations.\nPrivacy and Data Security:\nAutonomous agents rely on vast amounts of data to make informed decisions and provide personalized experiences. However, this raises concerns about privacy and data security. As these agents interact with users and collect sensitive information, ensuring the protection of personal data becomes paramount. Robust security measures, data anonymization techniques, and transparent data usage policies must be implemented to address these concerns.\nBias and Fairness:\nAutonomous agents are trained on large datasets, which can inadvertently introduce biases present in the data. If these biases are not identified and mitigated, autonomous agents can perpetuate discriminatory practices or amplify existing societal inequalities. Developing algorithms that are fair, transparent, and free from bias is crucial to ensure equitable outcomes in the deployment of autonomous agents.\nHuman-Autonomous Agent Collaboration:\nWhile the autonomy of these agents is a defining feature, ensuring effective collaboration between humans and autonomous agents is essential. Building systems that allow cooperation between humans and agents can lead to better productivity, decision-making, and problem-solving. Designing user-friendly interfaces and fostering trust and transparency in human-agent interactions are important factors to consider.\nTake-away on Autonomous Agents in AI\nAre Autonomous Agents the next step in AI? Well\u2026 it\u2019s kinda soon to know. At this moment, all the projects use the ChatGPT API, since it\u2019s the LLM that gives the best performance, so building several agents talking to each other is relatively expensive. Also, since the agent needs a response from another agent to start processing the next request, it makes the process slow, and other problems like hallucinations can interfere in the process.\nRemember, we are still on the first steps on AI, but the adoption curve has been almost non-existent for a product like ChatGPT, so the projects above mentioned are just the first early steps to something bigger than what we currently have.\nWant to know more? Make sure to read some of our other blogs.\n", "tags": [], "categories": ["Blog", "Machine learning &amp; AI"]}
{"post_id": 38362, "title": "1-daagse workshop klantgericht werken, hoe organiseer je dat?", "url": "https://www.luminis.eu/blog/1-daagse-workshop-klantgericht-werken-hoe-organiseer-je-dat/", "updated_at": "2023-07-02T20:54:55", "body": "In de eerste blog van deze blogserie wordt o.a. de 1-daagse workshop genoemd als methode om klantgericht werken in je organisatie in te bedden. Leuk natuurlijk, maar hoe doe je dat nou, zo\u2019n workshop organiseren en faciliteren?!\nIn deze blog deel ik de belangrijkste ervaringen die ik de afgelopen jaren op heb gedaan bij het voorbereiden en uitvoeren van zo\u2019n 1-daagse workshop. Valkuilen en tips en trucs komen zeker aan bod.\nEen 1-daagse workshop klantgericht werken, waarom?\nMet een 1-daagse workshop klantgericht werken wil je een ontwerpuitdaging concretiseren, een oplossingsrichting defini\u00ebren en focus (aan een project) geven zodat je daarna weer snel aan de slag kunnen.\u00a0Bij een 1-daagse worden allerlei stakeholder betrokken, van opdrachtgever tot eindgebruikers. Een 1-daagse heeft een innovatief karakter en gebruikt Design Thinking methodes om een innovatieve mindset te stimuleren.\nNaast de link met klantgericht werken zijn er dus meer redenen om een 1-daagse workshop te houden.\nEen 1-daagse:\n\nkan een groep stakeholders op 1 lijn krijgen;\nhelpt bij het vlottrekken van een vastgelopen proces;\ngeeft hernieuwde energie.\n\nKenmerken van een 1-daagse\nEen 1-daagse is gebaseerd op Design Thinking. In 1 dag doorloop je een design cyclus, via Empathy, Define, Ideate en Prototype naar Test. Voor elke fase zijn er dan weer allerlei werkvormen die je kunt gebruiken om inzichten op te halen. Daarover later meer\u2026\nEen design cyclus past in mijn ogen binnen timebox die je zo groot en zo klein kunt maken als je wilt. En dus ook binnen 1 dag! Daarbij wel de aantekening dat test de presentatie aan verschillende stakeholders is aan wie je de uitkomst van de workshop graag wilt presenteren.\nBij een 1-daagse:\n\nwerken deelnemers met diverse achtergronden samen waardoor wederzijds begrip en enthousiasme ontstaat;\nzijn deelnemers \u2018slechts\u2019 1 dag kwijt (voorbereiden, begeleiden en uitwerken kosten meerdere dagen maar dat is vooral het werk van een facilitator);\nwordt het topje van de ijsberg uitgewerkt. Het is \u2018slechts\u2019 1 dag waarna vervolgstappen bijna altijd nodig zijn.\n\nWaarvoor kun je een 1-daagse workshop zoal inzetten?\nEnkele voorbeelden uit de praktijk:\n\nstrategiesessies\ncontentarchitectuur\nnavigatie tussen je portalen\nchatbot\napps\nhet uitwerken van nieuwe Design System componenten\n\nStappenplan c.q. planning\nOm een 1-daagse succesvol te laten verlopen heb je een facilitator en enthousiaste deelnemers nodig. De facilitator kan helpen met het voorbereiden van het materiaal dat nodig is. Daarbij hanteer ik grofweg de volgende planning:\n\nminimaal 2 weken voor de 1-daagse: intake;\nuiterlijk twee weken voor de 1-daagse: uitnodiging om deel te nemen versturen;\n1 week voor de 1-daagse: agenda, inleiding en verwachting verspreiden;\n1 week voor de 1-daagse: start met het verzamelen van informatie en evt. aan de genodigden vragen om \u2018huiswerk\u2019 te maken;\nde 1-daagse workshop zelf;\n1 week na 1-daagse: conclusies verspreiden en vervolgacties uitzetten.\n\n\nDe intake\nEen intake met je opdrachtgever kun je in max. 2 uur doen en tijdens de intake komen de volgende onderwerpen aan bod:\n\nuitdaging\nverwachting, wat willen we bereiken op 1 dag?\ncijfers\nfocus bepalen\ngenodigden\n\nEen uitdaging wordt als HKW geformuleerd. HKW staat voor Hoe Kunnen We er voor zorgen dat\u2026\nBij het bepalen van een HKW gaat het om een uitdaging en niet om een oplossing. Een HKW wil je \u2018open\u2019 houden, de deelnemer hebben nog de hele dag om oplossingen te bedenken! En natuurlijk werken we graag met cijfers, als je een ontwerpuitdaging kunt onderbouwen helpt dat bij het maken van keuzes en stellen van prioriteiten bij je oplossing(en).\nAgenda en werkvormen\nTijdens de workshop wordt de Design Thinking cyclus gevolgd. Samen met de werkvormen die je wilt gebruiken bepalen ze de agenda van de dag. Elke ontwerpuitdaging ziet er echter wel anders uit waarmee ik vind dat je de agenda op maat samen zult moeten stellen. Een 1-daagse rond eenzelfde ontwerpuitdaging, bijvoorbeeld een herhalende workshop met designers in de IP sprint (zie SAFe) kun je prima \u2018van de plank\u2019 halen.\nDe indeling van de dag ziet er voor veel* 1-daagses ongeveer zo:\n\n09:00\u00a0 Deelnemers druppelen binnen\n09:30\u00a0 Start, koffie en introductie deelnemers\n09:45\u00a0 Uitdaging en wat weten we zoal?\n10:15\u00a0 \u00a0Koffie, niet onbelangrijk\n10:30\u00a0 Heatmappen (meeschrijven op stickies en dot-voten)\n10:45\u00a0 Scope bepalen\n11:15\u00a0 \u00a0Lightning demo\u2019s voorbereiden\n11:45\u00a0 \u00a0Pauze\n12:30\u00a0 Demo\u2019s\n13:00\u00a0 User test flows\n13:30\u00a0 Conceptualisatie\n15:00\u00a0 Uitwerken en presentabel maken concepten\n16: 00 Demo en actiepunten\n\nFase: Uitdaging, Define, Ideate, Prototype en test.\n*voor strategiesessies zal het tweede gedeelte van de agenda niet op demo\u2019s maar meer op uitwerking, beslissen en afspraken maken gericht zijn.\nEen dag begint altijd met een gesprek rond de uitdaging, wat er bekend is en het stellen van vragen om je in te leven. Op basis van je ontwerpuitdaging ga je verder met het programma waarbij je de volgende werkvormen toe kunt passen:\n\nontwerpvraag\nsituatieschets omschrijven/huidige situatie + knelpunten\ninzicht in data\ndesk research\nlightning Demo\u2019s (inspiratie)\npersonas\nempathymap\nuserflow + dot-voting\nflow uitwerken in scenario based sketchboard / solution sketch (notes, ideas, cray 8, scenario sketch)\n1 of meerdere schermen of idee\u00ebn uitwerken\nvervolgstappen benoemen\ndemo/oplevering\n\nVeel van deze werkvormen kun je online vinden. Kijk bijvoorbeeld eens op:\n\nWhiteboard templates for brainstorming, team management, strategy and mapping.\nDesign thinking template\nService design toolkit\nService design tools\n\nWie betrekken?\nOmdat ik geloof in Design Thinking en het collectief, nodig ik graag een multidisciplinaire groep uit. Tijdens een intake (zie stappenplan) komt de samenstelling van de groep aan bod. Standaard worden mensen vanuit business (haalbaar), eindgebruiker (wenselijk) en techniek (mogelijk) uitgenodigd. Dit betekent zoiets als opdrachtgever, eindgebruiker, engineer en helpdesk medewerkers, die zijn er dus sowieso bij.\n\nMijn checklist voor de deelnemers:\n\nEindgebruiker(s)\n(Realisatie)team\nOpdrachtgever\nKeten- of stuurgroepleden/management\nTechniek\nChat/social/\u2026\nHelpdesk\n\nVergeet ook niet een facilitator een uitnodiging te sturen, en te bedenken wie uit te nodigen voor de demo aan het einde van de dag.\nDe overige deelnemers worden aan de hand van de uitdaging bepaald.\u00a0 De invulling van Team kan realisatieteam zijn maar ook managementteam. Zoals gezegd, dit verschilt per uitdaging, het gaat om de kennis van de uitdaging die in wordt gebracht.\nValkuilen en tips\nDe workshop zelf heb ik nooit \u2018mis\u2019 zien gaan. Hoe meer materiaal je verzameld hebt hoe makkelijk de workshop verloopt. Dan nog, tijdens zo\u2019n dag haal je veel informatie op en ga er maar vanuit dat je echt niet alles in het voren weet. Berust in de known unknowns. Het voorbereiden opvolgen van een 1-daagse kost wel tijd en vooral daar moet je rekening mee houden.\nPunten van aandacht\n\nVoorbereidingstijd \u2192 neem je tijd om onderzoek te doen en materiaal te verzamelen\nOpvolging \u2192 als de 1-daagse voorbij is begint het pas, je hebt een richting en het is waarschijnlijk dat je die uit wilt gaan werken. Reserveer tijd voor het schrijven van een rapport en communicatie ervan.\n\nTips\n\nZoals je in de globale planning kunt zien vindt er een intake plaats. Bespreek tijdens de intake de volgende vragen:\n\nWelke uitdaging(en) speelt er?\nWat willen we bereiken op 1 dag?\nWelke cijfers of feiten zijn beschikbaar? \u2192 als je het niet weet, doe je huiswerk!\nWaar zou de focus op moeten liggen?\nWie moet er worden uitgenodigd?\n\n\nZorg voor een demo aan het einde van de dag waarin je de resultaten aan management en overige stakeholders presenteert. Dit brengt de energie van de dag over en geeft meer kans dat er ruimte komt voor vervolgstappen.\nOver de rol van een facilitator\u2026 Hoewel het vaak makkelijk is om als facilitator inhoudelijk betrokken te raken denk ik dat een facilitator mensen activeert door ze de uitdaging zelf te laten ervaren en een oplossing te laten bedenken. Een facilitator is niet verantwoordelijk voor de kwaliteit van het product. Hij geeft anderen de verantwoordelijkheid en begeleidt het proces en de ontwikkeling van de organisatie.\nHet helpt om een 1-daagse op een locatie te organiseren waar iedereen het gevoel heeft \u2018er uit\u2019 te zijn en aan een onderwerp te werken wat aandacht krijgt.\n\nEn nu?\nIk hoop dat je enthousiast bent over deze werkwijze en er mee aan de slag kan gaan. Gewoon doen en ervaring opdoen om te leren zou ik zeggen!\nHulp nodig?\nWe kunnen vanuit Luminis natuurlijk ook volgende dingen voor je regelen:\n\nMeewerken om een programma samen te stellen wat past bij jouw vraagstuk;\nFaciliteren van de 1-daagse workshop;\nVerzorgen van workshopmethodes en materialen die nodig zijn voor de 1-daagse.\n\nNeem contact met ons op voor meer informatie.\n\n\n\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 38298, "title": "Data Quality Series, part 2: Data Quality Testing with Deequ in Spark", "url": "https://www.luminis.eu/blog/data-quality-testing-with-deequ-in-spark/", "updated_at": "2023-08-13T12:12:10", "body": "In this blog, we explore how to ensure data quality in a Spark Scala ETL (Extract, Transform, Load) job. To achieve this, we leverage Deequ, an open-source library, to define and enforce various data quality checks.\nIf you need a refresher on data quality and its importance in data processing pipelines, you can refer to my previous blog post in this series, \u201cIntroduction to Data Quality\u201d. To recap, data quality is essential for accurate data analysis, decision-making, and achieving business objectives. It involves maintaining clean and standardized data that meets expectations. Ensuring data quality requires measuring and testing the data at different stages of the data pipeline. This may include unit testing, functional testing, and integration testing. A few testable properties of data are schema, freshness, quality, and volume, which we focus on in this blog.\nTo illustrate these concepts, we use a mock dataset based on the Iowa Liquor Sales Dataset as a running example. The dataset and the complete code for this blog can be found in the following GitHub repository.\nThe rest of the blog is structured as follows:\n\nTechnologies\nSetup\nThe Dataset\nBuilding Schema Checks\nProfiling Your Data\nAdding Data Quality Checks\nCollecting Data Quality Metrics\nGracefully Handling Data Changes\nAnomaly Detection\nConclusion\n\n1. Technologies\nEnsuring data quality in Spark can be achieved using various tools and libraries. One notable option is Deequ, an open-source library developed by AWS. It is a simple, but featureful tool that integrates well into AWS Glue or other Spark run times. By incorporating Deequ into our pipeline, we can perform schema checks, validate quality constraints, detect anomalies, collect quality metrics for monitoring, and use data profiling to gain insights into the properties of our data. Deequ effectively translates high-level rules and metrics into optimized Spark code, using the full potential of your Spark cluster.\nOther popular choices for data quality testing are tools like Great Expectations and Soda Core. These tools are rich in features, but also require additional configuration and setup, which may be explored in future blogs. For users already working within an AWS Glue ecosystem, exploring options that are tightly integrated with Glue, such as Deequ, can be more convenient and seamless.\nFor brevity, we focus on adding data quality to bare-bones Spark ETL scripts. While the implementation is similar, if you are using AWS Glue, we won\u2019t cover it in this blog. Instead, you can find an example glue script in the code repository.\n2. Setup\nTo begin, you need a working Scala development environment. If you don\u2019t, install Java, Scala, and sbt (Scala Build Tool). For Linux x86 the installation would look as follows:\n\n# Install Java (on Debian)\r\nsudo apt install default-jre\r\n# Install Coursier (Scala Version Manager)\r\ncurl -fL https://github.com/coursier/coursier/releases/latest/download/cs-x86_64-pc-linux.gz | gzip -d >> cs &&chmod +x cs &&./cs setup\r\n\r\n# Install Scala 2.12 and sbt\r\ncs install scala:2.12.15 &&cs install scalac:2.12.15\n\nNext, download a compatible Apache Spark distribution (version 3.3.x is recommended) and add the bin folder to your system path. If you can run spark-submit, you are all set.\n\n# Download Spark\r\ncurl https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz --output hadoop.tgz\r\ntar xvf hadoop.tgz\r\nmv spark-3.3.2-bin-hadoop3 /usr/local/spark\r\n\r\n# Add the following line to your .bashrc (adds Spark to PATH)\r\nexport PATH=\"$PATH:/usr/local/spark/bin\"\n\nSample Script\nIf you haven\u2019t already, clone the example project and open it in your editor of choice.\ngit clone git@github.com:EgorDm/deequ-spark-example.git\nYou find an empty example Spark script that reads a CSV file and writes it in parquet format to the output path. It takes the input path, output path and a path for metric storage as command line arguments.\n\ndef main(sysArgs: Array[String]): Unit = {  \r\n    // Parse job arguments  \r\n    val args = Map(  \r\n        \"input_file_path\" -> sysArgs(0),  \r\n        \"output_file_path\" -> sysArgs(1),  \r\n        \"metrics_path\" -> sysArgs(2)  \r\n    )  \r\n      \r\n    // Read the CSV input file  \r\n    val rawDf = spark.read  \r\n        .option(\"header\", \"true\")  \r\n        .csv(args(\"input_file_path\"))  \r\n      \r\n    logger.info(s\"Do some preprocessing\")  \r\n      \r\n    // Write the result to S3 in Parquet format  \r\n    rawDf.write  \r\n        .mode(\"overwrite\")  \r\n        .parquet(args(\"output_file_path\"))  \r\n}\n\nCompile the script with the following command, which outputs the jar as target/scala-2.12/glue-deequ_2.12-0.1.0.jar.\nsbt compile && sbt package\nRunning this Spark job is straightforward:\nspark-submit \\\r\n    --class EmptyExample \\\r\n    ./target/scala-2.12/glue-deequ_2.12-0.1.0.jar \\\r\n    \"./data/iowa_liquor_sales_lite/year=2022/iowa_liquor_sales_01.csv\" \\\r\n    \"./outputs/sales/iowa_liquor_sales_processed\" \\\r\n    \"./outputs/dataquality/iowa_liquor_sales_processed\"\nInclude Deequ Library\nSince we use the Deequ library, it must be added as a dependency to our project. While the library is already included in the project\u2019s dependencies, it is deliberately not bundled into the compiled jar. Instead, you can use the following command to extract it to the target/libs folder, or you can download it yourself from the maven repository.\nsbt copyRuntimeDependencies\nPass the --jars option to the Spark job, so the library is loaded at runtime:\n\nspark-submit \\\r\n    --jars ./target/libs/deequ-2.0.3-spark-3.3.jar \\  \r\n    --class ExampleSpark \\  \r\n    ./target/scala-2.12/glue-deequ_2.12-0.1.0.jar \\  \r\n    \"./data/iowa_liquor_sales_lite/year=2022/iowa_liquor_sales_01.csv\" \\  \r\n    \"./outputs/sales/iowa_liquor_sales_processed\" \\  \r\n    \"./outputs/dataquality/iowa_liquor_sales_processed\"  \n\nAfter running the command, the output parquet files are stored in outputs/sales/iowa_liquor_sales_processed and can be inspected with Spark, Pandas, or data tools like tad.\n3. The Dataset\nNow that our example ETL script works, let\u2019s look at the dataset. The mock dataset is based on the Iowa Liquor Sales dataset, which is simplified and modified to contain various data issues representative of the real world.\nThe dataset is partitioned by year, where each partition introduces schema and/or distribution changes.\ndata/iowa_liquor_sales_lite/\r\n    year=2020/iowa_liquor_sales_*.csv\r\n    year=2021/iowa_liquor_sales_*.csv\r\n    year=2022/iowa_liquor_sales_*.csv\nAssuming we already conducted exploratory data analysis, we start building our data quality checks by using the 2022 partition, and consider at the end how the other partitions impact our solution.\nPreview of Iowa Liquor Sales dataset.\n4. Building Schema Checks\nThe first step is validating the schema of our dataset. A schema defines the structure and organization of the data, including the names and types of columns. By performing schema checks, we can ensure that our data conforms to the expected structure and identify any inconsistencies or missing columns.\nTo define the schema, we use Deequ\u2019s RowLevelSchema class. Here, each column and its properties are defined using methods like withStringColumn, withIntColumn, withTimestampColumn, or withDecimalColumn. For our dataset, the schema is as follows:\n\nval schema = RowLevelSchema()  \r\n    .withStringColumn(\"Invoice/Item Number\", isNullable = false)  \r\n    .withStringColumn(\"Date\", isNullable = false)  \r\n    .withStringColumn(\"Store Name\", isNullable = false)  \r\n    .withStringColumn(\"Zip Code\", isNullable = false)  \r\n    .withStringColumn(\"Vendor Name\", isNullable = false)  \r\n    .withIntColumn(\"Item Number\", isNullable = false)  \r\n    .withIntColumn(\"Bottles Sold\", isNullable = false)  \r\n    .withDecimalColumn(\"Sale\", isNullable = false, precision = 12, scale = 2)  \r\n    .withDecimalColumn(\"Volume Sold (Liters)\", isNullable = true, precision = 12, scale = 2)\n\nAfter defining the schema, it can be validated against the data (rawDf) using the RowLevelSchemaValidator.validate method.\n\nval schemaResult = RowLevelSchemaValidator.validate(rawDf, schema)  \r\nif (schemaResult.numInvalidRows > 0) {  \r\n    logger.error(  \r\n    s\"Schema validation failed with ${schemaResult.numInvalidRows} invalid rows. Results: ${schemaResult}\")  \r\n    schemaResult.invalidRows.show(10, truncate = false)  \r\n    sys.exit(1)  \r\n}\r\n\r\nval validDf = schemaResult.validRows\n\nThe result (schemaResult) contains two Data Frames, specifically the valid rows that conform to the schema and invalid rows that do not. In some cases, data quarantining can be applied by preserving invalid rows and moving forward. Here, we break and display faulty data in the console instead.\n5. Profiling Your Data\nThe next step is data profiling, which is an essential step for understanding the characteristics and properties of your dataset. It provides insights into the structure, content, and statistical properties of the data, enabling you to identify potential issues or anomalies, and make informed decisions about data cleansing or transformation.\nDeequ provides a convenient way to profile your data using the ConstraintSuggestionRunner. Based on the analyzed data, it collects various statistics and suggests constraints using predefined rules.\n\nConstraintSuggestionRunner()  \r\n    .onData(validDf)  \r\n    .useSparkSession(spark)  \r\n    .overwritePreviousFiles(true)  \r\n    .saveConstraintSuggestionsJsonToPath(\r\n        s\"${args(\"metrics_path\")}/suggestions.json\")  \r\n    .saveColumnProfilesJsonToPath(\r\n        s\"${args(\"metrics_path\")}/profiles.json\")  \r\n    .addConstraintRules(Rules.DEFAULT)  \r\n    .run()\n\nIn the metrics folder, profiles.json is created as output. It contains extracted statistics in a semi-structured format, which can be useful for data quality checks creation, as well as data monitoring.\n\n\"columns\": [  \r\n    {  \r\n        \"column\": \"Vendor Name\",  \r\n        \"dataType\": \"String\",  \r\n        \"isDataTypeInferred\": \"true\",  \r\n        \"completeness\": 1.0,  \r\n        \"approximateNumDistinctValues\": 166  \r\n    },  \r\n    {  \r\n        \"column\": \"Item Number\",  \r\n        \"dataType\": \"Integral\",  \r\n        \"isDataTypeInferred\": \"false\",  \r\n        \"completeness\": 1.0,  \r\n        \"approximateNumDistinctValues\": 1469,  \r\n        \"mean\": 59981.83674981477,  \r\n        \"maximum\": 995530.0,  \r\n        \"minimum\": 567.0,  \r\n        \"sum\": 2.42866457E8,  \r\n        \"stdDev\": 104855.01628803412,  \r\n        \"approxPercentiles\": []  \r\n    },  \r\n    {  \r\n        \"column\": \"Volume Sold (Liters)\",  \r\n        \"dataType\": \"Fractional\",  \r\n        \"isDataTypeInferred\": \"false\",  \r\n        \"completeness\": 0.8992343788589775,  \r\n        \"approximateNumDistinctValues\": 97,  \r\n        \"mean\": 11.238700906344382,  \r\n        \"maximum\": 1512.0,  \r\n        \"minimum\": 0.05,  \r\n        \"sum\": 40920.1099999999,  \r\n        \"stdDev\": 40.87384345937876,  \r\n        \"approxPercentiles\": []  \r\n    },\r\n    ...\n\nThe suggestions.json includes a list with some basic data quality rule suggestions based on the profiled metrics. Some suggestions are more useful than others. I noticed that sometimes columns with medium cardinality are mistaken for categorical variables, suggesting value constraints. Having tight checks is valuable, but be wary of over-fitting your tests.\n\n\"constraint_suggestions\": [  \r\n    {  \r\n        ...\r\n        \"column_name\": \"Invoice/Item Number\",  \r\n        \"current_value\": \"Completeness: 1.0\",  \r\n        \"rule_description\": \"If a column is complete in the sample, we suggest a NOT NULL constraint\",  \r\n        \"code_for_constraint\": \".isComplete(\\\"Invoice/Item Number\\\")\"  \r\n    },\r\n    {  \r\n        ...\r\n        \"column_name\": \"Volume Sold (Liters)\",  \r\n        \"current_value\": \"Minimum: 0.05\",  \r\n        \"rule_description\": \"If we see only non-negative numbers in a column, we suggest a corresponding constraint\",  \r\n        \"code_for_constraint\": \".isNonNegative(\\\"Volume Sold (Liters)\\\")\"  \r\n    },\n\n6. Adding Data Quality Checks\nNow that we identified expectations for our data, we write the data quality checks to help us identify and address any issues or inconsistencies present in the dataset.\nThe checks are defined in groups with associated description and severity. Under the hood, the checks are translated to metric calculations and predicates that indicate success or failure based on the result of said metric.\nThe checks address different types of issues and may operate on both column and dataset level. See this file for an overview of all supported checks. If you can\u2019t find the right check, a custom check can be written in Spark SQL with the satisfies() method.\nHere is an example of relevant data quality checks to our business case.\n\nval checks = Seq(  \r\n    Check(CheckLevel.Error, \"Sales base checks\")  \r\n        .hasSize(_ >= 0, Some(\"Dataset should not be empty\"))  \r\n        .isComplete(\"Invoice/Item Number\")  \r\n        .isComplete(\"Date\")  \r\n        .isComplete(\"Store Name\")  \r\n        .isComplete(\"Zip Code\")  \r\n        .isComplete(\"Vendor Name\")  \r\n        .isComplete(\"Item Number\")  \r\n        .isComplete(\"Bottles Sold\")  \r\n        .isComplete(\"Sale\")  \r\n        .isUnique(\"Invoice/Item Number\")  \r\n        .hasPattern(\"Invoice/Item Number\", \"^INV-[0-9]{11}$\".r)  \r\n        .hasPattern(\"Date\", \"^[0-9]{4}-[0-9]{2}-[0-9]{2}$\".r)  \r\n        .hasPattern(\"Zip Code\", \"^[0-9]{5}$\".r)  \r\n        .isNonNegative(\"`Bottles Sold`\")  \r\n        .isNonNegative(\"`Sale`\")  \r\n        .isNonNegative(\"`Volume Sold (Liters)`\")\r\n)\n\nThe data quality checks can be executed using the VerificationSuite:\n\nvar verificationSuite = VerificationSuite()  \r\n    .onData(validDf)  \r\n    .useSparkSession(spark)  \r\n    .overwritePreviousFiles(true)  \r\n    .saveCheckResultsJsonToPath(s\"${args(\"metrics_path\")}/checks.json\")  \r\n    .addChecks(checks)\r\n\r\nval verificationResult = verificationSuite.run()\r\nif (verificationResult.status == CheckStatus.Error) {  \r\n    logger.error(s\"Data quality checks failed. Results: ${verificationResult.checkResults}\")  \r\n    sys.exit(1)  \r\n}\n\nRunning the checks as it is, results in a failure. The generated report (e.g., checks.json) generally provides enough information to determine which checks fail and why. By examining the report, we see the following error, implying that ~1.1% of our zip codes don\u2019t follow the five-digit format.\n\n...\r\n{  \r\n    \"check_status\": \"Error\",  \r\n    \"check_level\": \"Error\",  \r\n    \"constraint_status\": \"Failure\",  \r\n    \"check\": \"Validity checks\",  \r\n    \"constraint_message\": \"Value: 0.9898740429735737 does not meet the constraint requirement!\",  \r\n    \"constraint\": \"PatternMatchConstraint(Zip Code, ^[0-9]{5}$)\"  \r\n},\r\n...\n\nThis is correct, as the zip code column in the dataset may contain some straggling characters. This can be fixed by either reducing the check sensitivity or addressing the issues before the checks are run:\n\nval validDf = schemaResult.validRows  \r\n    .withColumn(\"Zip Code\", F.regexp_extract(F.col(\"Zip Code\"), \"[0-9]{5}\", 0))\n\n7. Collecting Data Quality Metrics\nMetrics provide valuable insights into the health and quality of our data. They can help us see trends, make improvements, and find anomalies in our data. Some metrics are necessary for configured checks and are computed automatically, while others may be needed for external systems, such as monitoring dashboards or data catalogs, and need to be specified manually.\nThe additional metrics need to be added manually as analyzers:\n\nprivate def numericMetrics(column: String): Seq[Analyzer[_, Metric[_]]] = {\r\n    Seq(  \r\n        Minimum(column),  \r\n        Maximum(column),  \r\n        Mean(column),  \r\n        StandardDeviation(column),  \r\n        ApproxQuantile(column, 0.5)  \r\n    )\r\n}  \r\n  \r\nprivate def categoricalMetrics(column: String): Seq[Analyzer[_, Metric[_]]] = {  \r\n    Seq(  \r\n        CountDistinct(column),  \r\n    )  \r\n}\n\nBelow, we create analyzers to generically compute the distribution of numeric columns.\n\nval analysers = (  \r\n    numericMetrics(\"Bottles Sold\")  \r\n    ++ numericMetrics(\"Sale\")  \r\n    ++ numericMetrics(\"Volume Sold (Liters)\")  \r\n    ++ categoricalMetrics(\"Store Name\")  \r\n    ++ categoricalMetrics(\"Vendor Name\")  \r\n    ++ Seq(  \r\n        Completeness(\"Bottles Sold\"),  \r\n    )  \r\n)\n\nSimilar to quality checks, the metrics are computed using the VerificationSuite.run() method:\n\nvar verificationSuite = VerificationSuite()  \r\n    ... \r\n    .saveSuccessMetricsJsonToPath(s\"${args(\"metrics_path\")}/metrics.json\")  \r\n    .addRequiredAnalyzers(analysers)\r\n    ...\n\nThe collected metrics are written to metrics.json file, which can be loaded by external tools. Alternatively, Deequ defines a concept of metric repositories as an interface for saving the metrics to other systems in a generic manner. You can write your own repository to store the metrics in, for example, Prometheus or AWS Cloud Watch.\nAnother useful feature is KLL Sketches which supports approximate, but highly accurate metric calculation on data by sampling.\nIncremental Computation of Metrics\nIn the realm of ETL workloads, it is rare for data engineers to reprocess the entire dataset. Typically, pipelines are designed to be incremental, processing only new data. However, if your data quality checks rely on metrics computed over the entire dataset, this can lead to a continuous increase in load on your Spark cluster.\nInstead of repeatedly running the batch computation on growing input data D, incremental computation is supported hat only needs (t) to consume the latest dataset delta \u2206D and a state S of the computation. Source: technical paper.\n\u00a0\nTo address this challenge, Deequ introduces a concept of \u201cAlgebraic states.\u201d These states store calculated metrics and the corresponding data, enabling their aggregation across multiple pipeline runs. Consequently, only the incremental data needs to be processed, significantly reducing the computational burden.\nWe demonstrate this by adding complete dataset checks within our incremental ETL script. The first step is to record incremental metrics in a temporary in-memory state provider:\n\nval incrementalStateProvider = InMemoryStateProvider()\r\n\r\nval verificationResult = VerificationSuite()\r\n    ...\r\n    .saveStatesWith(incrementalStateProvider)  \r\n    ...\n\nTo load the aggregated state from a persistent provider, a persistent state provider is needed. Additionally, we check if the state already exists to determine whether it should be included in the aggregation, specifically necessary for the first pipeline run:\n\n// Initialize state for incremental metric computation  \r\nval completeStatePath = s\"${args(\"metrics_path\")}/state_repository/\"\r\nval completeStateProvider = HdfsStateProvider(spark, s\"${completeStatePath}/state.json\", allowOverwrite = true)\r\n\r\n// Determine if the complete state already exists\r\nval fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)  \r\nval aggregateStates = try {  \r\n    fs.listFiles(new Path(completeStatePath), false).hasNext  \r\n} catch {  \r\n    case _: FileNotFoundException => false  \r\n}\n\nNow, once again, we can run VerificationSuite, but this time we use the providers to load state data. Consequently, the checks and metrics are computed and merged over the aggregated state, which, in this case, represents the complete dataset:\n\n// Merge incremental metrics with complete metrics, and run data quality checks  \r\nval completeChecks = Seq(  \r\n    Check(CheckLevel.Error, \"Sales complete checks\")  \r\n        .hasSize(_ >= 0, Some(\"Dataset should not be empty\"))  \r\n)\r\n\r\nlogger.info(\"Running complete dataset checks\")  \r\nval completeVerificationResult = VerificationSuite.runOnAggregatedStates(  \r\n    validDf.schema,  \r\n    completeChecks,  \r\n    if (aggregateStates) Seq(completeStateProvider, incrementalStateProvider)  \r\n    else Seq(incrementalStateProvider),  \r\n    saveStatesWith = Some(completeStateProvider)  \r\n)\r\nif (completeVerificationResult.status == CheckStatus.Error) {  \r\n    logger.error(s\"Complete data quality checks failed. Results: ${completeVerificationResult.checkResults}\")  \r\n    sys.exit(1)  \r\n}\n\nThis feature provides granular control over metric computation and therefore supports a series of implementations. For instance, you may choose to save the state only when the entire pipeline succeeds, or you may want to perform anomaly detection on the complete dataset.\n8. Gracefully Handling Data Changes\nWhen working with external data sources, it\u2019s common for changes to occur, which can lead to failed checks if not properly handled. To ensure backward compatibility and smooth data processing, there are two options you can consider:\nFilterable Constraint Checks: these are conditional checks that are only executed if a specific condition is satisfied, such as when the input data is from an older dataset version. This allows you to accommodate changes in the data structure while still maintaining compatibility.\n\nval checks = Seq(  \r\n    Check(CheckLevel.Error, \"Sales base checks\")\r\n        ...,\r\n    Check(CheckLevel.Error, \"Legacy checks\")\r\n        .hasPattern(\"Date\", \"^[0-9]{2}/[0-9]{2}/[0-9]{4}$\".r)\r\n        .where(\"year < 2022\")\r\n)\n\nSplitting by Data Version: Unfortunately, conditional checks are not applicable for schema checks. Cases such as column addition or deletion need to be addressed separately. In such cases, it\u2019s recommended to keep your data versions close at hand and use them as a discriminator to run various checks for different versions. Splitting by version enables you to granular control the checks, while still keeping the code reusability.\n9. Anomaly Detection\nAnomaly detection is a crucial aspect of testing data quality that helps identify unexpected or unusual patterns in the data based on historical observations. Deequ provides several anomaly detection strategies that can be applied to different aspects of the data.\nBefore applying anomaly detection, it is important to store the metrics in a persistent repository. This ensures that historical metrics are available for comparison and trend analysis. In the code snippet below, we use a FileSystemMetricsRepository to store the metrics in a file system location:\n\nval metricsRepository: MetricsRepository =\r\n      FileSystemMetricsRepository(spark, s\"${args(\"metrics_path\")}/metrics_repository.json\")\r\n\r\nvar verificationSuite = VerificationSuite()  \r\n    ... \r\n    .useRepository(metricsRepository)\r\n    ...\n\nOnce at least one data point is collected and stored in the metrics repository, we can apply anomaly detection strategies.\nOne useful application of anomaly detection is keeping the data volume in check. If your dataset is expected to grow at a predictable pace or remain stationary, you can add anomaly detection on the row count. This helps identify unexpected changes introduced by external systems or transformation scripts.\n\nvar verificationSuite = VerificationSuite()  \r\n    ... \r\n    .addAnomalyCheck(\r\n        RelativeRateOfChangeStrategy(maxRateIncrease = Some(2.0)),\r\n        Size(),\r\n        Some(AnomalyCheckConfig(\r\n            CheckLevel.Warning,\r\n            \"Dataset doubling or halving is anomalous\"\r\n        ))\r\n    )\r\n    ...\n\nSimilarly, anomaly detection can be applied to specific columns where you know the expected distribution or behavior of the data.\nWhen an anomaly is found, you can handle it based on the severity of the issue. If the anomaly is critical, you can stop the pipeline to avoid propagating the issue further, or if the anomaly is not severe, you can emit a warning to your monitoring systems for further investigation.\nBy incorporating anomaly detection into your data quality checks, you can proactively identify and address unexpected or unusual patterns in your data, ensuring the quality and reliability of your data pipelines.\n10. Conclusion\nIn this blog, we set up a data quality checking solution for our Spark ETL pipeline by incorporating the open-source library Deequ. We discussed how to use Deequ for schema checking, data profiling, quality constraints testing, quality metric collection, and anomaly detection.\nIf you prefer writing scripts in Python (i.e., PySpark), then PyDeequ can help, which is a Python library for Deequ. At the time of writing this blog, this library is a bit behind and doesn\u2019t yet support some features we discussed.\nCheck out the first part of this blog series \u201cIntroduction to Data Quality\u201d if you haven\u2019t yet. It gives you ideas on how to implement your data quality checks.\nMore Resources\n\nTest data quality at scale with Deequ | AWS Big Data Blog\nBuilding a serverless data quality and analysis framework with Deequ and AWS Glue | AWS Big Data Blog\nAutomating Large-Scale Data Quality Verification \u2013 Original Deequ Technical Paper\nAWS is currently building an integrated solution for data quality checking in AWS Glue. It still lacks many features of Deequ, but it is worth keeping an eye on this one, as it is in active development.\n\nData Quality \u2013 AWS Glue Data Quality\u2013 Amazon Web Services\n\n\nSee more examples on Deequ GitHub page\n\n", "tags": ["Data Quality"], "categories": ["Blog", "Data"]}
{"post_id": 38158, "title": "Using Machine Learning to Improve Search \u2013 Part 1 of 5", "url": "https://www.luminis.eu/blog/using-machine-learning-to-improve-search-part-1-of-5/", "updated_at": "2023-06-16T09:35:52", "body": "\nLarge Language Models and Generative AI\nMachine learning (ML) is a powerful tool to optimize search engines and natural language processing (NLP) capabilities. Using ML can result in more accurate and contextually relevant search results. ML algorithms can analyze vast amounts of data, including user queries, search patterns, and content, to improve search rankings and understand user intent. Another thing you can achieve with ML is searching images with text or even extracting information from images to enrich data.\nI recently started exploring the fascinating area of combining ML with search and decided to write blog posts to explain the possibilities.This first blog post is part of a series: Using Machine Learning to improve search. It consists of five parts. In this first part, I present how to leverage large language models and Generative AI to improve search.\nWhen integrated with search systems, NLP techniques improve the search experience by understanding and responding to user queries in a more sophisticated and context-aware manner. Large language models, such as GPT-4 from OpenAI, represent a significant breakthrough in the field of NLP. These models are designed to understand and generate human-like text by learning patterns and structures from vast amounts of training data.\nShort Introduction to LLMs\nLarge language models are advanced artificial intelligence systems designed to process and generate human-like text. They are built using deep learning techniques and consist of millions or even billions of parameters. These models are trained on massive amounts of text data to learn language patterns, grammar, and context. Once trained, they can perform a wide range of language-related tasks. This includes text completion, summarization, translation, and question-answering. LLMs have the ability to generate coherent and contextually relevant responses, mimicking human language with impressive fluency. Some of these cool things can also be applied to improve the search experience.\nIf you want to know more about LLMs, a lot has been written about it lately. I found this website helpful. It contains a lot of information about machine learning, and the particular page linked explains large language models.\nWays to Use LLMs to Improve Search\nReranking\nAfter obtaining the initial list of documents, we can further improve the search results using a technique called reranking. Reranking involves taking the initial list of documents, generally produced by a more straightforward retrieval model, and applying a more sophisticated model to reorder the documents based on relevance. In this context, the LLM can be used to understand the semantic and contextual relevance of each document to the query and reorder the documents accordingly.\nI recently came across a research paper where the OpenAI GPT-4 model was benchmarked against other reranking models and it seemed to have outperformed them in most areas. This means LLMs have real potential for reranking. If you\u2019re interested in this research, the paper can be found here. There are downsides to using OpenAI or similar APIs depending on the use case, like latency and costs. More on this I explain in the caveats section below.\nThis is a visual representation of how reranking can be implemented.\n\nMatch Explanation\nA search engine works by examining the terms you entered and then comparing these to its index. The engine uses complex algorithms to determine the relevance of each indexed document to your search terms. Sometimes it is unclear to the user why a specific result matched the query, for example, if the result doesn\u2019t contain any search terms. Understanding why a particular search result was ranked highly or presented to the user is crucial for building trust and providing transparent search experiences.\nWhen given a search query and a search result, a LLM can analyze the text of the query and the result, drawing on its extensive training data to identify the likely relevant features. Based on my experience so far, LLM can point out that the result contains many instances of the search terms, that the terms appear in important places like the title or first paragraph, or that the result\u2019s content is closely related to topics associated with the search terms.\nImportant here is how you phrase the prompt (prompt engineering). The way you frame a prompt can guide the model\u2019s responses in terms of length, detail, tone, context, or subject matter. A well-crafted prompt can help the model provide more useful and relevant responses. Because the user\u2019s screen is limited and you want to avoid long texts on your result page, you should instruct the model in such a matter that it returns a short and clear explanation.\nAnother option is to generate a list of the most semantically similar sentences in the document to the query and show these to the user.\nI\u2019ve created some sample code for you to check out and to easily try out match explanation.\nRelevancy Judgement Assistance\nA topic within the search community that is currently being discussed and experimented with is using LLMs to assist with relevancy judgments. Relevancy judgments are used in the field of information retrieval and search engines to evaluate and improve search results. This involves assessing the relevance of the results returned by a search engine in response to a particular query.\nUsually, a certain scale is used to rate the relevance. For instance, a binary scale (relevant, not relevant) or a multilevel scale (highly relevant, somewhat relevant, not relevant) might be employed. In many cases, these judgments are done by human reviewers who manually evaluate the relevance of each search result to the original query. This can be a time-consuming and costly process, but it is often necessary for optimizing the accuracy of search results.\nThis research paper explains automating this process with the use of LLMs. While completely automating this is not ready yet, LLMs can already be used to assist human reviewers with their judgments. Human reviewers struggle to see a pertinent connection when they are lacking world knowledge. LLMs can generate rationales that can explain such connections, similar to the match explanation above.\nContent Enrichment\nThis might not seem search related, but the opposite is true. Quality content is at the heart of every good search experience. This is why I also wanted to mention this part.\nGenerative AI models have the capability to generate high-quality content that can significantly enhance data enrichment. For instance, these models can generate pertinent summaries, comprehensive product descriptions, or contextual details for search queries. This offers considerable value when managing product fact sheets for example. The created summaries and descriptions can subsequently be employed in search functions.\nMoreover, generative AI models can offer assistance by expanding on existing content and synthesizing additional paragraphs, examples, or detailed explanations pertinent to a specific topic. This helps users understand better by providing them with lots of information and context, which expands their knowledge of the topic. Importantly, this wealth of information can also be incorporated into search operations.\nAs with match explanation, prompt engineering is important here too. In my experience, it\u2019s even more important in this part because your content should match the tone of voice of the company and should fit in the data structure (e.g. not too short or too long).\nEmbeddings\nOne of the key challenges in search is understanding the context and intent behind a user\u2019s query. Vector search is a technique used to find similar items based on their vector representations in a high-dimensional space. In the context of NLP, embeddings are vector representations of words, phrases, or documents. These embeddings capture semantic and contextual information. This allows similar items to be represented, as vectors are closer together in the embedding space.\nEmbeddings are typically created by ML models which are trained on datasets of specific domains. LLMs, on the other hand, are trained on large text corpora, enabling them to develop a robust understanding of language and context. This means they are often better at creating more meaningful and contextually aware embeddings than traditional models, resulting in more accurate search results.\nThat being said, it\u2019s important to note that the effectiveness of using LLMs for creating search embeddings depends on the specific use case and dataset. In some cases, simpler or more traditional models might perform just as well or even better. Especially when computational resources or data are limited or your data is niche.\nAn easy way to combine LLMs with vector search is to use the LangChain framework. My colleague Jettro has made a blog post about that, you can read it here.\nIn Part 2 of this blog series, I dive deeper into vector search and embeddings.\nCaveats\nThis all sounds very cool, but I feel it is important to also mention some things that should be taken into account before applying any of the above.\n\nOverfitting and irrelevant information: While the LLMs are designed to respond based on patterns it has observed during their training, they can occasionally generate outputs that include irrelevant or inaccurate information due to over-generalizing from the data it was trained on.\nNot up to date: LLMs were not designed for real-time learning or updating their knowledge. They are trained on a static dataset and do not have the ability to learn new information after training. This means they might not have information on recent events or developments.\nData privacy: There could be potential privacy concerns if a search engine built on an LLM is not designed with strong data privacy protections. Users would need to be assured that their queries are handled confidentially, and that the system isn\u2019t retaining or learning from their personal data.\nNiche data: LLMs can struggle with highly specialized terminology or context, as their understanding is based on patterns they\u2019ve observed in their training data. If a niche topic has unique contexts that were not adequately represented in the training data, the model might not respond accurately.\nLatency: Larger models require more computational power to process and generate responses. This can lead to longer response times, especially if the model is not optimized or if hardware resources are limited. The length of the generated responses and the number of requests the model has to handle concurrently can impact latency too. Longer responses take more time to generate and transmit. If a model serves a high volume of queries simultaneously, response times may increase.\nCost: If you\u2019re using a closed-source model, you probably have to pay for each request sent to the API wrapped around the model. Larger and more comprehensive models tend to be more expensive. Experiment with different models to see which one aligns best with your requirements, and whether it justifies the associated expenses.\n\nFinal Thoughts\nThe examples above are just a few of the capabilities of LLMs. Development in this area is progressing rapidly. Each day, these tools become more efficient, accurate, and easier to implement, signaling a transformative shift in search mechanisms.\nHowever, there are caveats, but I firmly believe with rigorous experimentation and refinement, we can navigate these hurdles. It\u2019s essential to know that LLMs are not futuristic constructs. They are here, now, and accessible for everyone to use. Now is the time to start experimenting!\n", "tags": ["Generative AI", "Large Language Models", "Machine Learning", "search"], "categories": ["Blog", "Search"]}
{"post_id": 38233, "title": "The sustainability of IT: interview with Software Architect Steef", "url": "https://www.luminis.eu/blog/de-verduurzaming-van-it-architecturen-interview-met-software-architect-steef/", "updated_at": "2023-08-22T11:13:46", "body": "Luminis helps organizations innovate successfully by empowering them with modern software technologies. However, we cannot separate the work we do from the impact we have on the world. We are working on that collectively in various ways, both in terms of social engagement, sustainability, and diversity & inclusion. In this article, we dive into the sustainability of IT architectures.\nWith the expertise we have, we can and want to work hard for a better world. We like to give our colleagues the freedom to contribute their own ideas and projects. For this interview, we spoke with Steef Burghouts about his experiences and ambitions regarding software development & sustainability.\n\nSteef is a Software Architect at Luminis. Two years ago, he started with Accelerate, a training program to fast-track the tech leaders of the future. During this program, his ambition to make social impact was sparked. Since then, Steef has a variety of interesting projects to his name. His interest lies in sustainability and climate in combination with technology.\nClimate change issues in IT\nSteef\u2019s interest in social impact projects has grown over the past two years. When asked why he has a greater focus on this, he answered:\n\u201cAt the beginning of my career, I mostly went from project to project, without really thinking about the subsequent impact of what I was working on. During the Accelerate program, you work on your personal growth. Part of this includes conversations with coaches who ask questions about your ambitions and dreams. I realized that I also want to commit myself to social projects, for example concerning climate change issues. Climate change is one of the biggest challenges of our generation. I like to be part of the solution.\u201d\nSteef\u2019s mission? To create awareness about the CO2 emissions of IT and the effect it has on the climate. The CO2 emissions of the IT industry are less visible and less measurable than for example aviation. As a result, IT\u2019s impact on the climate sometimes doesn\u2019t get enough attention during climate discussions. However, research indicates that the global IT industry is responsible for two to four percent of global CO2 emissions. By 2040, it is estimated to be between six and 14 percent.[1] This amount is comparable to the total emissions from aviation, so it is about time for some changes.\nOne reason that the IT has such high CO2 emissions is because of the continuous running of entire IT landscapes. This is where a major gain can be made, both in terms of CO2 emissions and financially. To achieve this, Steef is working on a tool for mapping the climate impact of IT architectures. Within IT, security gets attention in almost every project and conversation, so why not talk more often about sustainability?\nObviously, it\u2019s easier said than done: starting the conversation with each client about sustainability and the impact of their IT landscape. This takes time and money. Fortunately, we have more colleagues like Steef who see the importance of this development. They test different methods during their work with customers, with the objective to pragmatically come up with a new approach.\nMore impact using smart systems at Milieudefensie\nIn 2022, Steef introduced Milieudefensie (Dutch for \u201cEnvironmental Defense\u201d) as a customer to Luminis. Milieudefensie has a big impact on the climate policy of the Netherlands, for example by filing lawsuits against big players like Shell. For these lawsuits, Milieudefensie makes many WOO (Wet Open Overheid, Dutch for \u201cOpen Government Act\u201d requests to the Dutch government. Milieudefensie receives the answers to these requests as images, of which certain parts cannot be shared publicly and are taped off. The requests sometimes consist of thousands of pages of images, which are not easily searchable. Milieudefensie had to search these requests page-by-page, which took an awful lot of time.\nWhat have our colleagues done for Environmental Defense? Luminis developed a document system that converts the images of WOO requests into text and makes them searchable. This makes it possible to search through the thousands of pages of images in a targeted manner through which useful information is found sooner. Milieudefensie saves more than 1 FTE per month by doing this. They can now use this time to make even more impact.\nWhat Luminis has done for Milieudefensie is in line with the role we want to play more often. Milieudefensie showed us the value of making an impact. Steef explains why he finds this customer so important:\n\u201cI think Milieudefensie is a good organization because they are constantly looking very pragmatically at how they can make an impact. Moreover, it is very clever how they have filed and won various cases against, for example, the Dutch government and powerful companies. I sent Milieudefensie an email and asked what their challenges are and how we can help them. Their focus is not on IT, but good IT facilities do make their work easier. So it was a natural choice to use our knowledge and expertise there.\u201d\nA tool for every customer: the Sustainability Scan\nSustainability and climate chance are present in Steef\u2019s work every day. To start the conversation about sustainability with all our customers, he is now developing the Sustainability Scan. Its purpose is to map the emissions of an IT landscape and offer suggestions on how to reduce them. Currently, IT emissions are not included in calculating an organization\u2019s carbon footprint. However, measuring and knowing emissions is essential for creating awareness to reduce emissions step-by-step.\nThe question is why should our customers want this? Research shows that it is sometimes difficult to consider sustainability when making technology decisions.[1] All too often the emphasis is still on speed or continuity. Because security and reliability are paramount, overcapacity is often built in, for example by using extra servers. This not only costs a lot of money, but also causes extra CO2 emissions. As part of the Climate Agreement, the Dutch IT sector has stated its ambition to be climate neutral by 2030. An ambitious goal, to which everyone must contribute: from organizations and suppliers to developers.\nTherefore, at Luminis, we are busy testing and further developing the Sustainability Scan. We started this initiative in 2022, with which we also mapped our own emissions. Something we also like to make possible for our customers. Steef explains how the Sustainability Scan works:\n\u201cThe first step is a conversation with the customer to see what their IT landscape currently looks like. Once that is mapped out, we calculate the carbon footprint of that IT landscape. The next step is figuring out how this can be reduced using tools from cloud providers like AWS and Azure.\u201d\nA tangible example of applying the Sustainability Scan is analyzing an organization\u2019s running servers. It is common knowledge that many companies, under the pretense of continuity and availability, regularly operate with too many or too heavy servers. A less heavy server is a more sustainable solution in terms of climate as well as financially. However, you need to know the customer very well to advise on this.\n\u201cA company like Ticketmaster needs very heavy servers at times, but not when there are no tickets being sold. One solution could be to deploy additional servers at peak times and switch back at quiet times.\u201d\nThese kinds of insights and desires are included in the Sustainability Scan and ultimately provide each client with appropriate advice. Curious about the Sustainability Scan? Contact Steef Burghouts at <steef.burghouts@luminis.eu>.\n[1] ABN-AMRO. 2021. Verduurzaming van IT.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 38195, "title": "Accelerate Craftsmanship, opleidingstraject van Thales, De Belastingdienst en Luminis vliegend van start!", "url": "https://www.luminis.eu/blog/accelerate-craftsmanship-opleidingstraject-van-thales-de-belastingdienst-en-luminis-vliegend-van-start/", "updated_at": "2023-05-31T17:16:45", "body": "Apeldoorn, 31 mei 2023 \u2013 Op vliegveld Teuge start vandaag Accelerate Craftsmanship met een feestelijke kick-off. Dit opleidingstraject, waarvoor Thales, de Belastingdienst en Luminis de handen voor de derde keer ineenslaan is een op maat gesneden traject voor toptalenten van de drie organisaties. Het centrale thema is Software Craftsmanship, belicht vanuit zowel technisch inhoudelijk vlak maar vooral in combinatie met persoonlijke ontwikkeling.\n\n\u00a0\nWaarom Accelerate Craftsmanship?\nIn een wereld waarin software steeds belangrijker en zelfs onmisbaar wordt voor organisaties die zich willen onderscheiden van de concurrentie, zijn vakmensen nodig. Zij die de kennis en skills hebben om organisaties naar de toekomst te leiden. De hiervoor benodigde expertise ontwikkel je niet zomaar, daar zijn intensieve trainingen op gebied van technologie en soft skills voor nodig. Dat is precies wat Accelerate Craftsmanship biedt.\nIn samenwerking met How Company worden sessies gegeven over communicatie en persoonlijk leiderschap. Daarnaast komt ook technologie uitgebreid aan bod in sessies over onderwerpen als OO en clean coding principes, service design, refactoring, database ontwerp en best practices voor het ontwikkelen van veerkrachtige systemen en services. De combinatie van intensieve begeleiding vanuit coaches, de stuurgroep van het traject en How Company helpt deelnemers hun potentieel optimaal te benutten en hun persoonlijke doelstellingen te behalen.\n\u201cOndanks dat er geen compressiemechanisme voor ervaring bestaat, halen we in dit traject alles uit de kast om deelnemers in 10 maanden tijd een ontwikkeling te laten doormaken waarover ze anders jaren zouden doen. Met wat ze leren in Accelerate worden deelnemers niet alleen betere engineers maar zullen ze ook vooral op persoonlijk vlak grote stappen zetten. Iets waar ze de rest van hun leven van kunnen profiteren.\u201d Vertelt Bert Ertman, VP Technology bij Luminis en initiatiefnemer van het Accelerate concept.\n\u00a0\nEen traject vol life-changing inzichten\nAccelerate Craftsmanship is de derde in een succesvolle reeks Accelerate-trajecten. Thales en Luminis startten in 2020 met dit initiatief om tech-talent uit hun eigen organisaties klaar te stomen voor een toekomst waarin software steeds belangrijker wordt. De voorgaande trajecten, waarin ook al werd samengewerkt met de Belastingdienst, vormen de basis van deze verkorte Accelerate versie (10 maanden in plaats van 18 maanden red.), waarin de meest relevante onderwerpen en sessies uit eerdere trajecten aan bod komen.\n\n\u00a0\nDe impact van Accelerate op deelnemers is groot. De intensiviteit en vorm van het traject bieden deelnemers bijzondere groeikansen en inzichten op persoonlijk ontwikkelvlak. Zowel op werkgebied als priv\u00e9 heeft Accelerate oud-deelnemers veel gebracht. De deelnemers vervullen nu soms een rol als coach in nieuwe trajecten, of zijn doorgestroomd naar invloedrijke posities binnen de deelnemende organisaties.\nMarleen Hurenkamp, Productmanager vakontwikkeling IV bij de Belastingdienst vertelt over hun deelname aan Accelerate: \u201cIn het kader van de strategische doelstelling van de directie Informatievoorziening van de Belastingdienst om te gaan behoren tot een van de top IT werkgevers geven we onze medewerkers alle ruimte om bezig te zijn met hun ontwikkeling om zo te groeien in hun vakmanschap, zowel op inhoud als op vaardigheden en competenties. We willen talent laten bloeien. Het Accelerate-programma in de variant van Craftmanschap voor onze medioren past naadloos bij deze doelstelling en geeft onze talentvolle software engineers de stuwkracht en ruimte om versneld door te groeien tot het niveau van een senior.\u201d\nEn ook voor mede-initiatiefnemer Thales biedt dit traject veel, aldus Henk van Steeg, Head Software Engineering bij Thales: \u201cThales heeft de wind mee, we groeien enorm. Naast het aantrekken van veel nieuwe mensen investeren wij ook fors in de groei van onze collega\u2019s. We hebben interne trainingsprogramma\u2019s en werken intensief samen met de andere partijen uit het Accelerate-traject. Dit stelt onze mensen in staat hun kennis te verbreden en te sparren met peers.\u201d\nWe wensen de deelnemers, coaches en stuurgroep veel succes en kijken uit naar een intensieve en leerzame samenwerking de komende 10 maanden.\nMeer weten over het Accelerate-programma of IT-trainingen? Bekijk de website van de Luminis Academy of neem contact op met Louis Pouwels, contactpersoon van de Luminis Academy (academy@luminis.eu).\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 38179, "title": "Vector search using Langchain, Weaviate and OpenSearch", "url": "https://www.luminis.eu/blog/vector-search-using-langchain-weaviate-and-opensearch/", "updated_at": "2023-05-30T09:53:04", "body": "\nWith the popularity of ChatGPT and Large Language Models (LLM), everybody is talking about it. On my Linkedin home page, about 90% of the posts seem to speak about ChatGPT, AI, and LLM. With my experience in search solutions and interest in everything related to Natural Language Processing and Search, I also had to start working on solutions.\nA few weeks ago, I visited the Haystack conference in Charlottesville. Listened to good talks and had interesting conversations with like-minded people. I learned about a framework called Langchain. LangChain is a framework for developing applications powered by language models, making working with other products like vector databases and large language modes much more straightforward. OpenSearch and Weaviate are among the integrations which I know well. I decided to experiment with a similarity search using both products.\nThis blog post discusses the sample. It shows the different steps to accomplish the following task:\n\nRead content using a Langchain loader.\nStore data in OpenSearch and Weaviate using the Langchain VectorStore interface.\nPerform a similarity search using the Langchain VectorStore interface\nPrint the results, including the score used for sorting\n\nRunning the project yourself\nYou can find the source code of the project on GitHub:\ngit clone git@github.com:jettro/MyDataPipeline.git\r\ngit checkout blog-langchain-vectorstores\nYou need to set up Python to run the sample. Next to Python, you need a running OpenSearch instance. I have provided a docker-compose file in the infra folder. For Weaviate, I advise using a sandbox environment. You must also create a .env file in your project\u2019s root folder. You can use the env_template file as a template for your .env file.\npython3 -m venv .venv\r\nsource .venv/bin/activate\r\npip install -r requirements.txt\nNow you can run the file run_langchain_ro_vac.py, but before you do, change the properties do_load_content at the bottom to True. That way, you will load the content into OpenSearch and Weaviate. After one successful run, you can switch them back to False.\nLoad the content using LangChain.\nLangChain uses Loaders to fetch data. It comes with a lot of different loaders to load data from databases, csv files, remote json files. It does however not support remote XML out of the box. The dataset we are using is coming from the Dutch government. It contains frequently asked questions, called \u201cVraag Antwoord Combinaties\u201d. The default format is XML, you can request json if you want, but I stuck with XML. Therefore I had to create a customer XML loader. The code for the loader is in the code block below.\nimport xml.etree.ElementTree as ET\r\nfrom urllib.request import urlopen\r\n\r\nfrom langchain.document_loaders.base import BaseLoader\r\nfrom langchain.schema import Document\r\nclass CustomXMLLoader(BaseLoader):\r\n    def __init__(self, file_path: str, encoding: str = \"utf-8\"):\r\n        super().__init__()\r\n        self.file_path = file_path\r\n        self.encoding = encoding\r\n\r\n        def load(self) -> list[Document]:\r\n        with urlopen(self.file_path) as f:\r\n\r\n        tree = ET.parse(f)\r\n        root = tree.getroot()\r\n\r\n        docs = []\r\n        for document in root:\r\n            # Extract relevant data from the XML element\r\n            text = document.find(\"question\").text\r\n            metadata = {\"docid\": document.find(\"id\").text, \"dataurl\": document.find(\"dataurl\").text}\r\n            # Create a Document object with the extracted data\r\n            doc = Document(page_content=text, metadata=metadata)\r\n            # Append the Document object to the list of documents\r\n            docs.append(doc)\r\n\r\n        return docs\nBelow is the function that makes use of the XML loader. The specific VectorStore is passed into the method.\ndef load_content(vector_store: VectorStore) -> None:\r\n    custom_xml_loader = CustomXMLLoader(file_path=\"https://opendata.rijksoverheid.nl/v1/infotypes/faq?rows=200\")\r\n    docs = custom_xml_loader.load()\r\n\r\n    run_logging.info(\"Store the content\")\r\n    vector_store.add_documents(docs)\nBefore inserting documents, you have to provide the schema for the class. Weaviate does create a default schema for your content, but that does not work with similarity search. You need a field that stores embedded versions of the content. Since loading the schema has nothing to do with LangChain, I will not post all the code. Please look at the provided repository. I want to show you part of the schema that works for the LangChain similarity search. In the schema, I configure the class RijksoverheidVac and the field text, which is the default field for LangChain. We can change this if we want.\n{\r\n  \"class\": \"RijksoverheidVac\",\r\n  \"description\": \"Dit is een vraag voor de Rijksoverheid \",\r\n  \"vectorizer\": \"text2vec-openai\",\r\n  \"properties\": [\r\n    {\r\n      \"dataType\": [\r\n        \"text\"\r\n      ],\r\n      \"moduleConfig\": {\r\n        \"text2vec-openai\": {\"skip\": false, \"vectorizePropertyName\": false}\r\n      },\r\n      \"name\": \"text\"\r\n    }\r\n  ]\r\n}\nCreate Weaviate VectorStore and execute a similarity search\nNext, we can construct the Weaviate client, the Weaviate VectorStore, and the VectorStoreIndexWrapper. Notice in the code below that:\n\nI use a wrapper around the Weaviate client. This wrapper makes interacting with Weaviate easy. LangChain uses its\u2019 own wrapper within the VectorStore.\nUsing the parameter do_load_content, you can control a fresh load of the content.\nIn the additional field, we pass the field certainty (I added this feature in a pull request :-))\nI use the field certainty in the _additional field of metadata to print the Weaviate certainty, which is used as a score.\n\ndef run_weaviate(query: str = \"enter your query\", do_load_content: bool = False) -> None:\r\n    weaviate_client = WeaviateClient()\r\n    vector_store = Weaviate(\r\n        client=weaviate_client.client,\r\n        index_name=WEAVIATE_CLASS,\r\n        text_key=\"text\"\r\n    )\r\n\r\n    if do_load_content:\r\n        load_weaviate_schema(weaviate_client=weaviate_client)\r\n        load_content(vector_store=vector_store)\r\n\r\n    index = VectorStoreIndexWrapper(vectorstore=vector_store)\r\n    docs = index.vectorstore.similarity_search(\r\n        query=query,\r\n        search_distance=0.6,\r\n        additional=[\"certainty\"])\r\n\r\n    print(f\"\\nResults from: Weaviate\")\r\n    for doc in docs:\r\n        print(f\"{doc.metadata['_additional']['certainty']} - {doc.page_content}\")\r\n\r\n\nThis method gives you the power to do a similarity search against Weaviate.\nCreate OpenSearch VectorStore and execute similarity search.\nNext, I show you that working with OpenSearch is similar to Weaviate. It is not the same, but similar. Managing the index does work from LangChain. So there is no need to create it by ourselves. The following code block should now be self-explanatory. Notice that we have to provide our own embedding here. Weaviate uses the schema to determine how to create the embeddings. With OpenSearch, we provided our own embeddings. In the end, we use OpenAI for both embeddings.\ndef run_opensearch(query: str = \"enter your query\", do_load_content: bool = False) -> None:\r\n    auth = (os.getenv('OS_USERNAME'), os.getenv('OS_PASSWORD'))\r\n    opensearch_client = OpenSearchClient()\r\n\r\n    vector_store = OpenSearchVectorSearch(\r\n        index_name=OPENSEARCH_INDEX,\r\n        embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv('OPEN_AI_API_KEY')),\r\n        opensearch_url=\"https://localhost:9200\",\r\n        use_ssl=True,\r\n        verify_certs=False,\r\n        ssl_show_warn=False,\r\n        http_auth=auth\r\n    )\r\n\r\n    if do_load_content:\r\n        opensearch_client.delete_index(OPENSEARCH_INDEX)\r\n        load_content(vector_store=vector_store)\r\n\r\n    docs = vector_store.similarity_search_with_score(query=query)\r\n    print(f\"\\nResults from: OpenSearch\")\r\n    for doc, _score in docs:\r\n    print(f\"{_score} - {doc.page_content}\")\r\n\r\n\nTesting the integration\nNow you can test the example. You should see similar results using the code below.\nif __name__ == '__main__':\r\n    run_logging.info(\"Starting the script Langchain Rijksoverheid Vraag Antwoord Combinaties\")\r\n\r\n    query_str = \"mag ik een groen zwaai licht\"\r\n\r\n    run_weaviate(query=query_str,\r\n                 do_load_content=False)\r\n\r\n    run_opensearch(query=query_str,\r\n                   do_load_content=False)\r\n\r\n\nThis is the output.\nResults from: Weaviate\r\n0.9312953054904938 - Mag ik een zwaailicht of een sirene gebruiken op mijn auto?\r\n0.9251135289669037 - Hoe kan ik mijn duurzame initiatief voor een Green Deal aanmelden?\r\n0.9233253002166748 - Wanneer moet ik mijn autoverlichting gebruiken?\r\n0.9228493869304657 - Wat is groen sparen of beleggen?\r\n\r\nResults from: OpenSearch\r\n0.78848106 - Mag ik een zwaailicht of een sirene gebruiken op mijn auto?\r\n0.7636849 - Wat is groen sparen of beleggen?\r\n0.755817 - Hoe kan ik mijn duurzame initiatief voor een Green Deal aanmelden?\r\n0.75559974 - Wanneer moet ik mijn autoverlichting gebruiken?\nConcluding\nI hope you learned that it is not hard to start working with vector-based similarity search using OpenAI, LangChain, Weaviate, and OpenSearch. These are exciting times. We can improve search results using vector-based search. We can start using Hybrid Search\u2014more on these topics in another blog.\n", "tags": ["LangChain", "OpenAI", "OpenSearch", "weaviate"], "categories": ["Blog", "Search"]}
{"post_id": 38149, "title": "The Evolution of AWS from a Cloud-Native Development Perspective: Data, Sustainability, Builder Experience, and Serverlessification", "url": "https://www.luminis.eu/blog/the-evolution-of-aws-from-a-cloud-native-development-perspective-data-sustainability-builder-experience-and-serverlessification/", "updated_at": "2023-05-26T12:32:04", "body": "AWS doubles down on its initiatives announced at re:Invent 2021. Data, Sustainability, and a lower barrier to integrating services and onboarding developers seemed to be the recurring themes for AWS re: Invent 2022. Now that 5 months have passed, it seems like a good moment to re:Cap and evaluate what happened in the world of AWS post re:Invent \u201822.\nIn this blog post, we will walk you through the most important movements announced by AWS at re:Invent 2022 and discuss the impact on AWS cloud-native builders.\n\nLingering impressions\nLike the \u201821 edition, the \u201822 edition of the AWS annual conference didn\u2019t feature any game-changing announcements, but quite some very nice enhancements on existing services. Next to that, AWS seems to pivot into developing verticals: Services that cover a specific industry. Verticals can be really beneficial to customers that don\u2019t have the capital to invest in developing their own solution or don\u2019t want to be distracted by developing services that are not their core business. However, the need to spin up the innovation flywheel to find product-market-fits is more important than ever, because with the announcement of AWS Supply Chain, AWS has proven that it can and will take over your commodity business. The AWS cloud is maturing at a pace that could make a lot of businesses irrelevant.\nNowadays we have many ways to store data in the cloud. The next step is to have tools and knowledge to make use of this data. AWS is focusing on this next step with their announcements at re:Invent 2022.\nA lot of effort is being put into improving the builder experience. AWS wants you to be able to build as much as you can as fast as possible on their platform, whoever you might be. It does not matter if you are a seasoned developer or have absolutely no experience, there is an entry point for you. Or at least, that is the goal.\nMaking more use of the term \u2018serverless\u2019 has also been on the agenda of AWS since 2021. We have seen new serverless versions of existing services arrive, even though not all of them can scale down to 0 and some have even an entry cost. While on the one hand, it is always nice to have options, the other side of the story is that we need to have a discussion about what the term serverless means nowadays.\nAnother topic that AWS builds upon from the previous year is sustainability. Where in 2021 the major announcement was the addition of the sustainability pillar to the Well-Architected Framework, this year it is about concrete ways of having your workload consume less energy and also a promise to be water positive by 2023.\nIn the rest of this blog, we will dive deeper into these topics.\n\nBuilding a future-proof data foundation\nBesides the several announcements of new services like Amazon DataZone or enhancements of existing services like\u00a0 AWS Glue Data Quality, AWS has come to realize that education is necessary for us to make sure all the data we have available is put to work.\nInstead of collecting dust inside our data warehouses, AWS wants to build a future-proof data foundation by educating current and future generations and providing them with the skills necessary to leverage all the information that is hidden in the vast mountains of data. They provide over 150 courses online courses, low-code, and no-code tools to enable people to become data literate.\nServices that don\u2019t incur the need for re-architecting your system or add technical debt when future requirements change are key characteristics of the announcements done regarding data-related services. One concrete example of this is easier integration of services to prevent unnecessary moving around of data like Amazon red shift auto-copy from s3.\n\nOngoing serverlessification\nRelated to this is the ongoing serverlessification of the AWS services. Originally serverless was used only for services that utilized a pay-as-you-go model, however, AWS seems to move away from this paradigm with the announcement of other \u2018serverless\u2019 services.\nAn example is Amazon OpenSearch Serverless. The serverless nature of this service is debatable since you already have a hefty bill of $700 for only enabling it. Not really pay-as-you-go anymore, is it? However, AWS seems to be serious about the serverlessification of the AWS services. Last re:Invent several enhancements of services were announced that provide a higher level of abstraction onto services to lower the barrier-of-entry (Amazon OpenSearch Serverless), make it more cost-efficient (efforts around a zero-ETL world), or remove the necessity of makeshift solutions to bend services to your business needs (Amazon EventBridge Pipes).\nAWS is changing the meaning of serverless to \u2018a service that entails a higher abstraction\u2019 so that customers don\u2019t have to be bothered with all kinds of technical stuff under the hood. More focus on adding value for customers, faster time to market, and less distraction caused by implementing and managing commodities.\n\nA more efficient AWS builder\nAs we mentioned last year in our recap of re:Invent, AWS aims to improve business agility for their users by reducing complexity.\nLooking at the announcements made during pre:Invent and re:Ivent it becomes clear that AWS still tries to remove as much of the undifferentiated heavy lifting done by builders on their platform. Every line of code can be considered a liability and if AWS can take care of a piece of (glue) code for us, I would always opt for that. As a developer or engineer you just want to get business value out to the customer as soon as possible and it clearly shows that AWS is listening to the developers using their platform. The announcements of Eventbridge pipes, the new filtering SNS payloads feature and more advanced filtering capabilities in Amazon Eventbridge are an example of that and will reduce the amount of code being written by developers while working on the AWS platform. This in turn will let the builders focus more on their actual business logic.\nAnother great example of this is a recent improvement in AWS Lambda. From our experience, a lot of enterprises are standardized on Java and/or .Net, and as more and more companies are migrating their workloads to the cloud we see that Java developers are also trying to adopt new compute models. We sometimes see some of these Java and .Net developers skip services like AWS Lambda and directly go running their application in containers due to the fact that Java-based applications suffer from slower start times a.k.a. \u2018cold start\u2019 issues. This is mainly due to the fact that Java is a statically compiled language and the JVM and popular frameworks like Spring require more memory and take seconds before being able to serve traffic compared to dynamic interpreted languages like Python or Javascript.\nWithin the Java ecosystem, \u2018new\u2019 frameworks like Quarkus and Micronaut aim to solve some of these issues and also be more lightweight. These frameworks tightly integrate with new innovations like GraalVM, which compiles the applications to native code, resulting in faster startup times and a much lower memory footprint for Java-based applications.\nHowever, learning new frameworks like Quarkus, Micronaut or GraalVM takes time and it takes time away from delivering business value. To help builders spend more time on building on the platform instead of for the platform, AWS introduced AWS Lambda Snapstart. With Lambda Snapstart you can get up to 10x improvement on your Java Lambda cold start. SnapStart takes a memory snapshot of the state of the JVM after it has launched and initialized. Lambda Snapstart is a clear example of what can be done if you own the entire stack. It makes use of the snapshot functionality of the underlying FireCraker VM and a checkpoint and restore feature of the JVM.\n\nOne other change AWS seems to make is taking a suite approach in supporting builders during the entire software development lifecycle (SDLC), from feature request to deployment. Up until now, Amazon offered a lot of separate tools (CodeCommit, CodeBuild, CodePipeline, etc) that could be combined to support developers, but some important parts were missing and teams had to find those outside the AWS environment. We see that bigger enterprises have adopted suite-based solutions like (Microsoft) Github + Github Actions and Azure DevOps. With the announcement of Amazon CodeCatalyst that might be about to change for organizations working extensively with AWS. Amazon CodeCatalyst is a product offered outside of the well-known AWS Console as a standalone product. It allows you to integrate or connect to familiar products like JIRA and GitHub and the syntax for pipelines is based on the GitHub actions workflow model. This is an interesting move and we hope to see the product evolve over the next couple of years.\nOne Cloud for All\nWhen you walk around re:Invent or even watch some of the sessions online you will notice the word \u201cbuilder\u201d come up a lot. Not engineers, not developers, but builders. With the maturity of the foundation of the AWS platform, one focus of AWS has been to lower the entry barrier for building solutions on AWS and increase the target audience to extend beyond the technical-apt.\nWe have seen this previously with tools like AWS CDK and AWS Amplify, where the complexity of AWS CloudFormation has been abstracted away behind a more user-friendly and user-specific layer. The latter is more prevalent when looking at AWS Amplify, which caters specifically to frontend engineers that want to build AWS solutions, allowing them to quickly spin up an AWS backend and easily hook it up to their frontend framework of choice without having to know too much about what is going on under the hood.\nHowever, these tools seem to be only the first stop down this avenue that AWS is moving through. The steps AWS is making indicate a goal of getting everyone and their parents to be able to build AWS solutions. This strategy makes sense from a competitive point of view. Making AWS a household name where the creatives of the world can easily bring their ideas to life will introduce a new revenue stream previously blocked by the technology barrier.\nOnce building AWS solutions become \u201ceasy\u201d enough, it will allow a new wave of disruptors to reach the market with a development speed and cost-effectiveness never seen before. And if they are going to change the world anyway, why not have it all run on the AWS infrastructure? Going this route will be a win-win for AWS and the innovators of tomorrow.\nOne of the new announcements this year at AWS re:Invent is the launch of AWS Application Composer, which allows you to visually design serverless applications, including APIs, background batch processing, and event-driven workloads. You can focus on what you want to build and let Application Composer handle the how. While this launch moves AWS closer to its world domination goals, because compared to CDK you don\u2019t even need to code anymore, it still requires a knowledge of the building blocks to be able to create a working and useful solution. Nevertheless, this is a step in the right direction.\n\nAnother development in this area is the announcement of AWS Code Whisperer, which will allow users to generate code by simply writing something similar to \u201cgenerate a function that uploads a file to s3 using server-side encryption\u201d. No doubt an answer to GitHub\u2019s (Microsoft) Co-Pilot, it brings AI into the mix to assist the AWS Builder in creating their solutions.\nWhile Code Whisperer requires even more low-level knowledge than App Composer to get your solution running, the interesting property of Code Whisperer is the use of natural language as the human interface. We have seen what the combination of natural language and AI can do with the rise of ChatGPT. If this is the way people are going to \u201cdo things\u201d moving forward, it would be in the best interest of AWS to jump on the bandwagon and provide a ChatGPT-like interface to build AWS solutions. We have no doubt this is already being looked at.\n\nAll cloud for one\nAWS is not only targeting \u201ceveryone\u201d, they are also targeting \u201cyou specifically\u201d. AWS has released high-level services over the years that cater to very specific/vertical use cases. One example is AWS Connect, which was featured in one of the customer\u2019s highlights during a keynote this year. AWS Connect is an AWS service specifically designed to improve customer service by adding a cloud contact center and using AI to improve agent efficiency.\nA new service of this type announced this year is AWS Supply Chain, which is an AWS-managed service that, as the name implies, helps you manage your supply chain by leveraging AWS services and machine learning to provide visibility and help make decisions. Amazon as an e-commerce company has many years of experience in this domain, so it stands to reason that they have packaged many relevant best practices into this service. This is not only AWS providing a service, it is also them sharing their knowledge through this service.\n\nContinuing this trend of offering functionality derived from their own experience, AWS has released AWS Ambit scenario designer, an open-source set of tools to create 3D content. AWS has been functioning as a game studio for the last 10 years. With the release of this toolset, this again focuses on a completely new set of builders.\nWhat these developments aim to do is invite new builders to AWS that are trying to achieve very specific goals. AWS will offer them a user-friendly, end-to-end solution to do this, backed by the power of the AWS cloud. These managed services make it possible for users with minimal AWS knowledge to get things up and running for their vertical use cases. However, in our experience, as soon as you want to extend beyond what the managed service has to offer or if you should run into an error, a more advanced level of knowledge becomes necessary, where the gap in knowledge between the two scenarios is unexpectedly large. This is where the AWS expert builders come into play and why having a deeper level of AWS knowledge will be beneficial.\nWe expect AWS to continue this trend and announce even more of these vertical services related to their own expertise. One of their key areas of focus is sustainability. They have pledged to become \u201cwater-positive\u201d by 2030. It wouldn\u2019t surprise us to see a service appear down the line that allows you to receive actionable insights about your water and energy usage for your facilities, thus helping other companies also become more sustainable.\n\u00a0\n\n\u00a0\nBeing more sustainable\nA topic that is hot and happening: Sustainability and future-proof computing. Something we cannot ignore, given the ongoing change of our climate and all the coverage it gets in the media.\nThis is not a new topic for AWS, as last year they released the Carbon footprint tool and introduced the sustainability pillar to name a few actions.\nOne of the causes of this climate change can be attributed to our energy hunger and seemingly lack of efficiency in producing and consuming that energy. And let\u2019s be fair: Our electronics have become more and more efficient over the last decades, but our demand also grew exponentially. The computing power of my iPhone has increased significantly compared to one from 2013, while my battery doesn\u2019t last any longer. While efforts to increase power efficiency remain, we don\u2019t back off on the demand of workloads.\nThis re:Invent AWS announced several improvements in performance and efficiency: Smaller-sized hardware or compute resources performing the same workload, which results in less energy consumption.\nAWS\u2019 driver to being more sustainable seems uncertain, however: it could be caused by laws or the expectation that laws regarding sustainability will be created in the near future, or it\u2019s just marketing (greenwashing). Regardless of the reason that drives them, AWS seems to be serious about this, because of the number of announcements that have some relation with sustainability.\nManaged services increase the sustainability of your cloud workloads\nAWS is heavily advising customers to use managed services over EC2 instances. Besides the fact that managed services increase business agility and make experimentation easier and cheaper, it also contributes to more sustainable workloads: AWS has a strong driver to increase the efficiency of their services to be able to handle more workloads on equal or less hardware. So, designing and building your workloads in AWS using managed services automatically contributes to a more sustainable architecture. This is nicely pointed out in the shared responsibility model of the sustainability pillar in the well-architected framework.\nZero ETL\nIn this keynote, Adam Selipsky promised a \u2018zero ETL\u2019 future. ETL stands for \u2018Extract, Transform, and Load\u2019 and is a data pipeline that is applied to almost all data warehouse ingestion flows. This seems a somewhat technical announcement, but don\u2019t be mistaken: Data pipelines are subject to require a lot of compute, let alone the labor to design and implement them, to get the data into shape before adding them into the destination data warehouse. So, a zero ETL future would be great. This re:Invent a new integration was announced that should be the premise of a zero ETL future: A managed way to ingest data from Aurora into Redshift, without the need for a separate data pipeline to setup and maintain.\nWhen you listen carefully to the keynotes of this reinvent, one can discover that a lot of efforts have been made to let people think that AWS is boarding the sustainability train, full throttle. It seems that everything they develop has somehow to do with improved efficiency, doing more with equal or fewer resources, and higher-level abstractions in the form of managed services.\nAs far as we are concerned this is a strategy AWS may double down on since it will be a win-win outcome.\nTake advantage of the future\nTo stay relevant is getting more and more important for businesses now that the cloud seems to be maturing and cloud vendors like AWS are starting to pivot into developing verticals. Before you know it AWS has taken over your business because it demoted your core business to commodity. There is no denying anymore that the cloud is hype and will pass when you look at the economic size of the cloud platform. Rather sooner than later It will look like AWS will surpass the $100 billion in revenue mark this year. So, get on that train if you haven\u2019t done so, and learn to leverage everything the cloud has to offer to speed up your business innovation to stay relevant and bring value to your customers! And if so, we\u2019d love to help!\n", "tags": [], "categories": ["Blog", "Cloud", "News"]}
{"post_id": 38138, "title": "Klantgericht werken (in een grote organisatie), hoe doe je dat?", "url": "https://www.luminis.eu/blog/klantgericht-werken-in-een-grote-organisatie-hoe-doe-je-dat/", "updated_at": "2023-09-04T14:33:56", "body": "In de media lees je met enige regelmaat over wat er niet goed gaat bij dienstverlening van grote overheidsinstanties zoals de Belastingdienst. Vanuit mijn rol als Digitaal Strateeg richt ik me op de dienstverlening richting burgers en bedrijven, en de digitale producten die daarbij komen kijken.\nHet klantgericht werken of centraal zetten (sorry, beroepsdeformatie) van de eindgebruiker kan en moet beter. Dat laatste komt voort uit mijn eigen ervaring en blijf ik me voor in zetten. Waarvoor precies? Het ontwerpen en leveren van betere diensten en producten waar eindgebruikers makkelijk en met vertrouwen hun zaken met de Belastingdienst kunnen regelen. Die dus.\nMijn ervaring\nAls Digitaal Strateeg werk ik vanuit Luminis op dit moment voor de Belastingdienst. De omschrijving van de opdracht: heeft het voor IV (Informatie Voorziening, het ICT bedrijf van de Belastingdienst) waarde als we Design Thinking in ons proces toepassen? Bij Design Thinking werk je met een multidisciplinair team aan innovaties waarbij de eindgebruiker centraal staat. Geen dikke documenten vanuit een ivoren toren maar echt een eindgebruiker betrekken en samen met deze eindgebruiker, business en IV oplossingen bedenken. Creatief oplossen van uitdagingen met nadruk op prototypen en zo snel mogelijk testen.\n3 pijlers van SAFe\nNaast het aanbieden van een cursus Design Thinking wordt inmiddels ook SAFe binnen de organisatie uitgerold. Dit framework stelt je in staat om op grote schaal agile te werken. SAFe richt zich op 3 belangrijk pijlers: voorspelbaar, wendbaar \u00e8n klantgericht. Top! Customer Centricity en Design Thinking hebben een plek binnen het framework en \u2013 hoewel SAFe naar mijn mening snel neigt naar de \u2018release\u2019 kant \u2013 ben ik met de uitspraak zeker blij. Het centraal zetten van een eindgebruiker en empathie wordt gezien, en daarmee ook het belang om te kijken naar deze eindgebruiker en zijn uitdagingen. Ik gebruik bewust het woord \u201cuitdagingen\u201d omdat ik in allerlei projecten (ook bij Luminis en haar opdrachtgevers) te vaak meemaak dat de oplossing al bedacht is en er gebouwd wordt zonder eerst goed te kijken wat en voor wie er iets opgelost moet worden. Daar hebben we binnen onze Agile Release Train (ART in SAFe: een groep van Agile-teams dat stapsgewijs een of meer oplossingen in een waarde stroom ontwikkelt, levert en exploiteert) ervaring mee opgedaan. Ook steeds meer buiten deze ART trouwens. Er is dus beweging, ik zie dat we de goede kant op gaan. In deze blog deel ik de belangrijkste ervaringen die ik de afgelopen jaren (bij de Belastingdienst) op heb gedaan.\nIn vogelvlucht\nMaar eerst even terugkijken\u2026 Tot zo\u2019n 15 jaar geleden bedachten digitaal productontwerpers, na een briefing door opdrachtgevens, wat een eindgebruiker voor product wilt hebben. Ik hoorde vaak:\n\u201cJij bent de creatieveling, ik heb een idee maar ik kan dat niet \u2018vertalen\u2019, verzin jij het maar!\u201d Waarbij ik dan altijd voelde: \u201cga aan de slag ontwerper, bedenk iets slims en kom je hok uit als je het doordacht en ontworpen hebt. Ps. Zorg wel dat het te maken is\u2026\u201d.\nHoewel met deze methode ook vandaag de dag nog steeds prima producten ontworpen worden, werkt het voor mij tegenwoordig gelukkig anders. Daar probeer ik ook altijd op te sturen. Waarom? Omdat ik denk dat je samen betere producten kunt maken. Eindgebruikers en opdrachtgevers betrekken waardoor de producten in mijn ogen beter en dus succesvoller zijn.\nDe designcyclus\nMet de ontwikkeling en toepassing van Design Thinking (sinds 1970, en in 1990 bij een grotere groep bekend) en Service Design staan eindgebruiker en samenwerken met opdrachtgevers steeds centraler. Empathie (inleven in de eindgebruiker en zijn behoeftes) is iets wat in deze beide methodes in verschillende vormen terugkomt. Een designcyclus start met Empathy en eindigt via Define, Ideate en Prototype bij Test. Voor elke fase hebben zijn er dan weer allerlei werkvormen die je kunt gebruiken om tot inzichten te komen. Op basis van een uitdaging kijk je bij een designcyclus eerst in de breedte (divergeren) om vast te leggen waarvoor je gaat ontwerpen (convergeren) om vervolgens hetzelfde te doen bij het bedenken van idee\u00ebn en het maken en testen van een prototype.\n\n* Divergeren en convergeren in de \u2018double diamond\u2019 die gebruikt wordt om het design thinking proces te visualiseren.\n** Empathy heb ik begrijpen genoemd. Wat mij betreft gaat het in deze fase om behoeftes, beperkingen en data.\nVeel opdrachtgevers verwachten inmiddels dat er een proces gebruikt wordt waarin omschreven staat hoe en wanneer eindgebruikers gesproken worden en wie (van de opdrachtgever) we hierbij betrekken. Dat had ik 15 jaar geleden niet kunnen bedenken. Want op dat moment gaf ik zelf aan dat ik graag eindgebruikers wilde interviewen en via een (usability) test feedback over het product op wilde halen. Daarbij heb ik meerdere keren het volgende te horen gekregen:\n\u201cJij bent toch de ontwerper en dan weet je wat goed is, daar huur ik je toch voor in? Hoezo dan nog testen met eindgebruikers? Daarbij, dat is ook duur en verstoort/vertraagt het voortbrengingsproces. We willen nu gaan bouwen!\u201d\nMijn argument \u201cmaar wat nou als ik mis zit en we iets opnieuw moeten bouwen, dat kost toch meer tijd en geld?!\u201d veranderde helaas weinig aan de mening van opdrachtgevers. In die zin is niet de hele wereld veranderd, we werken namelijk nog steeds voor opdrachtgevers die het lastig en spannend vinden om eindgebruikers te betrekken en onderzoek duur vinden. In mijn ogen klopt mijn argument overigens nog steeds, misschien eens onderbouwen met cijfers?\nWaarom eindgebruikers betrekken?\nDe pitch (een gedeelte van) die we bij Luminis gebruiken om ons verhaal uit te leggen\u2026\nDigitale producten die er toe doen!\nLuminis maakt voor haar opdrachtgevers en eindgebruikers digitale producten die er toe doen. We ontwerpen en maken digitale producten waar eindgebruikers blij van worden en onze opdrachtgevers succesvol. Hierbij willen we vanuit business, gebruikers en technologie de volgende vragen beantwoorden in ons werk om de juiste digitale producten te kunnen maken:\n\n\n\nWat is wenselijk vanuit het perspectief van de gebruikers?\nWat is waardevol vanuit het perspectief van de opdrachtgever?\nWat is mogelijk vanuit het perspectief van technologie?\n\n\n*3 perspectieven op productontwikkeling om succesvolle producten te maken.\nBegin bij de eindgebruiker\nAls je een product wilt maken waar eindgebruikers blij van worden, zul je bij hen moeten beginnen. Vandaar de volgorde wenselijk, waardevol en mogelijk. Over blij valt in de context van een overheidsinstantie nog wel wat te zeggen. Vaak heeft de eindgebruiker het maar te doen met dit ene product en is er geen keuze. Daarom is een eindgebruiker wellicht eerder tevreden door soepel en makkelijk proces. Zonder gedoe en lang wachten aan de telefoon\u2026 dat je weet waar je aan toe bent en wat je moet doen om compliant te zijn.\nEn dan die eindgebruiker, wat wil die (echt)? Door je in te leven kom je daar achter, dit is eerder benoemd als empathie. Een collega schreef:\n\u201cAls professionele ontwerpers weten we dat het essentieel is om in de schoenen van onze doelgroep te kruipen om de wereld te kunnen beleven zoals zij het ervaren.\u201d\nHelemaal mee eens. En ik weet heus wel dat er meer nodig is dan alleen het resultaat uit een gebruikersonderzoek (waar op wordt gehint) om een overtuigend keuze te kunnen maken om een uitspraak te kunnen doen over de impact en het (verwachte) effect van een concept. Het aspect \u201cmogelijk\u201d bepaalt natuurlijk veel, maar realiseer je: als je weet wat een gebruiker wilt, hoef je dat niet pers\u00e9 (in 1 keer) te maken. Het maken van een ontwerp zie ik dan ook als richtinggevend en in gesprek aan te passen. Maar wel met een eindgebruiker in gedachte.\nEen antwoord geven op de vraag \u201cWat wilt een eindgebruiker echt?\u201d is voor onze opdrachtgevers vaak lastig. Ook voor mij als ontwerper trouwens. Dat heeft meerdere redenen want een eindgebruiker:\n\nzal je al snel willen \u2018pleasen\u2019 en wenselijke antwoorden geven;\nweet vaak niet precies wat hij wil;\nzegt wat hij wil maar wat hij wil zal niet helemaal overeenkomen met de doelen die je (stiekem) al in je achterhoofd hebt.\n\nNaast het feit dat het voor opdrachtgevers daarmee maar de vraag is hoe betrouwbaar zo\u2019n gesprek met een eindgebruiker is ontstaat er ook vaak weerstand om met een eindgebruiker in gesprek te gaan want opdrachtgevers:\n\nweten niet hoe en wanneer ze klanten kunnen betrekken om inzicht te krijgen;\ndenken dat het lang duurt en duur is om met eindgebruikers onderzoek te doen;\ndenken dat het hun eigen doelen schaadt als ze (te)veel naar eindgebruikers luisteren.\n\nHoe vind je een eindgebruiker?\nVoor het geval je niet weet hoe je een eindgebruiker kunt betrekken, de simpelste stap is rondvragen. Vrienden, familie etc. zijn in mijn ervaring snel bereid te helpen. Door een paar vragen te stellen krijg je snel reactie op je idee. Ook bestaande klanten kun je benaderen en er is niks leerzamer (en leuker) dan naar ze toe te gaan. Gewoon bellen, bijna iedereen zegt \u2018ja\u2019. De leukste en meest leerzame gesprekken en testen heb ik in de steenfabriek, de garage of op de boot uitgevoerd.\nJe kunt het werven van respondenten ook uitbesteden, dan gaat het je misschien meer kosten maar vaak heb je dan wel eerder een meer diverse groep waar je mee kunt testen. En het is natuurlijk gemakkelijk iemand te laten werven op basis van een opgesteld profiel. De test wordt serieuzer en \u2013 hoewel het een veel gehoorde opmerking dat je niet uit kunt gaan van een enkele testen \u2013 5 tot 6 respondenten is echt genoeg voor de eerste inzichten die je op wilt halen. Gebruik kwalitatief onderzoek als je iets wilt begrijpen (concepten, gedachten of ervaringen).\nOver punt 2 kan ik zeggen: je kunt het zo klein en zo groot maken als je wilt (van 1 uur tot meerdere weken) maar als je door 1 uur te besteden de inzichten ophaalt dat het anders, moet heb je snel je geld verdiend. Over punt 3, ach \u2013 je bent er zelf bij, je hoeft heus niet (alles) te maken wat een klant wil.\nAls je kijkt naar de verschillende frameworks, waar SAFe er eentje van is, zou je kunnen zeggen dat het mooi is dat Customer Centricity en Design Thinking genoemd wordt maar in mijn ervaring ben je er dan nog niet. Ook omdat deze frameworks niet beschrijven hoe je dit soort, voor de meeste mensen, ongrijpbare dingen toe kunt passen. Een veelgehoorde opmerking in dit verband: \u201d\nIk doe dat al, ik denk toch al vanuit de klant?!\u201d Stel dan de tegenvraag: wanneer heb je die voor het laatst gesproken? Het antwoord? \u201cJa nee, dat niet maar ik weet het wel wat goed voor ze is\u2026\u201d\nDat is niet wat ik met klantgericht werken bedoel.\nIn mijn ervaring kun je nog zoveel over de theorie vertellen maar is het vooral een kwestie van doen. Bijvoorbeeld deelnemers van workshops laten ervaren wat dat klantgericht werken is. Bij het uitleggen van Design Thinking heb ik een aantal keer meegemaakt dat ik het licht in de ogen van mensen zag doven. Terwijl als je een workshop doet en daar eindklanten bij betrekt (zonder precies je beweegredenen uit te leggen) worden medewerkers enthousiast en geven ze aan dat \u2018we\u2019 \u2018dit\u2019 vaker zouden moeten doen. Ha, een stapje gemaakt! Dan nog steeds, hoe pas je dat nou in de praktijk toe?\nKlantgericht werken (in een grote organisatie), hoe doe je dat?\nIn deze blog is het al subtiel aangestipt: dat is best lastig. De belangrijkste oorzaak hiervan is dat het een organisatie niet zomaar lukt om van de ene op de andere dag om anders te gaan werken en er medewerkers met weerstand overtuigd moeten worden. Een werkwijze aanpassen lukt niet van de ene op de andere dag, daarvoor heb je een lange adem nodig. Als je klantgericht wilt gaan werken, realiseer je dan dat je hier zomaar eens langer dan twee jaar mee bezig kunt zijn voordat je op een niveau bent gekomen dat je klantgericht(er) werkt.\nAls je klantgericht wilt gaan werken is het handig een aantal manieren of methodes te hebben waar klantgericht werken in zit. De belangrijkste workshopvormen die (met multidisciplinaire teams) in mijn ervaring werken:\n\n1-daagse workshop\nDesign sprints (zie de whitepaper Design Sprints die ik geschreven heb)\nService design trajecten\nKlantreis trajecten\n\nIn mijn ogen betekent een klant centraal stellen dat je op verschillende momenten met die klant moet gaan praten. In alle bovenstaande werkvormen zit het ingebakken dat je dit doet. Het zit dus in de werkwijze met als voordeel dat iedereen het na verloop van tijd normaal gaat vinden. Dan kun je wanneer je bijvoorbeeld een bestaand product wilt verbeteren de volgende (open) vragen aan een eindgebruiker stellen:\n\nWelk onderzoek heb je voordat je het product ging gebruiken gedaan?\nWat kun je me over dit formulier vertellen? Hoe heb je dit formulier ingevuld?\nWaar liep je tegenaan tijdens het invullen?\nWat wordt er met deze regeling gevraagd?\nHoe heb je dat opgelost?\n\nBij de projecten waar we interviews afnemen proberen we zoveel mogelijk stakeholders mee te laten luisteren. Omdat het voor een businessowner, domeinexpert, architect, etc. net zo goed is om te weten wat er bij een klant speelt. Dit is geen ontwerp feestje. Scheelt voor de designer ook weer tijd om anderen van het juiste product of dienst te overtuigen.\nDe genoemde werkvormen kun je inzetten om een oplossing voor kleine en grote uitdagingen uit te werken. In het geval van SAFe: als je elk increment (5 tot 6 sprints van twee weken binnen een kwartaal) een design sprint rond een belangrijke (klantgerichte) feature doet, ben je al klantgericht met je team aan het werk. In een design sprint praat je aan de start met eindgebruikers en test je aan het eind ook met eindgebruikers. En meteen realiseren zodat je kunt meten en naar klantwaarde kunt kijken. Het is wellicht eng om succes te meten (heb ik wel de juiste investering gedaan?) maar hoe eerder je test des te eerder je je product aan kunt gaan passen en meer (klant)waarde kunt leveren.\nTot slot\nIk denk dat klantgericht werken voor elke organisatie iets anders betekent. De markt, tijd, budget, teamsamenstelling, mindset, etc. allemaal factoren die van invloed zijn. Een vastomlijnd plan ligt er dus niet maar denk aan de volgende takeaways:\nAls je meer klantgericht wilt gaan werken, denk dan aan:\n\nhet betrekken van eindgebruikers;\nhet gebruiken van (design thinking) werkvormen waarmee je multidisciplinair werkt en teamleden zich in de gebruiker in kunnen leveren;\nhet testen met eindgebruikers.\n\nzodat:\n\nhet juiste probleem op wordt gelost;\nhet voor eindgebruikers makkelijker wordt om in 1 keer goed hun zaken kunnen regelen (met als bijkomend voordeel minder belasting van supportmedewerkers);\nrework achteraf voorkomen wordt (1st time right);\nje blije gebruikers hebt, klantwaarde levert en succesvoller met je product bent!\n\n\n\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 38095, "title": "Data Quality Series, part 1: Introduction to Data Quality", "url": "https://www.luminis.eu/blog/introduction-to-data-quality/", "updated_at": "2023-08-13T12:11:33", "body": "We\u2019ve all heard the phrase \u201cgarbage in, garbage out\u201d which highlights the importance of quality data for data-driven systems. Here, quality data can be interpreted in two ways: firstly as clean and well-standardized data that meets expectations, and secondly, as well-thought-out data that fits a particular business case. Although the latter is typically determined during the research or data strategy phase, in this blog we will focus on the former interpretation.\nMotivation\nMaintaining high-quality data is critical for accurate data analysis, decision-making, and achieving business objectives. Real-world data is often noisy and subject to constant changes, which makes maintaining data quality a challenging task. Therefore, it\u2019s crucial to identify data quality issues early on and address them before they have any effects on downstream analytics tasks or decision making processes. One of the responsibilities of Data and MLOps Engineers is to ensure that quality is maintained throughout its lifecycle.\nMeasuring Data Quality\nTo ensure data quality throughout the data pipeline, it\u2019s important to measure and test it at different stages. A typical testing workflow in data engineering would involve several types of tests:\n\nUnit testing: focuses on testing separate components of your pipeline in isolation. For example, testing whether (a part of) an SQL or a Spark script does what it is supposed to do.\nFunctional Testing: includes data flow validation such as transformation logic validation based on business rules, as well as data integrity, which validates data based on constraints and schema checks. This type of testing occurs frequently at different stages of the pipeline (think ingestion, processing, and storage).\nIntegration Testing: ensures that the data pipeline meets the business requirements. Generally, this is done by running fake data through the pipeline and validating the result.\n\nAlthough we have covered different types of tests, it\u2019s worth noting that traditional software engineering practices only cover these points to a certain extent. For this reason, let\u2019s focus on functional testing of data and take a look at testable properties attributed to quality data.\n\nCompleteness: checks whether all expected data is present and accounted for in the data pipeline. Simple checks may test for null values, while more complex checks may also condition based on value or other columns.\nUniqueness: verifies if there are no duplicate records in the data pipeline. Duplicate records can cause issues with aggregation and analysis, leading to incorrect results.\nDistribution: focuses on closely examining the validity of column values. This may involve checks to ensure that the data falls within an accepted range or that the units used in a given column are consistent.\nValidity: enforces known invariants that should always hold true, regardless of the input data. These may be defined based on data standards or business rules. For example, the price column may never be negative, or the total column should equal to the sum of pre-tax subtotal and tax amount.\nAccuracy: measures the level to which data reflects the real world by using a verifiable source. For example, a customer phone number can be validated.\nIntegrity: takes into account relationships of data with other systems within an organization. It involves limiting changes or additions to the data that may break connections and generate orphaned records.\nConsistency: ensures that the data is accurate, and aligned with the organization\u2019s attributes. By having consistent attribute values on can building relationships between different data systems, prevent data duplication, and inconsistencies.\n\nData Observability\nHaving proper testing mechanisms set up is only the first step, as it is only natural that the data keeps evolving and may change unexpectedly. This aspect of data is not easy to tame, since you don\u2019t always have control over the source systems. That\u2019s why it\u2019s crucial to continuously test and monitor data quality, which is where Data Observability comes into play.\nThe five pillars of data observability[1] provide good guidance criteria you would want to include in your testing and\nmonitoring.\n\nFreshness: Does the asset use the most recent data? This is a critical aspect of data quality, as outdated or stale data can lead to incorrect decisions being made. Depending on the use case, you may need to validate that the data is fresh within a certain time window, such as the past hour or day.\nQuality: The quality checks are vital in verifying the quality of data, as they ensure that the data is in the correct format and within acceptable limits. These checks are useful for ensuring that the transformation pipeline can handle the input data, as well as, validating the output data, as is commonly applied when writing data contracts[2].\nVolume: Did all the data arrive? How many rows were changed? Did the dataset decrease in size? These are important questions to answer when monitoring the volume of your data pipeline. Sudden spikes or drops in volume could indicate issues with the pipeline or changes in the underlying data sources.\nSchema: The schema of a dataset defines the structure and type of each field in the data. It is often used in contracts between producers and consumers of the data. Especially when working with raw data sources, schema validation checks can help catch issues such as missing or incorrectly formatted fields, and ensure that any changes to the schema are properly communicated to downstream consumers.\nLineage: Lineage refers to the record of the origins, movement, and transformation of data throughout the data It can answer questions about upstream sources that the data depends on and downstream assets that would be impacted by any change. The data lineage is a critical component during compliance auditing and root cause analysis.\n\nLineage of Continent Statistics table visualized in Dagster\nYou can not test for everything, and as things inevitably break, you may be unknowingly making decisions on bad data. There is an exponential relation between lost revenue and how far down the line data issues are diagnosed.\nWhen you write tests for your data, you are testing for \u201cknown unknowns\u201d. The next step you can take is testing for \u201cunknown unknowns\u201d, which on contrary are not apparent during the creation of the data systems. Detecting these issues is typically done through health checks and anomaly detection on collected metrics through simple thresholding or forecasting-based methods.\nMonitoring the percentage of faulty rows or checking whether the number of days since the last update does not exceed the historical average duration are good examples of proxy measures that can detect \u201cunknown unknowns\u201d.\nAnomaly Detection on Row Count quality metric in Soda Cloud [3]Performing data profiling by defining rule-based sets of metrics to be computed on all columns within your dataset can give you a good starting point when writing tests. Some data processing tools like OpenRefine and AWS DataBrew have built-in data profiling to aid in building cleaning transformations. Similarly, it can also be a powerful tool when combined with anomaly detection for building automated monitoring systems.\nData Profiles in AWS DataBrew [4]Presenting the data profiles, quality information, and schema as part of a dashboard or data catalog can provide a lot of value for your business[5]. Similarly, setting the right governance structure where the issues and alerts reach the appropriate team is an important aspect of maintaining high data reliability.\nFor additional guidelines on improving data reliability, consider reviewing AWS Well-Architected Data Analytics Lens [6].\nData Quality Score Cards in Monte Carlo\u2019s Data Reliability Dashboard [7]When it comes to designing reliable data systems, it\u2019s essential to handle errors gracefully both during data quality testing and transformation. Depending on your environment, there are various testing approaches you can take. A common first concern is determining when and where to run data quality checks.\nMany cases such as ETL pipelines prefer on-demand execution where the quality of raw data is evaluated at the source or the destination after loading the data. This approach ensures that the transformation step can handle the data before actual processing is applied. Both approaches have their benefits; testing before load requires query access to the source database and may put excessive load on the application database, while loading data beforehand may result in additional latency.\nSimilarly, scheduled execution periodically tests data quality in source tables and reports if any issues arise. This approach is typically found in data warehouse solutions, where transformation is postponed until query evaluation using views.\nA notable benefit of on-demand execution is that one can immediately act on it. As such, the circuit breaker pattern is utilized to break off pipeline execution if (batch) data does not pass the error checks or an anomaly is detected. The tradeoff is that the rest of the system keeps using stale or partial data until the issue is resolved.\nTo expand on this methodology, data quarantining is another related pattern that defines a flow where faulty data is set aside. The quarantined data can be used to fix the issues and reprocessed at a later date to ensure that no data loss occurs. This approach works particularly well for incremental processing pipelines or pipelines without idempotency property (i.e., processing data multiple times results in a different dataset).\nSelf-healing pipelines combine none or multiple of the mentioned properties to gracefully recover from failure. This may be as simple as retrying data submission, reprocessing the full dataset, or waiting until prerequisite data is in the system.\nChoosing Your Tools\nWe evaluated several open-source data quality tools (aside from AWS Glue) to use in our ETL pipelines. Our evaluation criteria included features, integrations, and compatibility with our existing pipeline architecture.\nGreat Expectations (GX): is the tool of choice for many data workloads. It has a large collection of community-made checks and a large collection of features. Supported integrations include some common data tools, cloud analytics (including Amazon Athena, AWS Glue, and AWS Redshift), and pandas dataframes.\n\nCodified data contracts and data docs generation\nData profiling\nThe Quality metrics are limited to what checks calculate.\nOn-demand execution\n\nAWS Deequ: is an open-source library built by AWS that covers a wide range of data quality needs. Deequ is based on the concept of data quarantining and has the functionality to filter out and store bad data at various stages of the process.\nThe tool is built in Scala on top of Apache Spark, but it has a Python bindings library which unfortunately lags quite far behind. If you don\u2019t use these tools in your stack, you will find them of limited use.\n\nAnomaly detection\nSchema checks\nData profiling\nQuality metrics calculation and merging\nOn-demand execution\n\nAWS Glue Data Quality Rules: Recently, AWS introduced a variety of data quality tools as part of their serverless computing platform Glue. The tool itself uses Deequ under the hood and provides excellent interoperability with the rest of AWS stack, such as AWS CloudWatch and result storage.\nAs of writing this article, the functionality is still in public beta, does not offer a way to store quality metric results for anomaly detection nor has a way to run the checks outside AWS glue environment (closed source). Similarly, many of the features included in deequ are not yet supported, such as quality metrics calculation or custom checks.\n\nConfiguration-based tests\nWell integrated with AWS infrastructure\n\nSoda Core: is a modern SQL-first data quality and observability tool. Similar to GX it includes a wide range of integrations. While Soda Core by itself is only for collecting metrics, a full-fledged data observability platform in form of Soda Cloud (proprietary) is provided with automatic monitoring of data quality results, data contracts, and anomaly detection.\n\nWide range of integrations\nSimple configuration\nSchema checks\nQuality measure calculation\n\nDBT Unit Tests: comes as part of the DBT which is an SQL-first tool for managing and modeling your data in data warehouses. The integrations are not limited to data sources, but also other data quality tools. The tool itself is meant for unit testing and therefore runs separately from the data flow.\n\nCustom metric calculation.\nCommunity support (resources, plugins, and integrations).\n\nApache Griffin: As a complete data quality platform, it provides an integrated dashboard for data quality analysis, and monitoring data quality over time. The quality testing runs are conducted within the tool but separate from the data flow. The integrations are limited to the Apache Stack (Kafka, Spark, Hive), and a select few other tools.\n\nStreaming data processing support\nDashboard for data quality analysis\nAnomaly detection\n\nAll listed tools have their use cases and as such there is no clear winner. For simple ETL workloads, you might want to try Deequ. In a data warehouse setting, dbt in combination with Soda or GX might prove useful. When working in a data science setting or with streaming data, GX and Apache Griffin respectively might be good choices. If your infrastructure runs on AWS, it\u2019s worth keeping an eye on developments in their Glue-based data quality tools.\nConclusion\nIn conclusion, maintaining high-quality data is essential for accurate data analysis, decision-making, and achieving business objectives. Data quality testing is a huge part of the testing process for data systems, and there are many options on how this can be implemented. In this blog, we have covered a few fundamentals, which I hope give you a starting point for exploring more on the topic and applying it in your projects. Stay tuned for part two, where we will use deequ for data quality testing within an ETL pipeline on AWS.\nCitations\n1. What Is Data Observability? 5 Key Pillars To Know In 2023\n2. Fine, let\u2019s talk about data contracts \u2013 by Benn Stancil\n3. Time Series Anomaly Detection with Soda | Soda Data\n4. AWS Launches Visual Data Prep Tool\n5. Build a data quality score card using AWS Glue DataBrew, Amazon Athena, and Amazon QuickSight | AWS Big Data Blog\n6. Data Analytics Lens 1 \u2013 Monitor the health of the analytics application workload\n7. Announcing Monte Carlo\u2019s Data Reliability Dashboard, A Better Way Understand The Health Of Your Data\n", "tags": [], "categories": ["Blog", "Data"]}
{"post_id": 37946, "title": "The Amplify Series, Part 7: Track app usage with Amplify Analytics", "url": "https://www.luminis.eu/blog/the-amplify-series-part-7-track-app-usage-with-amplify-analytics/", "updated_at": "2023-05-08T12:55:21", "body": "In the previous part of this series, we added the Amplify Predictions category, which allowed us to use the power of AI and Machine learning to identify text in images, convert text to speech and interpret sentiment of the text. In this part of the blog series, we will add functionality to be able to track the usage of our application in order to learn how our users use the application and be able to improve the application user experience based on this information. We will do this by adding the Amplify category called Amplify Analytics.\nWe will start by adding this category to the project. We will create a new page for the Amplify Analytics category and add some interactable elements to that page so that we can track the usage of these elements. We will also be updating the previous pages when needed in order to track the usage of those pages. In the end, you will have a better understanding of this category and be able to track the usage of your Amplify application.\nWe will continue in the repository where we left off in the previous blog post.\nAdding the Analytics category\nWe will start by generating the backend resources needed for the tracking functionality.\u00a0\nWe will run amplify add analytics with the following options:\n\nAnalytics provider: Amazon Pinpoint\nResource name: <<use default>>\nUnauthenticated users: No\n\n\r\n? Select an Analytics provider Amazon Pinpoint\r\n\u2714 Provide your pinpoint resource name: \u00b7 theamplifyapp\r\n\u26a0\ufe0f Auth configuration is required to allow unauthenticated users, but it is not configured properly.\r\n\u26a0\ufe0f Adding analytics would add the Auth category to the project if not already added.\r\n? Apps need authorization to send analytics events. Do you want to allow guests and unauthenticated users to send analytics events? (we recommend you a\r\nllow this when getting started) No\r\n\u26a0\ufe0f Authorize only authenticated users to send analytics events. Use \"amplify update auth\" to modify this behavior.\r\n\u2705 Successfully updated auth resource locally.\r\n\u2705 Successfully added resource theamplifyapp locally\r\n\nThis command will add a new directory called analytics to our\u00a0 amplify/backend directory, which will contain information needed to create the resources in AWS to support the new functionality.\nWe will run amplify push to create these resources in AWS. The AWS service used here are Amazon Pinpoint.\u00a0\nNote that using this service will cost you money. The first 100 million events recorded per month are free. After that, you pay $0.000001 per event you collect. A piece of general advice:\u00a0 configure a budget with billing alerts for your AWS account so you don\u2019t get surprised by high costs.\u00a0\nAfter running the amplify push command you will not be charged since the application must be configured to send events to AWS Pinpoint. Once we configure this, however, the application will automatically start sending a minimal set of events such as page visits. This can be turned off and will be covered in the next section.\nOnce we have created the resources, we can run amplify console analytics to open the Amazon Pinpoint console. Initially, all charts should be empty:\n\nIn the next step, we will configure the frontend application to start sending data to Amazon Pinpoint to fill these graphs.\nConfiguring the frontend for Analytics\nTo start, we will update our main.ts to include the following:\nimport { Predictions, Analytics } from 'aws-amplify';\nBy simply importing this dependency,running the application again and visiting it in the browser, we can see that the charts are being updated:\n\nWe can also click on the demographics to see information about the users:\n\nWith this small addition to the frontend application, we can now see how much the application is being used and from what type of devices. If you want to have more information about the users of the application, it is possible to customize the information sent about a user. We will not cover that part in this blog, however, a complete example can be found on this page.\nSession tracking\nThe session metric in AWS Pinpoint provides information about how often your application has been opened. However, once a session is registered for a device (mobile, browser, etc.) the session counter does not go up if the same device visits the site again within a certain time period. Refreshing the page or closing the browser and opening it again is not a new session. This is because the Amplify SDK creates a unique identifier for your device. \nAfter the page has not been active on the device for some time or if your application manually calls Analytics.stopSession the session will terminate. Opening the page again after this will register as a new session.\nThe session is tracked by default, however, you can turn it off by adding the following code in the main.ts:\nAnalytics.autoTrack('session', {\r\n\u00a0 \u00a0 enable: false\r\n});\nPage view tracking\nWe can also configure the application to track which pages are visited. We will update the main.ts with the following:\nAnalytics.autoTrack('pageView', {\r\n\u00a0 enable: true,\r\n\u00a0 type: 'SPA',\r\n\u00a0 attributes: {\r\n\u00a0 \u00a0 url: window.location.origin + window.location.pathname\r\n\u00a0 },\r\n})\nNow you can run your application and visit some of the pages. If you visit the events page in AWS Pinpoint you can see how much each page is viewed:\n\nDisclaimer: It can take up to 30 minutes for the events to show up in Pinpoint.\nRecording events\nWe can record events such as clicks on the pages with Amplify Analytics. This section describes two ways to do that.\nManual event recording\nThe first way is to do it manually. We will update our text-to-speech.component.ts to send an event to AWS Pinpoint every time the user uses the text-to-speech functionality. The event will contain the text that the user entered:\n// Other imports\r\nimport { Predictions, Analytics } from 'aws-amplify';\u00a0 // <--- Add import\r\n\r\n@Component({\r\n\u00a0 selector: 'app-text-to-speech',\r\n\u00a0 templateUrl: './text-to-speech.component.html',\r\n\u00a0 styleUrls: ['./text-to-speech.component.css']\r\n})\r\nexport class TextToSpeechComponent implements OnInit {\r\n\u00a0 // Other existing code\r\n\r\n\u00a0 convertToAudio = async () => {\r\n\u00a0 \u00a0 if (!this.textInput) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 // Add this \r\n\u00a0 \u00a0 Analytics.record({\r\n\u00a0 \u00a0 \u00a0 name: 'convertedTextToSpeech',\r\n\u00a0 \u00a0 \u00a0 attributes: { textInput: this.textInput },\r\n\u00a0 \u00a0 \u00a0 immediate: true\r\n\u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 // Other existing code\r\n\u00a0 };\r\n\r\n\u00a0 // Other existing code\r\n}\nThis will now show up in our events every time a user uses this functionality:\n\nIn a real application you might not want to set an attribute value as we did here for an input field. Since the users of the application are free to enter any value they want, the attribute value list in the filters will explode. However, this example gives an impression of what is possible with the minimal setup we have so far.\nAutomatic event recording\nThe other way to record events is to set up an event tracker that will activate when an element contains certain properties. We will update the main.ts to include the following:\nAnalytics.autoTrack('event', {\r\n\u00a0 enable: true,\r\n\u00a0 events: ['click'],\r\n\u00a0 selectorPrefix: 'data-amplify-analytics-'\r\n});\nThis will track all click events for HTML elements that have properties with the prefix data-amplify-analytics-.\nWe will now update our comments.component.html to make use of this so that an event is sent to AWS Pinpoint every time someone adds a comment to the post. \n\u00a0\u00a0\u00a0\u00a0<!-- Other existing code -->\r\n\r\n\u00a0 \u00a0 <div class=\"col-lg-4\">\r\n\u00a0 \u00a0 \u00a0 <button\r\n\u00a0 \u00a0 \u00a0 \u00a0 class=\"aws-button\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 data-amplify-analytics-on=\"click\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 data-amplify-analytics-name=\"commentAdded\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 (click)=\"addComment()\"\r\n\u00a0 \u00a0 \u00a0 >\r\n\u00a0 \u00a0 \u00a0 \u00a0 Add Comment\r\n\u00a0 \u00a0 \u00a0 </button>\r\n\u00a0 \u00a0 </div>\r\n\u00a0 <!-- Other existing code -->\nThis event will also show up similar to the previous one in our events dashboard:\n\nWe now have enough tools to be able to track the usage in our application.\nThe following code changes have been made to our repository in this article:\n\nAdding the backend resource\nUpdating the frontend\n\nAmplify Analytics vs Google Analytics\nWhen deciding to track app usage, you can also use Google Analytics, which is unarguably more feature-rich and streamlined than Amplify Analytics.\u00a0\nAmplify Analytics uses AWS Pinpoint, which has analytics capabilities but is more focused on user interaction, such as sending emails and notifications for campaigns.\u00a0\nGoogle Analytics is the industry standard when it comes to Analytics as it provides many features and is very user-friendly compared to AWS Pinpoint in my opinion. So how do you choose between the two?\nMy advice would be to choose AWS Pinpoint if you want to get a quick view of the usage of your application. If you already have an Amplify application, you can get up and running with analytics with just a few changes to your application as we have seen in this blog post.\u00a0\nIf you want to go further than seeing the general usage, event usage, and demographics, then you should consider using Google Analytics, especially if you or someone on your team already has experience with it.\u00a0\nUp next: Location functionality with Amplify Geo\nIn this blog we have used Amplify Analytics to track the app usage of our application. As with the previous categories, there is more that you can do with this category than what is covered in this article, so be sure to check the documentation for more information. In the next article, we will take a look at how to use Amplify Geo to add location-based functionality.\n", "tags": ["amplify", "angular", "aws", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 37914, "title": "Introduction of Frank and Amy at HaystackConf USA", "url": "https://www.luminis.eu/blog/introduction-of-frank-and-amy-at-haystackconf-usa/", "updated_at": "2023-05-02T15:04:11", "body": "Last week, at the end of April 2023, I attended the HaystackConf in Charlottesville, USA. HaystackConf is the conference to participate in if you are a search relevance engineer. With over a hundred people attending on-site and more than a hundred online, this is a good representation of the almost 4000 members at the time of writing in the search relevance community on Slack.\nAfter presenting at the online HaystackConf due to covid, this was the first time I joined the conference as a speaker. This was also my first speaker experience outside of Europe, and what a great experience it has been. This is the conference where I introduced Frank and Amy to the big public. This blog post will take you with me on my journey to the USA.\nSunday morning, we took the plane to Washington, DC. We, because I traveled with my Buddy and old colleague Byron Voorbach from Weaviate. We rented a car to drive to Charlottesville. After arrival, we did a walk through town, ate a hamburger, and took a drink.\nMonday was Jetlag day. Besides a bit of work, we explored the environment. We went to a park to shoot some pictures, and in the evening, we attended a local meetup at the Center for Open Science: Pre-Haystack US Meetup at Center for Open Science. The meetup had two talks.\nMatt Clark on COS, whose mission is to increase openness, integrity, and reproducibility of research, and their SCORE project: Systematizing Confidence in Open Research and Evidence.\nRen\u00e9 Kriegler, Director, E-commerce Search at Open Source Connections: A path to understanding and adopting evolving technologies in search.\nThe evening was a good warm-up for the conference\u2014excellent discussions about the relevance of Vector databases and ChatGPT for the search relevance domain.\nLego at the park\nTuesday, Conference Day 1\nCharlie Hull welcomed everybody on-site as well as online. Next was the keynote by Trey Grainger. Always a good speaker. I liked his overview of approaching search before and moving from Sparse to Dense information retrieval. It was clear that vector search is here; having a sparse search alone is not the future. Is hybrid search just a step from sparse to dense because dense still has some issues, or is hybrid the future? Will large language models be the silver bullet? The one clear thing is that search and information retrieval are going through an exciting time.\nThe next talk I attended has a summary in its title: \u201cLearning to hybrid search: combining BM25, neural embeddings and customer behavior into an ultimate ranking ensemble\u201d. Roman Grebennikov presented his thoughts on the current hype around neural search. Roman is the primary author of the project Metarank. An interesting project if you want to use learning to rank.\nThe next talk was by Karel Bergmann: \u201cCreating Representative Query Sets for Offline Evaluation\u201d. He explained how they used offline evaluation at Getty Images. Interesting to see the scale at which they use these evaluations to optimize searching for images.\nLunch at HaystackConf is going into town and selecting a restaurant. Everybody can find something they like with the number of choices available in town. After lunch, I visited Ohad Levi. His talk is titled: \u201cBreaking Search Performance Limits with Domain-Specific Computing\u201d. He discussed using domain-specific hardware to improve search performance with a factor of 100. Impressive numbers that work well with specific instances of AWS machines.\nFrank and Amy talk about search relevance.\nThe following presentation was my own. The title of my talk is \u201cTop 8 search topics to teach your team members\u201d. I introduced the main characters of my presentation, Frank and Amy. During the talk, I used Frank, the search relevance expert, to explain search features to Amy, a seasoned Python developer. It was fun for me to do the presentation. I got good feedback from the audience. They liked the presentation with all the drawings and recognized the topics. Not all of them explained these topics to their team members, which was a good lesson for them.\nAfter my talk, lightning talks followed by a reception and dinner at the Kardinal Hall. Good beer, food, and a game of Bocce. A relaxed atmosphere to talk about search topics and get to know each other.\nNot everybody returns to the hotel after dinner. Charlottesville has a lot of bars to offer, and drinks are diverse, as are the people from Charlottesville.\nAfter a satisfying day, time to go to bed.\n\u00a0\nWednesday, Conference Day 2\nThe second day of the conference started off with an Ask me anything session with the authors of the book \u201cAI-Powered Search\u201c. Participants were Trey Grainger, Doug Turnbull, and Max Irwin. Most of the exciting questions were again about vectors and Large Language Models\u2019 role in search. These three guys together have an incredible amount of knowledge in the search area. If you like this topic, you should read their book.\nNext, I attended the talk by Jay Flack titled \u201cEnterprise Search Relevance at Box: Simplicity\u201d. You should not be surprised this talk is about the experience they gained at Box to supply a search feature over all the documents they have. Security and speed are very important as the number of documents is enormous. They used Solr with an exterior ranking model built with Tensorflow running on AWS Sagemaker.\nLunchtime again, and waiting for the talk by Erika Cardenas. Erika is a colleague of Byron; this is her first talk in public. Yes, she was a bit nervous. But you step into a cinema room and present a keynote plus a short presentation about the women in search. Erika\u2019s talk was about \u201cBuilding Recommendation Systems with Vector Search.\u201d Erika did a great job. Of course, she used Weaviate for the demo application. An interesting talk about the ref2vec module for Weaviate. Go try it out if you are interested.\nNext up for me was the presentation from Chris Morley. His talk is titled \u201cPopulating and leveraging semantic knowledge graphs to supercharge search.\u201c\u00a0He was funny and had some interesting ideas, but it did not work for me. I did not really understand the point he was making. He mentioned it was more of a try-out for him; it needs some polishing if you ask me. The content is there, but the story is harder to follow.\nThe last presentation was from Colin Harman. The title of his talk is Stop \u201cHallucinations and Half-Truths in Generative Search\u201d. I really liked his talk. He showed examples of where ChatGPT produces wrong answers. His examples using simple calculations in your question to break the results were interesting. One example where he asked ChatGPT only to consider a provided document to answer a question was wrong. Also, an example about medicine, where the answer varied the code for the medicine a bit and could even give a dangerous answer. Great closing talk for me.\nThe closing event for the conference was a dinner sponsored by Weaviate. I sat next to Erika and Colin. Had really good conversations. It was an inspiring evening again.\nJettro on stage\nThat is where the conference ended, but not the learning. The following two days, we had fun driving to Washington through the mountains and visiting Washington. We also took time to work on a sample application using Weaviate and OpenAI. Using a cross-encoder as a re-ranker for a hybrid search in Weaviate. Feeling more confident now working with Weaviate.\nCall me if you are reading this and want to know more about search, vector databases, and Large Language Models. We can help you create AI-powered search solutions that help your user become effective searchers.\nStay tuned for announcements if you want to experience HaystackConf, but live in Europe and feel the USA is too far away. HaystackConf is coming to Europe again this year. Maybe to the Netherlands.\n", "tags": ["chatgpt", "haystackconf", "weaviate"], "categories": ["Blog", "Search"]}
{"post_id": 37425, "title": "Improve AWS security and compliance with cdk-nag", "url": "https://www.luminis.eu/blog/cloud-en/improve-aws-security-and-compliance-with-cdk-nag/", "updated_at": "2023-04-26T09:26:45", "body": "AWS Cloud Development Kit (AWS CDK) is a powerful tool that allows developers to define cloud infrastructure in code using familiar programming languages like TypeScript, Python, and Java.\nHowever, as with any infrastructure-as-code tool, it\u2019s important to ensure that the resulting infrastructure adheres to security and compliance best practices. This is where cdk-nag comes in.\nWhat is cdk-nag ?\ncdk-nag is an open-source tool that provides automated checks for AWS CDK code and the resulting Cloudformation templates to help ensure that they adhere to security and compliance best practices.\nAfter adding cdk-nag to your project it checks for a variety of known security and compliance issues including overly-permissive IAM policies, missing access logs and unintended public s3 buckets. cdk-nag also checks for common mistakes that can lead to security vulnerabilities, such as the use of plain text passwords and the use of default security groups.\nThe great thing about cdk-nag is that it allows you to catch mistakes at a very early stage in the process. Ideally, you can catch them while developing your infrastructure as code in CDK on your local machine. As an alternative, you can add cdk-nag to your CI/CD pipeline and make the build fail in case of any issues.\nAdding cdk-nag to your project\nUsing cdk-nag is simple. First, add it as a dependency to your AWS CDK project. If you\u2019re using Java you can add it to your pom.xml file.\n<dependency>\r\n  <groupId>io.github.cdklabs</groupId>\r\n  <artifactId>cdknag</artifactId>\r\n  <version>2.25.2</version>\r\n</dependency>\r\n\nAfter you\u2019ve added the dependency you will need to explicitly enable cdk-nag utilizing a CDK aspect. You can apply cdk-nag in the scope of your entire CDK application or just in the scope of a single CDK stack.\ncdk-nag works with rules which are defined in packs. Those packs are based on AWS Config conformance pack. If you\u2019ve never looked at AWS Config, the Operational Best Practices for HIPAA Security page is a nice page to look at in the context of these cdk-nag conformance packs. By default, cdk-nag comes with several rule packs out of the box.\n\nAWS Solutions\nHIPAA Security\nNIST 800-53 rev 4\nNIST 800-53 rev 5\nPCI DSS 3.2.1\n\nBased on your requirements you can enable one or more rule packs. Let\u2019s take a look at how to apply such a rule pack.\npublic class AwsCdkNagDemoApp {\r\n    public static void main(final String[] args) {\r\n        App app = new App();\r\n\r\n        new AwsCdkNagDemoStack(app, \"AwsCdkNagDemoStack\", \r\n            StackProps\r\n                .builder()\r\n                .env(Environment.builder()\r\n                .account(System.getenv(\"CDK_DEFAULT_ACCOUNT\"))\r\n                .region(System.getenv(\"CDK_DEFAULT_REGION\"))\r\n                .build())\r\n            .build()\r\n        );\r\n\r\n         Aspects.of(app)\r\n           .add(\r\n                AwsSolutionsChecks.Builder\r\n                .create()\r\n                .verbose(true)\r\n                .build()\r\n           );\r\n        app.synth();\r\n    }\r\n}\nAs you can see in the above code fragment we\u2019ve enabled the AwsSolutionsChecks rules for the scope of the entire CDK app. In this example, we\u2019ve explicitly enabled verbose mode as it will generate more descriptive messages.\nNow let\u2019s take a look at an example stack and see how cdk-nag responds to that. The stack below is a very simple stack which contains an AWS Lambda function processing messages from an SQS queue.\npublic AwsCdkNagDemoStack(final Construct scope, \r\n  final String id, final StackProps props) {\r\n      \r\n  super(scope, id, props);\r\n\r\n  final Queue queue = Queue.Builder.create(this, \"demo-queue\")\r\n                 .visibilityTimeout(Duration.seconds(300))\r\n                 .build();\r\n\r\n  final Function function = Function.Builder\r\n    .create(this, \"demo-function\")\r\n    .handler(\"com.jeroenreijn.demo.aws.cdknag.FunctionHandler\")\r\n    .code(Code.fromAsset(\"function.jar\"))\r\n    .runtime(Runtime.JAVA_11)\r\n    .events(List.of(\r\n      SqsEventSource.Builder.create(queue).build())\r\n    )\r\n    .build();\r\n\r\n  queue.grantConsumeMessages(function);\r\n}\r\n\nAnalyzing results\nNow when you run cdk synth from the command-line, it will trigger cdk-nag and it will automatically scan your resources in the resulting templates and check them for security and compliance issues. Once the scan is done, cdk-nag will either return successfully or return an error message and output a list of violations in a format that is easy to understand. After running cdk synth we will get the following messages in our output.\n[Error at /AwsCdkNagDemoStack/demo-queue/Resource] AwsSolutions-SQS3: The SQS queue is not used as a dead-letter queue (DLQ) and does not have a DLQ enabled. Using a DLQ helps maintain the queue flow and avoid losing data by detecting and mitigating failures and service disruptions on time.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-queue/Resource] AwsSolutions-SQS4: The SQS queue does not require requests to use SSL. Without HTTPS (TLS), a network-based attacker can eavesdrop on network traffic or manipulate it, using an attack such as man-in-the-middle. Allow only encrypted connections over HTTPS (TLS) using the aws:SecureTransport condition in the queue policy to force requests to use SSL.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-function/ServiceRole/Resource] AwsSolutions-IAM4[Policy::arn::iam::aws:policy/service-role/AWSLambdaBasicExecutionRole]: The IAM user, role, or group uses AWS managed policies. An AWS managed policy is a standalone policy that is created and administered by AWS. Currently, many AWS managed policies do not restrict resource scope. Replace AWS managed policies with system specific (customer) managed policies.This is a granular rule that returns individual findings that can be suppressed with 'appliesTo'. The findings are in the format 'Policy::' for AWS managed policies. Example: appliesTo: ['Policy::arn::iam::aws:policy/foo'].\r\n\r\n\r\nFound errors\r\n\n\u00a0\nAs you can see cdk-nag spotted some errors and explains what we can do to improve our infrastructure. Usually, it\u2019s quite easy to fix these errors. Level 2 CDK constructs already incorporate some of the best practices, so when using them you will probably find fewer errors compared to using Level 1 constructs.\nThe messages depend on the rule pack you select. For instance, when we switch to the HIPAASecurityChecks rule pack we will get some duplicates but also some additional error messages.\n[Error at /AwsCdkNagDemoStack/demo-function/Resource] HIPAA.Security-LambdaConcurrency: The Lambda function is not configured with function-level concurrent execution limits - (Control ID: 164.312(b)). Ensure that a Lambda function's concurrency high and low limits are established. This can assist in baselining the number of requests that your function is serving at any given time.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-function/Resource] HIPAA.Security-LambdaDLQ: The Lambda function is not configured with a dead-letter configuration - (Control ID: 164.312(b)). Notify the appropriate personnel through Amazon Simple Queue Service (Amazon SQS) or Amazon Simple Notification Service (Amazon SNS) when a function has failed.\r\n\r\n[Error at /AwsCdkNagDemoStack/demo-function/Resource] HIPAA.Security-LambdaInsideVPC: The Lambda function is not VPC enabled - (Control IDs: 164.308(a)(3)(i), 164.308(a)(4)(ii)(A), 164.308(a)(4)(ii)(C), 164.312(a)(1), 164.312(e)(1)). Because of their logical isolation, domains that reside within an Amazon VPC have an extra layer of security when compared to domains that use public endpoints.\r\n\r\n...\r\n\n\u00a0\nThe HIPAASecurityChecks also finds issues related to Lambda function concurrency and running your Lambda function inside a VPC. As you can see different packs look at different things, so it\u2019s worthwhile to explore the different packs and see how they can help you improve. It\u2019s worth mentioning that cdk-nag does not implement all rules defined in these AWS Config conformance packs. You can check which rules are excluded in the cdk-nag excluded rules documentation.\nSummary\nOverall, cdk-nag is a powerful tool for ensuring that your AWS CDK code and templates adhere to security and compliance best practices. By catching security issues early in the development process, cdk-nag can help you build more secure and reliable infrastructure. I\u2019ve used it in many projects over the last couple of years and it\u2019s adding value. Especially if you work in a team that does not have a lot of AWS experience it shines. If you\u2019re using AWS CDK, I highly recommend giving cdk-nag a try. The example code in this post and a working project can be found on GitHub.\n", "tags": ["aws", "aws cdk", "cloud security", "compliance"], "categories": ["Blog", "Cloud", "Development", "Security"]}
{"post_id": 37344, "title": "Ronald Voets new Managing Director Luminis", "url": "https://www.luminis.eu/blog/ronald-voets-new-managing-director-luminis/", "updated_at": "2023-03-30T09:36:05", "body": "M80 accelerates international ambitions with the appointment of Ronald Voets as new Managing Director Luminis\nBrussels, March 30, 2023 \u2013 To further fulfill M80\u2019s ambitions of becoming a best-in-class international player in the field of digital transformation, Ronald Voets will join the team in the role of Managing Director Luminis as of April 1, 2023.\nDutch technology company Luminis, which provides solutions and services in the Cloud and data field, has been part of M80\u2019s digital transformation platform since 2022 with the aim to accelerate its focus abroad. With Ronald Voets\u2019 previous experience at Visma, Raet, PinkRoccade and Exact, he has the strategic capabilities to shape Luminis\u2019 future professionalization and internationalization.\nHans Bossenbroek, Group CEO:\n\u201cWith Ronald we bring on board highly relevant knowledge and experience, which will contribute to our ambitions. I am confident that Ronald will take Luminis to the next level with a customer and employee centric approach.\u201d\nRonald Voets, Managing Director Luminis:\n\u201cM80 aims to create a portfolio of best-in-class specialists in the field of digital transformation. A European IT group at Champions League level. That appeals to me enormously and I look forward to taking Luminis and the software development within the rest of our portfolio to the next level to realize the international ambitions. Together I want to build a future in which our customers are central and Luminis remains an attractive employer for talent.\u201d\nAbout M80\n\nM80 Partners is the management company of M80 Capital, a private equity fund founded in 2018 that invests in companies in Belgium, France, the Netherlands, and Luxembourg.\nThe investment team consists of seasoned private equity professionals, as well as entrepreneurs, former CEOs, and digital pioneers.\nThe company focuses on growth companies in healthcare, consumer, business services and manufacturing. The M80 team invests in companies it can help digitally transform to accelerate revenue and improve operations.\nM80\u2019s digital transformation platform currently includes XPLUS, Luminis, BPSOLUTIONS and Total Design.\nMore information: https://m80partners.com/\n\nAbout Luminis\n\nFounded in 2002, Luminis provides customers with high-quality Cloud and Data solutions. Luminis has partnerships with Amazon Web Services (AWS) and Microsoft, among others.\nIn addition, Luminis is initiator and powerhouse of IT training programme Accelerate with Bosch and the Dutch Tax Authority, among others.\nLuminis has 150 employees and offices in Amersfoort, Amsterdam, Rotterdam, Arnhem, and Apeldoorn and provides its services to, for example, Thales, Alliander, Huuskes, BDR Thermea, bol.com and The Learning Network.\nMore information: https://www.luminis.eu\n\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 37210, "title": "The Amplify Series, Part 6: Using the power of AI and Machine Learning with Amplify Predictions", "url": "https://www.luminis.eu/blog/the-amplify-series-part-6-using-the-power-of-ai-and-machine-learning-with-amplify-predictions/", "updated_at": "2023-05-08T12:58:26", "body": "In the last part of this series, we added the Amplify Storage category, which allowed us to retrieve and upload files to S3 directly from our application. In this part of the blog series, we will add even more functionality, such as identifying text from an image, converting text to speech, and interpreting the sentiment of text. We will do this by adding a new Amplify category called Amplify Predictions which will allow us to use the power of AI and machine learning to access powerful functionality out of the box.\nAs usual, we will start by adding the category to the project. We will then create a new page for the Amplify Predictions category and add sections to that page per functionality available. This page will serve as a showcase for all the possibilities within Amplify Predictions. In the end, you will have a better understanding of this category and be able to appreciate how easy it is to get this functionality working in your applications.\nAmplify app repository update\nWe will continue in the repository where we left off in the last blog post.\nThere have been 2 extra commits to this repository since the last blog post. These are:\n\nChanging the href to routerLink in the header so that the application behaves as a Single Page Application\nUpgrading Amplify CLI to the newest version (10.8.1)\u00a0 as of the point of writing\n\nNow we are ready to get started and add more functionality to our application.\nIdentify text from uploaded image\nIn this section, we will add functionality that will allow us to upload an image and retrieve the identified text inside the image.\nGenerating the backend resources\nWe will run amplify add predictions with the following options:\n\nCategory: identify\nWhat would you like to identify: Identify Text\nFriendly name: <<use default>>\nIdentify documents: yes\nAccess: Auth users only\n\nThe Amplify CLI output will look similar to this:\n\r\nEvertsons-MacBook-Pro:theamplifyapp evertsoncroes$ amplify add predictions\r\n\u2714 Please select from one of the categories below \u00b7 Identify\r\n\u2714 What would you like to identify? \u00b7 Identify Text\r\n\u2714 Provide a friendly name for your resource \u00b7 identifyTextd230b04a\r\n\u2714 Would you also like to identify documents? (y/N) \u00b7 yes\r\n\u2714 Who should have access? \u00b7 Auth users only\r\nSuccessfully added resource identifyTextd230b04a locally\r\n\nAs with the previous category, we want only to allow access to authenticated users. This command will add a new directory called predictions to our amplify/backend directory, which will contain information needed to create the resources in AWS to support the new functionality.\nBug: Before continuing, we need to add some manual changes to the generated output since there is a bug in the 10.8.1 version of the Amplify CLI. To fix the issue, open the amplify/backend/predictions/identifyText<<id>>/parameters.json file and add the following three key-value pairs to it:\n\n\u201cformat\u201d: \u201cPLAIN\u201d\n\u201caccess\u201d: \u201cauth\u201d\n\u201cidentifyDoc\u201d: \u201cdocument\u201d\n\nWe will run amplify push to create these resources in AWS. The AWS services used here are Amazon Rekognition for image recognition and Amazon Textract for document analysis.\u00a0\nNote that using these services will cost money. Amazon Rekognition will cost around $0.001 per image processed, while Amazon Textract costs around $0.0015 per document processed. As a general rule, be sure to set up a budget with billing alerts for your AWS account so you don\u2019t get surprised by large costs.\u00a0\nThese commands will add the following changes to our repository.\nAdding the predictions page\nThe first thing we need to do is add a new component to our frontend application that will contain all the predictions functionality:\nng generate component components/categories/predictions\nThis will generate the expected files for our component. We will then link up routing to be able to render this component. Refer to this commit for the details so far.\u00a0\nAdding Text identification functionality to the Predictions page\nWe will add a new frontend component that will contain the functionality to upload an image and identify the text in that image:\nng generate component components/categories/predictions/identify-text\nInside our predictions.component.html we must make sure to add the newly generated identify-text component:\n<app-identify-text></app-identify-text>\nInside the identify-text.component.html, we will add:\n<input\r\n\u00a0 type=\"file\"\r\n\u00a0 id=\"imageUpload\"\r\n\u00a0 name=\"imageUpload\"\r\n\u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 (change)=\"imageSelected($event)\"\r\n/>\nSimilar to the last blog, this will give us an input that we can use to select images from our device. We have to add logic to react to the image that is selected in our identify-text.component.ts:\n\r\nimport { Component, OnInit } from '@angular/core';\r\n\r\n@Component({\r\n  selector: 'app-identify-text',\r\n  templateUrl: './identify-text.component.html',\r\n  styleUrls: ['./identify-text.component.css']\r\n})\r\nexport class IdentifyTextComponent implements OnInit {\r\n  selectedFile: File | undefined = undefined;\r\n\r\n  constructor() {}\r\n\r\n  ngOnInit(): void {}\r\n\r\n  imageSelected = (e: Event) => {\r\n    const input = e.target as HTMLInputElement;\r\n\r\n    if (!input.files?.length) {\r\n      return;\r\n    }\r\n\r\n    this.selectedFile = input.files[0];\r\n  };\r\n}\r\n\nWe will now be able to select a file and it will be stored in the selectedFile variable. Normally it would be best practice to add this logic to a separate component so that we can reuse this code. However, to keep the blog shorter we will allow duplicate code.\nNow that we have the functionality to upload an image, we need to add a button that does something with the selected image. We will also show the identified text and also add some CSS to identify-text.component.html:\n<div class=\"container-fluid card-background\">\r\n\u00a0 <h2>Identify Text</h2>\r\n\u00a0 <input\r\n\u00a0 \u00a0 type=\"file\"\r\n\u00a0 \u00a0 id=\"imageUpload\"\r\n\u00a0 \u00a0 name=\"imageUpload\"\r\n\u00a0 \u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 \u00a0 (change)=\"imageSelected($event)\"\r\n\u00a0 />\r\n\r\n\u00a0 <button class=\"aws-button\" (click)=\"identifyText()\">\r\n\u00a0 \u00a0 Identify Text\r\n\u00a0 </button>\r\n\u00a0 <div class=\"identified-text\" *ngIf=\"identifiedText\">\r\n\u00a0 \u00a0 Identified words:\r\n\u00a0 \u00a0 <div *ngFor=\"let word of identifiedText.text.words\">\r\n\u00a0 \u00a0 \u00a0 {{ word.text }}\r\n\u00a0 \u00a0 </div>\r\n\u00a0 </div>\r\n</div>\nOnce this button is clicked, the identifyText function is called. We will define this function in the following way in our identify-text.component.ts:\n\r\nimport { Component, OnInit } from '@angular/core';\r\nimport { Predictions } from 'aws-amplify'; // <---NEW\r\nimport { IdentifyTextOutput } from '@aws-amplify/predictions'; // <---NEW\r\n\r\n@Component({\r\n  selector: 'app-identify-text',\r\n  templateUrl: './identify-text.component.html',\r\n  styleUrls: ['./identify-text.component.css']\r\n})\r\nexport class IdentifyTextComponent implements OnInit {\r\n  selectedFile: File | undefined = undefined; // <---NEW\r\n  identifiedText: IdentifyTextOutput | undefined = undefined; // <---NEW constructor() {} ngOnInit(): void {} identifyText = async () => {\r\n    if (!this.selectedFile) {\r\n      return;\r\n    }\r\n\r\n    //ADD THIS FUNCTION\r\n    Predictions.identify(\r\n      {\r\n        text: {\r\n          source: {\r\n            file: this.selectedFile\r\n          }\r\n        }\r\n      },\r\n      {}\r\n    )\r\n      .then(response => (this.identifiedText = response))\r\n      .catch(err => console.log({ err }));\r\n  };\r\n\r\n  //OTHER CODE\r\n}\r\n\nIn these changes we import some components we need related to Amplify Predictions. We add two properties to our components, the selectedFile which will hold the latest uploaded image and the identifiedText which will hold the latest results of identified text we received from AWS. We will then call the identify function with the selected image to be sent to AWS. The response will be set to the identifiedText field and the words will show up on the screen.\nThere is one more step we need to take. The Predictions component needs to be supplied with a Provider. This can be done in the main.ts file:\n\r\n#other imports\r\n\r\nimport { Predictions } from 'aws-amplify';\r\nimport { AmazonAIPredictionsProvider } from '@aws-amplify/predictions';\r\nAmplify.configure(aws_exports);\r\nPredictions.addPluggable(new AmazonAIPredictionsProvider());\r\n\r\n#other code\r\n\nOnce this is all done, we can run our application, upload an image, identify the text and see the results on the screen. When I used this\u00a0image: \n\nI got the following result:\n\nThere are more options to play around with, including identifying entities and labels in images and many ways to finetune the results. For more information on this, checkout the Amplify Predictions documentation.\nThe changes for this section can be found in this commit, including the changes needed for the CSS.\nConvert text to speech\nIn this section we are going to add functionality to convert text to speech using Amplify Predictions.\nGenerating the backend resources\nWe will first generate the backend resources needed. We will run amplify add predictions with the following options:\n\nCategory: Convert\nWhat to convert: Generate speech audio from text\nFriendly name: <<use default>>\nSource language: US English\nSpeaker: Kevin \u2013 Male\nAccess: Auth users only\n\nThe Amplify CLI output will look similar to this:\n\r\nEvertsons-MacBook-Pro:theamplifyapp evertsoncroes$ amplify add predictions\r\n\u2714 Please select from one of the categories below \u00b7 Convert\r\n\u2714 What would you like to convert? \u00b7 Generate speech audio from text\r\n\u2714 Provide a friendly name for your resource \u00b7 speechGenerator11c4cfca\r\n? What is the source language? ...  (Use arrow keys or type to filter)\r\n\u2714 What is the source language? \u00b7 US English\r\n\u2714 Select a speaker \u00b7 Kevin - Male\r\n\u2714 Who should have access? \u00b7 Auth users only\r\nSuccessfully added resource speechGenerator11c4cfca locally\r\n\nThis is very similar to what we previously did for the text identification. We can run amplify push again to create the resources in AWS. The AWS service that will be used for this functionality is Amazon Polly. The costs for using Amazon Polly is around $4.00 per 1 million characters.\u00a0\nThese commands will add the following changes to our repository. \nAdding text-to-speech functionality to the Predictions page\nSimilar as we did for the text-identification, we will add a component that will handle all of the text-to-speech functionality:\nng generate component components/categories/predictions/text-to-speech\nInside our predictions.component.html we will make sure to add the newly generated text-to-speech component:\n<app-text-to-speech></app-text-to-speech>\nInside the text-to-speech.component.html, we will add a text input and a button to play the text:\n<div class=\"container-fluid card-background\">\r\n\u00a0 <h2>Text to speech</h2>\r\n\u00a0 <input\r\n\u00a0 \u00a0 type=\"text\"\r\n\u00a0 \u00a0 id=\"textInput\"\r\n\u00a0 \u00a0 name=\"textInput\"\r\n\u00a0 \u00a0 (change)=\"textInputUpdated($event)\"\r\n\u00a0 />\r\n\r\n\u00a0 <button class=\"aws-button\" (click)=\"convertToAudio()\">\r\n\u00a0 \u00a0 Play\r\n\u00a0 </button>\r\n</div>\nNow we need to hook up these elements to our text-to-speech.component.ts and call the Predictions component to do the conversion from text to an audio buffer for us. Finally, we play the audio:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Predictions } from 'aws-amplify';\r\nimport { TextToSpeechOutput } from '@aws-amplify/predictions';\r\n\r\n@Component({\r\n  selector: 'app-text-to-speech',\r\n  templateUrl: './text-to-speech.component.html',\r\n  styleUrls: ['./text-to-speech.component.css']\r\n})\r\nexport class TextToSpeechComponent implements OnInit {\r\n  textInput: string | undefined = undefined;\r\n\r\n  constructor() {}\r\n\r\n  ngOnInit(): void {}\r\n\r\n  convertToAudio = async () => {\r\n    if (!this.textInput) {\r\n      return;\r\n    }\r\n\r\n    Predictions.convert({\r\n      textToSpeech: {\r\n        source: {\r\n          text: this.textInput\r\n        },\r\n        voiceId: 'Amy'\r\n      }\r\n    })\r\n      .then(async result => {\r\n        this.playAudio(result);\r\n      })\r\n      .catch(err => console.log({ err }));\r\n  };\r\n\r\n  playAudio = async (audio: TextToSpeechOutput) => {\r\n    const context = new AudioContext();\r\n    const buffer = await context.decodeAudioData(audio.audioStream);\r\n    const source = context.createBufferSource();\r\n    source.buffer = buffer;\r\n    source.connect(context.destination);\r\n    source.start();\r\n  };\r\n\r\n  textInputUpdated = (e: Event) => {\r\n    const input = e.target as HTMLInputElement;\r\n    this.textInput = input.value;\r\n  };\r\n}\r\n\nBug: There is currently, at the time of writing, a bug in Amplify that does not allow us to use the voiceId \u201cKevin\u201d, which we selected when creating the backend resources. Selecting the voiceId \u201cAmy\u201d works, so we will use that.\nIn the code snippet above we create a field that will hold the text in the textInput. We have a method that is called to convert the text to audio using the Predictions component. The convert function will send an http request to Amazon Polly and an audioBuffer will be returned. This can be given to an AudioContext component to play to audio in the browser.\n\nThe changes made in these steps can be found in this commit. \nInterpret the sentiment of text\nThe final set of functionality we are going to add is the ability to interpret the sentiment of text.\u00a0\nGenerating the backend resources\nWe will add the backend resources by again running amplify add predictions with the following options:\n\nCategory: Interpret\nFriendly name: <<use default>>\nKind of interpretation: ALL\nAccess: Auth users only\n\nThe Amplify CLI output will look similar to this:\nEvertsons-MacBook-Pro:theamplifyapp evertsoncroes$ amplify add predictions\r\n\u2714 Please select from one of the categories below \u00b7 Interpret\r\nOnly one option for [What would you like to interpret?]. Selecting [Interpret Text].\r\n\u2714 Provide a friendly name for your resource \u00b7 interpretText9001208a\r\n\u2714 What kind of interpretation would you like? \u00b7 All\r\n\u2714 Who should have access? \u00b7 Auth users only\r\nSuccessfully added resource interpretText9001208a locally\r\n\nAnd now we can run amplify push to create the resources in AWS. The AWS service that will be used for this functionality is Amazon Comprehend. The pricing for this service can be found here.\u00a0\nThese commands will add the following changes to our repository. \nAdding text-interpret functionality to the Predictions page\nWe will first create a component that will handle the text-interpret functionality:\nng generate component components/categories/predictions/text-interpret\nInside our predictions.component.html we will make sure to add the newly generated text-interpret component:\n<app-text-interpret></app-text-interpret>\nInside the text-interpret.component.html, we will add a text area input and a button to trigger the interpretation of the text and a text to show what the sentiment is:\n<div class=\"container-fluid card-background\">\r\n\u00a0 \u00a0 <h2>Interpret text</h2>\r\n\u00a0 \u00a0 <textarea\r\n\u00a0 \u00a0 \u00a0 \u00a0 id=\"textAreaInput\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 name=\"textAreaInput\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows=\"5\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 cols=\"66\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 (change)=\"textInputUpdated($event)\"\r\n\u00a0 \u00a0 ></textarea>\r\n\r\n\u00a0 \u00a0 <button class=\"aws-button interpret-button\" (click)=\"interpretText()\">\r\n\u00a0 \u00a0 \u00a0 \u00a0 Interpret\r\n\u00a0 \u00a0 </button>\r\n\u00a0 \u00a0 <div *ngIf=\"interpretation\">\r\n\u00a0 \u00a0 \u00a0 \u00a0 Interpretation = {{ interpretation.textInterpretation.sentiment?.predominant }}\r\n\u00a0 \u00a0 </div>\r\n</div>\nNow we will update the text-interpret.component.ts to hook up the functions defined here and call the Predictions component to interpret the text:\n\r\nimport { Component, OnInit } from '@angular/core';\r\nimport { Predictions } from 'aws-amplify';\r\nimport { InterpretTextCategories, InterpretTextOutput } from '@aws-amplify/predictions';\r\n\r\n@Component({\r\n  selector: 'app-text-interpret',\r\n  templateUrl: './text-interpret.component.html',\r\n  styleUrls: ['./text-interpret.component.css']\r\n})\r\nexport class TextInterpretComponent implements OnInit {\r\n  textInput: string | undefined = undefined;\r\n  interpretation: InterpretTextOutput | undefined = undefined;\r\n  constructor() { }\r\n\r\n  ngOnInit(): void {\r\n  }\r\n\r\n  interpretText = async () => {\r\n    if (!this.textInput) {\r\n      return;\r\n    }\r\n\r\n    Predictions.interpret({\r\n      text: {\r\n        source: {\r\n          text: this.textInput\r\n        },\r\n        type: InterpretTextCategories.ALL\r\n      }\r\n    }).then(result => this.interpretation= result)\r\n    .catch(err => console.log({err}))\r\n  }\r\n\r\n\r\n  textInputUpdated = (e: Event) => {\r\n    const input = e.target as HTMLInputElement;\r\n    this.textInput = input.value;\r\n  };\r\n}\r\n\nWe can now try our entering text in our text area and doing a sentiment check. I Googled \u201chappy poems\u201d\u00a0 and entered the first one I found:\n\nTry adding different types of text to check the interpretation. Furthermore, the response to the interpret function also contains more information related to the interpretation of the text. Check the documentation for more information and possibilities.\u00a0\nThe changes made in these steps can be found in this commit\nUp next: Tracking app usage with Amplify Analytics\nIn this blog we have used Amplify Predictions to identify text in images, convert text to speech and interpret the sentiment of text. There are more possibilities in this category, however, these examples should give you an idea. In the next article, we will look at using Amplify Analytics to collect analytics data for your application.\n", "tags": ["amplify", "angular", "aws", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 37069, "title": "Hosting a static react website on Amazon S3 with CDK", "url": "https://www.luminis.eu/blog/hosting-a-static-react-website-on-amazon-s3-with-cdk/", "updated_at": "2023-02-27T11:15:31", "body": "I recently migrated an existing website to the cloud and decided to share the process.\nThe cloud platform I used for this task was Amazon Web Services (AWS). In AWS there is a service called S3 (Simple Storage Service). S3 \u201cBuckets\u201d, which are similar to file folders, store objects, which consist of data and descriptive metadata. Why are we using S3 Buckets to host our website you ask? Because it is cheap!\nAWS charges customers for storing objects in a bucket and for transferring objects in and out of buckets, this is perfect for a static website that doesn\u2019t have to change much. The average price of hosting a website on an S3 bucket is $0.50 per month when using the free tier. When outside the free tier, it will cost up to $1-3 per month.\nSo hosting a website on Amazon S3 is a simple and cost-effective way to make your website available to the world. In this blog post, we\u2019ll discuss the steps of hosting a React website on Amazon S3 with the use of the AWS Cloud Development Kit (CDK), which is a software development framework to define cloud infrastructure as code.\nYou can check out the result here\nWhy use AWS CDK?\nThe main reason for using the AWS CDK is to enable developers to define and manage their cloud infrastructure using familiar programming languages and tools, rather than having to write templates in JSON or YAML. This provides several benefits, including improved efficiency, increased agility, better collaboration, and improved security. Overall, the AWS CDK provides a flexible, powerful, and efficient way to define, provision, and manage cloud infrastructure, helping organizations to be more productive and successful in their cloud computing efforts.\nRequirements\nThe requirements of the migrating assignment were:\n\nThe site redirects from HTTP to HTTPS\nusing a CloudFront distribution\nand ACM certificate\nmove the existing domain name to AWS\n\nLet\u2019s get started\nBefore you start, ensure you have an AWS account and a React app you want to host. The code uses the AWS Cloud Development Kit (CDK), which is a software development framework to define cloud infrastructure as code.\nWe start by importing the necessary constructs, services, and utilities from the AWS CDK library.\nimport { Construct } from \"constructs\";\r\nimport * as s3 from \"aws-cdk-lib/aws-s3\";\r\nimport * as iam from \"aws-cdk-lib/aws-iam\";\r\nimport * as cloudfront from \"aws-cdk-lib/aws-cloudfront\";\r\nimport { RemovalPolicy, Stack, StackProps } from \"aws-cdk-lib\";\r\nimport * as s3deploy from \"aws-cdk-lib/aws-s3-deployment\";\r\nimport { HttpMethods } from \"aws-cdk-lib/aws-s3\";\nThe StaticSite class extends the Stack class from the AWS CDK library, which is used to create a CloudFormation Stack. The class takes three parameters: the scope of the construct, the ID of the construct, and the properties of the Stack (in this case, StaticSiteProps).\nThe StaticSiteProps interface extends the StackProps interface and includes an additional parameter: cloudfrontCertArn. This parameter is used to configure the CloudFront distribution, which serves as the primary access point to the static site.\nexport interface StaticSiteProps extends StackProps {\r\n  cloudfrontCertArn?: string;\r\n}\r\n\r\nexport class StaticSite extends Stack {\r\n  constructor(scope: Construct, id: string, props: StaticSiteProps) {\r\n    super(scope, id, props);\r\n   // put your infrastructure here\r\n\r\n}\nIn the constructor, we create an S3 bucket to store the site files. The bucket is set up to prevent public access and has a removal policy of DESTROY, which means that the bucket and its contents will be deleted when the Stack is deleted.\nconst websiteBucket = new s3.Bucket(this, \"WebsiteBucket\", {\r\n  bucketName: `my-website-bucket`,\r\n  publicReadAccess: false, // no public access, user must access via cloudfront\r\n  removalPolicy: RemovalPolicy.DESTROY,\r\n  autoDeleteObjects: true,\r\n  cors: [\r\n    {\r\n      allowedHeaders: [\"*\"],\r\n      allowedMethods: [HttpMethods.GET],\r\n      allowedOrigins: [\"*\"],\r\n      exposedHeaders: [],\r\n    },\r\n  ],\r\n});\nThen we create an Origin Access Identity (OAI), which is a special CloudFront user that is used to grant access to the objects in the S3 bucket. The OAI is granted access to the objects in the S3 bucket through an IAM policy statement.\nconst identity = new cloudfront.OriginAccessIdentity(this, \"id\");\r\n\r\nwebsiteBucket.addToResourcePolicy(\r\n  new iam.PolicyStatement({\r\n    actions: [\"s3:GetObject\"],\r\n    resources: [websiteBucket.arnForObjects(\"*\")],\r\n    principals: [\r\n      new iam.CanonicalUserPrincipal(\r\n        identity.cloudFrontOriginAccessIdentityS3CanonicalUserId\r\n      ),\r\n    ],\r\n  })\r\n);\nNext, we create the CloudFront distribution, which serves as the primary access point to the static site. The CloudFront distribution is set up to redirect all traffic from HTTP to HTTPS and is configured to use the ACM certificate specified by the cloudfrontCertArn property. This ACM certificate had to be made manually before using it in the code.\nconst distribution = new cloudfront.CloudFrontWebDistribution(\r\n  this,\r\n  \"cloudfront\",\r\n  {\r\n    originConfigs: [\r\n      {\r\n        s3OriginSource: {\r\n          s3BucketSource: websiteBucket,\r\n          originAccessIdentity: identity,\r\n        },\r\n        behaviors: [\r\n          {\r\n            viewerProtocolPolicy:\r\n              cloudfront.ViewerProtocolPolicy.REDIRECT_TO_HTTPS,\r\n            allowedMethods: cloudfront.CloudFrontAllowedMethods.GET_HEAD,\r\n            compress: true,\r\n            isDefaultBehavior: true,\r\n          },\r\n        ],\r\n      },\r\n    ],\r\n    viewerCertificate: {\r\n      aliases: [\"search-matrix.luminis.amsterdam\"],\r\n      props: {\r\n        acmCertificateArn: props.cloudfrontCertArn,\r\n        sslSupportMethod: \"sni-only\",\r\n      },\r\n    },\r\n    defaultRootObject: \"index.html\",\r\n    errorConfigurations: [\r\n      {\r\n        errorCode: 403,\r\n        responseCode: 200,\r\n        responsePagePath: \"/index.html\",\r\n      },\r\n    ],\r\n  }\r\n);\nFinally, the code deploys the site files to the S3 bucket using the BucketDeployment class from the AWS CDK. The site files are sourced from the ./front-end/build directory and are deployed to the websiteBucket created earlier in the code.\nnew s3deploy.BucketDeployment(this, \"DeployWebsite\", {\r\n    sources: [s3deploy.Source.asset(\"./front-end/build\")],\r\n    destinationBucket: websiteBucket,\r\n    distribution,\r\n    });\r\n}\nConclusion\nIn conclusion, this code sets up a static site infrastructure in AWS, using the AWS CDK. The infrastructure includes an S3 bucket to store the site files, a CloudFront distribution to serve the site, and an IAM policy to grant access to the objects in the S3 bucket. The site files are deployed to the S3 bucket using the BucketDeployment class from the AWS CDK.\nAnd that\u2019s it! You have your website up and running in the cloud, but what if you wanted to make a change to your website? Do you have to do everything over again? No! This is where Continuous Integration and Deployment (CI/CD) comes into play. Setting up a CI/CD pipeline in AWS will be explained next time!\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 36999, "title": "7 lessons about success in digital product development", "url": "https://www.luminis.eu/blog/7-lessons-about-success-in-digital-product-development/", "updated_at": "2023-02-20T20:01:30", "body": "In my first years as a UX designer, about 15 years ago, there was one question I feared: \u201cCan you\u202fprove\u202fthat your design will actually result in a good user experience?\u201d I did not know how to prove that my work was a success yet. My instincts told me my work would smoothly guide users in whatever they needed from the product, but I could not hand over any hard evidence.\nThe best I could do was to test a prototype with users; a time-consuming undertaking that most of the time did not even deliver any hard evidence but would uncover flaws in my design that called for a next design iteration. To be honest: I would rather stay with my instinct telling me I had done a good job, than face reality and discover I should have done better. Because that was how it felt: whenever a design flaw came to light, I thought I had failed as a designer. And I soooo needed that pat on my back.\nI do not fear those questions anymore and I even started to ask them to myself. Because there is no shame, but a lot of value in uncovering design flaws. And time spent on testing and experimenting is not wasted when it saves you from building the wrong thing.\nLesson 1: There is no such thing as the perfect design\nEventually I learned that designing (and building) complex digital products is a process of learning just as much as creating. There is no such thing as the perfect design; there is only a solid process to continuously improve a product\u2019s user experience. And that actually applies to digital product development in general.\nQuestion: When you\u2019re in a company developing a digital product or service, when do you feel you are doing a good job? Is your success about:\n\nthe lines of code written,\nstory points accomplished,\nproduct features added,\ndeadlines met,\n\u2026?\n\nOr is it about:\n\nrevenue increased,\nusers time spared,\ncustomer satisfaction improved,\ncost of ownership lowered,\n\u2026?\n\nIf your answer falls into the first category, I hope you have been feeling successful lately. But even if you have, chances are that the product you\u2019ve been working on has not been successful at all. The first category are output parameters; metrics of what you have\u202fproduced. Not about the\u202fvalue\u202fthat was delivered by your work. Measuring output instead of outcome is not wrong, but can be misleading. I\u2019ve seen well-oiled teams successfully burn through a backlog at a dizzying pace, absolutely convinced they were on top of their game. Only to discover 6 months later that the problem they thought they were solving for their customers, was not a big issue for them at all. Great work, but no value.\nThe second category, which are the outcomes, can be measured as well, but not as easy to measure as the first category. They represent value delivered by your work on the product, for either the business, the customer or the user. And that is what you as a developer, designer or manager of digital products should be looking at to know whether you are successful: did your work result in the value as intended?\n\nLesson 2: Separate output from outcome\nIt is not wrong to keep track of your production with output parameters; it gives you solid insights in your development process and helps you and your team monitor, tweak and optimize. Output parameters do in fact impact outcome in many ways (just think about the number of bugs found). But meeting your target on output parameters does not guarantee a positive outcome for the business, customer or user. Though\u202fnot\u202fmeeting your target on your output mostly\u202fwill\u202fhave a negative effect on the outcome.\nBest is to have data on both output and outcome. But\u202fwhat\u202fshould you be measuring? On the output side the DORA metrics (deployment rate, lead time for changes, change failure rate and time to restore service) give a good indication of your product team\u2019s performance. But measuring success on the outcome side is different. To know what you should be measuring, you first need to know what you are trying to accomplish for the business, customer or user. And this is where things often get merky, because: why are we actually building this thing?\nLesson 3: Make the business everybody\u2019s business\nAt a lot of companies I encounter a disconnect between the business people and the people that are developing the product or service. I\u2019ve often seen one of these things happening:\n\na lack of vision and/or strategy on the business side, leaving it up to the development team (and not getting the right thing built);\na lack of communication between business and development, both being frustrated (and not getting the right thing built);\na lack of understanding between business and development, leading to a misinterpretation of the business\u2019 intent (and not getting the right thing built);\na lack of strategic focus, with the business prioritizing all things that come up and seem important (and not getting the right thing built);\n\nDeveloping a product or service ultimately serves a function for the customer and user, but also should add value for the company developing it. Product teams should know what success looks like for both the customers and users as well as for the company, and how their work contributes. How else could you expect them to make the right decisions?\nA lot has been written about how to bridge the gap between business and development. An excellent read on this is \u201cEscaping the Build Trap\u201d, by Melissa Perri. In her book she stresses the importance of aligning strategy throughout the company, in order to keep developing your product effectively towards delivering the intended value. She gives the example of having three different recurring meetings to review progress towards strategic intents and to make strategic decisions on product level:\n\nBusiness review: Financial outcomes like revenues and costs, progress towards strategic intents and how product initiatives are contributing to this progress (and adjust product strategy accordingly)\nProduct initiative review: Progress made on the initiatives and how experiments and options are contributing to the initiative (and adjust initiative strategy accordingly)\nRelease review: Functionality that will be shipped, success metrics and roadmap updates (so marketing, sales and executive teams are aware).\n\nNot all strategic decisions will be made in these meetings. But they do help to keep everybody in the company aligned strategically.\nLesson 4: Focus on value instead of features\nProducts are delivering value for the company by delivering value for the customers and users. So, before thinking of any features, product teams should be exploring and answering these questions:\n\nWhere is the value for your customers and users (fast delivery, better service, \u2026)\nWhere is the value for your business (increase revenue, reduce costs, \u2026\n\nYou want to measure this value in order to determine how successful your product is and know if your efforts are paying off. Vision and strategy on how the company can deliver this value should be shared throughout the company. Also with the product team, so that product goals and initiatives can be aligned with company goals and strategy. Strategy maps and roadmaps help, both at company as product level (I will cover that in another blog, so stay tuned).\nThere are many ways to measure the value of a product, it depends on the specific goals and objectives of the product which ones are the most useful. Some common examples of metrics used to measure the success of a product are:\n\nUser Engagement: Metrics such as active users, time spent on the product, and frequency of use can be used to measure how engaged users are with the product.\nConversion Rates: Measuring the number of users who convert from free to paid plans or the percentage of website visitors who become customers can help determine if the product is delivering value.\nCustomer Satisfaction: Feedback from users, such as Net Promoter Scores or surveys, can provide insights into how satisfied customers are with the product.\nBusiness Metrics: Revenue, profit margins, and other financial metrics can help determine if the product is delivering value to the business.\nUser Retention: Measuring the number of users who return to the product over time can help assess the stickiness of the product and how well it meets user needs.\n\nLesson 5: Ask yourself what is preventing you to deliver (more of) this value\nAfter settling what value(s) the product should deliver, you can define success indicators on your product and figure out the way to measure these. To get the moving numbers (like those mentioned in the examples above) in the right direction, you need to explore the problem or opportunity and discover solutions:\n\nWhat is preventing us to deliver (more of) this value?\nWhich of these problems should we be solving first?\nWhat should we measure to know the impact of a solution to this problem?\n\nLet\u2019s take an example: Customer satisfaction is one of our core values and we are using Net Promoter Scores to measure this. We are seeing that our scores are dropping, so before thinking of measures, we start by looking for the cause: what is causing this lower customer satisfaction? Because in this example our customer is also our user, we are conducting user research to find out. Our quantitative research shows a lower task completion rate since last quarter\u2019s release. Subsequent qualitative research then points out that many users don\u2019t understand the new filter feature that we added in that release.\nOnce we have targeted the problem, we can explore possible solutions:\n\nWhat could be our options to solve the problem?\nWhat are direct success indicators in our solution and how could we measure them?\nWhat option emerges as our best bet, after preliminary experiments?\n\nIn our example, options to solve the filter problem could be to remove the filter altogether, to improve the usability of the feature or to make filtering optional instead of mandatory. We run some A-B tests with these options and find out that task completion increases most when the filter is removed. But this filter was introduced after previous research pointed out that users wanted it, so removing it might not be our best option after all. The usability improvement of the feature (a short tutorial explaining the filter feature to the user) did also result in slightly higher task completion rates, but not as much as making the filtering optional instead of mandatory. This last option seems to be our best bet in our example.\nBy first discovering what is preventing us to deliver more value, and experimenting with multiple solution options, we can be confident that our selected solution will effectively solve a real problem and therefore will lead to value increase.\n\nLesson 6: Experiments are meant to learn first and to scale later\nRunning experiments might seem like a waste of resources, because it means throwing away the less-optimal solutions. But as you\u2019ve seen in our example, it also teaches us about the users\u2019 behavior and preferences, and it eliminates most of the risk of releasing the wrong thing. Still, we should keep the costs of these experiments as low as possible. Sometimes that means using paper prototypes or other ways to experiment with little or no coding. The most reliable experiments are the ones conducted in a live production environment though. Luckily, cloud technology enables us to run these live experiments more easily and to scale the right solution faster.\nTo know whether you really are successful after scaling the right solution, you need to keep measuring:\n\nInitiative-level success indicators, like Task Completion rates.\nProduct-level outcomes, like Customer Satisfaction;\nCompany-level results, like Revenue;\n\nResults at company level are so-called lagging success indicators; it will take some time to notice the effect of your product initiatives, and it provides only indirect evidence of your success; results at this level are affected by many things. Outcomes at product level are also lagging indicators, but provide more direct evidence for your success developing the product. Lastly, success indicators at initiative level are the most direct way of measuring success, because they show whether your solution is working.\n\nLesson 7: Not wrong long\nIt is kind of scary to measure success, because you might find your investment and work has not been paying off. Nobody likes hearing that, but let\u2019s be realistic: the sooner you know you took a wrong turn, the sooner you can correct your course towards delivering value. Experimenting and measuring success doesn\u2019t completely eliminate the risk of being wrong, but it does make sure you are not wrong long.\n", "tags": [], "categories": ["Blog", "Concepting &amp; UX"]}
{"post_id": 36848, "title": "Learn Elasticsearch from my liveProject at Manning", "url": "https://www.luminis.eu/blog/learn-elasticsearch-from-my-liveproject-at-manning/", "updated_at": "2023-02-16T10:02:43", "body": "I like to share my knowledge. I have written 100s of blogs, performed 10s of talks at conferences, and trained 100s of people during onsite training programs. This week I reached a new milestone in my knowledge-sharing endeavor. This week I finalized my liveProject at Manning called Elasticsearch for a Search API. You can read my experiences creating this liveProject in this blog post. If you only want to know what it is, jump to the end of this post.\nHow I became an author for Manning\nI had the idea of writing a book for a long time. How do you start with such an endeavor? To gain experience, I began reviewing books from Manning and Packt publishing. A few years ago, Manning asked me to help with the book Elasticsearch in Action. The author could not finish some chapters of the book, and I had the chance to rewrite them. The book has my name on the cover as a technical editor, which makes me proud. I never got to writing a complete book, but in May 2022, Manning contacted me and asked if I would like to create a liveProject. I liked the idea and checked with Luminis if it was ok, which of course, was no problem. On the contrary, they loved the idea.\nHow does it work\nI started with writing an overview of the project. The outline gave an idea about what the students learn, the different projects in the series, and the tools to be used by the students. After approval of the concept, the second step is a detailed plan for the complete series. For a liveProject to succeed, students must feel connected. I came up with an online shoe store called \u201cSneakers To The Max\u201d. The student is a search relevance engineer responsible for the search service used by customers through the website and the mobile app. Some experts in the domain reviewed my proposal; they were all very positive. After approving the detailed plan, I signed the contract and got started.\nI started writing the boilerplate code to get students underway. I wanted the student to lose as little time as possible and begin learning by running the code.\nThe next step was breaking up the solution into multiple assignments grouped into four projects, projects into milestones, and milestones into steps. Each milestone ends with a short examination. Besides the code, the questions, and the texts, I created a few images. Which were then transformed by aa design artist, and I like the sneaker created based on an elementary sketch from me. Infographics are available for each project, and I had to record the audio descriptions for these infographics.\nThe sketch I send the designer\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Logo for Sneakers to the Max\nTesting the project\nAt first, one person acted as an alfa tester. This tester is the first person that gives feedback about the project. My alfa tester had some excellent suggestions. In general, he was positive about the project. After some corrections, it was time for the beta testing phase. During beta testing, more people get to try out the project. Communication through the discussion board, lots of review comments. I was lucky with the beta testers. Again, there is a lot of positive feedback and excellent suggestions for improvement. I learned that one of the projects was too easy. I took a few weeks to create a new milestone for one of the projects.\nNow it is your turn\nWith the liveProject finished, it is time for students to start working on it. Try out the series if you want to sharpen your Elasticsearch skills and are not afraid of a bit of python. For now, the first project is free to try. I hope to see you in the discussion group or during the online call to pass the certification exam after completing all projects.\nhttps://www.manning.com/liveprojectseries/elasticsearch-for-a-search-api-ser\nCover image for the series at Manning\nQuestions about search, Elasticsearch, AWS OpenSearch, Solr\nIf you do not want to learn Elasticsearch yourselves but have questions about your search solution, feel free to contact us. We have the knowledge and experience to create the best search solution for all situations\n", "tags": [], "categories": ["Blog", "News", "Search"]}
{"post_id": 36764, "title": "Resolving the paradox of cloud-native solutions: more agility without total control", "url": "https://www.luminis.eu/blog/resolving-the-paradox-of-cloud-native-solutions-more-agility-without-total-control/", "updated_at": "2023-04-20T13:45:49", "body": "Being cloud-native enables organizations to implement changes at the speed of light, but leveraging cloud technology also means giving away control. That sounds like a paradox, but it resolves once you realize that you don\u2019t need to give away control completely. But what do you keep in-house, and what power do you give to your cloud technology and third-party software vendors? Sneak peek: focus your efforts on your core competitive edge while smartly integrating with the non-differentiating stuff.\nDon\u2019t feel like reading? Watch this post (and more) as a video\nLast week, I joined an XPLUS Breakfast Session to talk about how organizations can leverage cloud technology to shorten their time-to-value. Our client Ctac, personified by Ivo van der Raad, started the webcast by presenting their cloud plans for their XV Retail Suite. Exciting stuff, so feel free to come back here later and watch the session first.\nI used my ten minutes to look at this typical retail challenge from a technical point of view. This blog post is an extended transcript of that presentation, with the upside that you don\u2019t have to look at my face the whole time. Win-win.\nNow, back to resolving the paradox.\nThe need for speed\nBefore I get into the technical stuff, let me briefly explain the problem in business terms first.\nConsumer-facing software is moving faster than ever, with customers expecting increasingly safer, better, and faster solutions. Evolving technology is a significant enabler and driving force behind this acceleration. Successful solution providers combine tailor-made software, third-party packages, and SaaS offerings to craft the stuff their customers crave. That\u2019s impressive! But it also sounds like a magic trick. An illusion, if you will. How is it possible to hand over so much control to external parties and maintain the agility needed to compete in the market?\nSmart integration, that\u2019s how.\nThe integration challenge: from central management to distributed control\nThree focus areas emerge if we compare this current reality to the world of a decade or more ago.\n\u00a0\n\n\u00a0\nThe first is a shift from centrally managed, self-hosted application landscapes to distributed solutions. Let\u2019s say an organization has a few systems containing customer data and many custom-made services and clients that integrate with them. A decade ago, their IT department would have self-hosted this custom and third-party software combination, employing a variety of service buses, API endpoints, and direct messaging solutions. They would have slowly evolved the landscape whenever they had to build new functionality or add a new system.\nThanks to the advent of Software as a Service, IaaS, PaaS, and high-level cloud services, organizations need to move much faster to keep up. Their landscapes are fragmenting. Keeping expertise in-house is costly and reduces innovation budgets.\nHere emerges the second challenge. The technology market\u2019s shift to SaaS and managed cloud services, combined with the never-ending desire for better customer functionality, forces companies to gain traction and move more dynamically.\nThat can only mean, whether they want to or not, that builders need to let go of some of the control they used to have. But they still need to integrate everything. They must learn how to leverage new ways of integrating their building blocks, and I propose they use those giant vendors\u2019 shoulders and shift a lot of the heavy lifting onto the public cloud and SaaS providers\u2019 offerings.\nThe cloud-native integration paradox: more agility, less control\nAll right, now we are looking at what seems to be a paradox. Organizations need to be in total control, but at the same time, they need to hand over some control to external parties. What gives?\nOrganizations need to be able to move fast to respond to the changing needs of their customers. They must deliver changes into production quickly, see what works, and continually adapt based on what they learn. To do so, they require the technology and skills to precisely determine what their customers want and give it to them. They must practice deploying changes rapidly but safely. And they need to develop a software delivery process that is solid and improving over time. In other words, organizations need to be in complete control.\n\u00a0\n\n\u00a0\nHowever, I claim that they hand over some control to external parties. That means they no longer have total power over all the underlying infrastructure or the services and software they need to combine into a solution.\nSo here we have the paradox of delivering value, end-to-end, through software using the cloud and software as a service: the need for more agility while simultaneously leveraging the offerings of external vendors.\nLuckily, this is not an actual paradox. It is possible to increase agility while giving up control, but the trick is to be smart about it. Let me explain.\nBuild, buy, outsource, integrate? All of the above (but: pick your battles)\n\n\u00a0\nLet\u2019s look at the choices the fictional organization behind this highly successful (but fictional, like I said) app has made.\nIn the image above, you see an example architecture of an AI-powered photo retouching app, broken down into its components and plotted on a Wardley Map. The y-axis shows us how valuable elements of the solution are to end users. The higher, the more visible a component is to them, and the more valuable they deem it: a user generally does not care if you use platform X or Y as long as their app works reliably. The x-axis is about the evolutionary stage of the components. Just-discovered technology, for example, will appear on the left, but the older it gets, the more it will shift to the right. Over a decade ago, container platforms were relatively new, so they would have been plotted much farther left than they are now.\nAt the top left, we see the mobile application users interact with daily. It uses custom algorithms running somewhere in the backend or the app itself. This functionality is by far the most customer-visible and valuable thing they own; that\u2019s why it\u2019s so high on the map. So, they want complete control: they design, develop, and deploy the app and algorithms themselves.\nTo further evolve the algorithms, their data scientists and business stakeholders need to gain continuous insight into their users\u2019 behavior. The blue dots are the systems and services that help them collect, combine, transform, and finally visualize their user\u2019s behavioral data using managed services, third-party tooling, and a hosting platform. They don\u2019t need complete control here, so they opt to hand off its management to service providers. However, they do need control over integrating this part of their landscape.\nThen there is the so-called boring stuff, at least from a software creator\u2019s point of view, at the bottom right in orange: the critical infrastructure. Their customers do not care one bit if they manage this themselves. But like water and electricity, it is fundamental for everything built on top to work. So, they want to exert less control here: they don\u2019t want to spend costly resources managing all this highly available, virtually infinitely scalable stuff. They want to leverage the years of experience of a party that knows how to run critical infrastructure while paying only for what they use. So they outsource this.\nHere we have the first part of the answer to the so-called paradox of less control and more speed. Now let\u2019s solve the integration part of this puzzle.\nIntegrating the solution: less control = looser coupling\nHere is the same solution from an integration perspective.\n\u00a0\n\n\u00a0\nWe are still looking at a mix of self-hosted applications integrated with managed, third-party services and externally hosted SaaS. At the infrastructure level, there is a mixture of public cloud, private cloud, or a private data center, and the magical SaaS infrastructure.\nThe (still fictional) AI-powered photo retouching app company smartly combines integration solutions to make something of these mixed ingredients. The general rule of thumb here is simple: the more control they have over solution lifecycles, the tighter they can couple them. And vice versa: less control, looser coupling.\nOn the left-hand side, there is all the stuff they control entirely. They integrate using direct connections, push notification services, and message queues.\nIn the middle are the managed services they employ. They use a central event bus, so they decouple the integration slightly more than on the left by introducing a highly configurable, flexible, but fast integration solution in the middle. It could be a serverless event router like Amazon Eventbridge or Azure Event Grid.\nAt the far end, on the right-hand side, in orange, are the external parts of our landscape: SaaS offerings, maybe another cloud, or externally hosted service. Since the photo app company has no control over them, they decouple these elements using data transfer services or an event bus at the edge of their specific hosting platforms. This way, the organization can still evolve its end-to-end solution, and the SaaS and external service providers can continuously update their offerings independently. In the case of an API or contract change, the AI photo app developers only need to change the data transformation logic, while the rest can keep running as before.\nNow the speed-up-with-less-control paradox is resolved. Again, we need to realize that our landscape consists of groups of subsystems, grouped by their properties \u2014 in this case, the amount of evolutionary control the organization has over each solution piece. From this perspective, they can then integrate them using fit-for-purpose integration solutions. They still control their total solution, just not all its internals.\nExcellent, but the story may not convince you entirely yet \u2014 the company, its users, and its success are fictional, after all. Let me try to convince you with a real-world example where I was closely involved.\nOHRA: cloud-native development and integration from the trenches\nOHRA, one of the Netherlands\u2019 biggest directly writing insurance companies, asked Luminis to help migrate their application landscape from their on-premise data center to the AWS cloud. Their application landscape was large, with almost 1,000 integrations between internal and external services, clients, and databases. Did I mention the fixed deadline? About a year from the start. We helped them make it, partly thanks to the abovementioned integration strategy.\nMost of OHRA\u2019s moving parts are under their control, so they are integrated using a mix of direct messaging and message queues. When moving to AWS, we helped them mostly replatform the applications from application servers running Java applications to a managed Kubernetes cluster running containers. Much of OHRA\u2019s data flows through third-party packages spread over their application landscape. So we employed the tactic we saw on the previous slide: loose coupling for SaaS integrations using data transfer logic at the edge and asynchronous communication with third-party apps installed in their application landscape.\nA big help in making OHRA\u2019s cloud migration deadline was their existing service-oriented architecture, an excellent stepping stone towards a more event-driven and, thus, cloud-native architecture.\nFurther reducing complexity and costs, improving agility\nAfter migrating, the cloud provided OHRA with new opportunities to evolve its solutions. One example is the modernization of batch applications.\nPreviously, scheduled jobs were deployed on self-managed servers, running 24/7, even though most of them only actively did something for a few minutes to a couple of hours a day. In the public cloud, you should pay only for what you use, so I helped OHRA envision a more cloud-native and cost-effective way of creating short-running jobs.\n\u00a0\n\n\u00a0\nDuring two weeks, I led a team that re-architected an existing solution and delivered a modern, lean batch application: a distributed data processor made from a combination of serverless building blocks like Amazon EventBridge rules, Amazon SQS queues, and AWS Lambda Functions. As you can see in the image above, this drastically reduced the amount of code needed (and thus its complexity), the resources used, and the costs accrued.\nOHRA can use this cloud-native way of working moving forward and enable their teams to free up time and resources, which they can then spend on stuff that will help the business grow its competitive edge.\nOne more time: the paradox resolved\nSo, now you have a complete, high-overview answer to the faux paradox of gaining speed while not having complete control. Allow me to recap.\nFirst off, from a market perspective, companies are forced to move away from centrally controlled, neatly contained solutions running in their data centers towards integrating their locally managed custom building blocks with third-party software and externally hosted SaaS solutions.\nThere is a two-part solution to this problem.\nThe first part concerns being smart about spending valuable time and resources. Organizations require complete control of the software development and deployment lifecycle for the user-facing features they develop. Everything that powers this end-to-end solution \u2014 but is less visible or even completely invisible to the customer \u2014 can be hired as a service or bought as a product from a cloud provider.\nThe other half of the answer concerns being smart about integrating a fragmented landscape. I identified three groups, from fully controllable and evolvable elements to managed services and non-transparent SaaS solutions. Tight coupling is a good solution for distributed applications under a company\u2019s control, but managed services and SaaS need looser coupling. Organizations can implement this solution by leveraging a combination of message queues, event buses, and data transfer services.\nAnd thus, the paradox is dissolved!\n", "tags": ["cloud", "devops", "innovation"], "categories": ["Blog", "Cloud"]}
{"post_id": 36736, "title": "Traceable tests", "url": "https://www.luminis.eu/blog/traceable-tests/", "updated_at": "2023-01-30T15:50:23", "body": "Have you ever spent hours or days trying to figure out why some API test is failing? Whenever something like that happens to me, my immediate thought is: what could have helped me find this problem faster?\nUnit tests are much better at this, when they fail, you just run them again locally, set a breakpoint if you have to, and find out what failed. But with tests that run against your API (yes, you need those too!) it can be much harder to figure out exactly what code got executed. There are all sorts of boundaries that make this hard, especially when those tests run as part of your pipeline (yes, you need to do that!). You can\u2019t just set a breakpoint in your pipeline and even if you have logging in your backend, how do you know which log entries resulted from which test scenario?\nI\u2019ve been playing with the idea of linking backend logs to tests for some time. Years back, I had a plan to add the name of the test to a custom HTTP header when calling the API from tests. In the backend I could add that value to the MDC context that we attach to log entries. The result would be that for each log entry, you could see which test triggered it.\nD\u00e9j\u00e0 vu\nSomehow I never got around to implementing that idea. Last year though, I helped out a team working on an API-only service that ran into the very same problem. Their system is comprised of a set of microservices and some of the API calls go through several of these services to handle the request. Next to unit tests, one of their other automated validations is a regression test suite that runs against the API to validate expected behavior.\nSo far, so good, and that test suite was really valuable. It allowed the team to quickly implement new functionality without spending a lot of time checking that existing functionality kept working.\nYou know what\u2019s coming now\u2026 whenever a test would fail, it was a nightmare to figure out why.\nLuckily, I had just finished hooking up al the services to an Elastic Cloud environment, with APM-powered tracing and all the good stuff. The traces gave excellent insight into what happened inside and between the micro services. However, it was still really hard to link a specific test failure to a set of traces.\nOpenTelemetry FTW \ud83d\ude03\nI still had that idea from before in my head, and when I saw the team struggling with these failing tests, a new idea popped in: what if I linked the traces to tests?\nThe typical method to pass trace ids across service calls is by using a trace header. OpenTelemetry provides the traceparent header for that, and the Elastic APM agent being used in the services picks that up out-of-the-box.\nMy first idea was to return the traceparent header in the API responses and then log them with each test. However, that\u2019s not really how the OpenTelemetry traceparent header is supposed to be used. You\u2019re supposed to include the header in the calling request. So that\u2019s what I tried next.\nShow me the code!\nFirst thing I did was locate how the test suite was making the API calls. Turned out this was done with HttpClient, which offers a nice way to inject additional headers\nimport static TraceInstrumentationInit.openTelemetry;\r\nimport static TraceInstrumentationInit.tracer;\r\n\r\npublic class HttpClientTraceInstrumentation implements HttpProcessor {\r\n  @Override\r\n  public void process(HttpRequest request, HttpContext context) {\r\n    injectTraceHeaders(request);\r\n  }\r\n\r\n  @Override\r\n  public void process(HttpResponse response, HttpContext context) {\r\n  }\r\n\r\n  private static void injectTraceHeaders(HttpRequest request) {\r\n    // TODO\r\n  }\r\n\r\n  public static HttpClient registerTraceInstrumentation(\r\n        DefaultHttpClient httpClient) {\r\n    HttpClientTraceInstrumentation traceInstrumentation = \r\n      new HttpClientTraceInstrumentation();\r\n    httpClient.addRequestInterceptor(traceInstrumentation);\r\n    httpClient.addResponseInterceptor(traceInstrumentation);\r\n    return httpClient;\r\n  }\r\n}\nOk, so that gives us a way to inject the traceparent header. First I figured I could generate the trace id myself. However, it turns out the traceparent value is not just a trivial UUID, it has a very specific structure and I wouldn\u2019t advise trying to generate it yourself.\nLuckily, OpenTelemetry has excellent SDKs that make it easy to generate and inject these headers. So we need a dependency on the OpenTelemetry API and SDK.\n<dependencyManagement>\r\n  <dependencies>\r\n    <dependency>\r\n      <groupId>io.opentelemetry</groupId>\r\n      <artifactId>opentelemetry-bom</artifactId>\r\n      <version>1.18.0</version>\r\n      <type>pom</type>\r\n      <scope>import</scope>\r\n    </dependency>\r\n  </dependencies>\r\n</dependencyManagement>\r\n<dependencies>\r\n  <dependency>\r\n    <groupId>io.opentelemetry</groupId>\r\n    <artifactId>opentelemetry-api</artifactId>\r\n  </dependency>\r\n  <dependency>\r\n    <groupId>io.opentelemetry</groupId>\r\n    <artifactId>opentelemetry-sdk</artifactId>\r\n  </dependency>\r\n</dependencies>\r\n\nNext, we had to initiate a trace for each scenario, and a sub-span for each API call being made.\nAfter looking at the codebase, we saw we could add that to an already present base class used by all tests to report scenario results to TestRail. Personally I\u2019m not a big fan of using base classes for tests as it can be hard to maintain a sensible hierarchy and the use of inheritance creates tight coupling between tests, which we don\u2019t want. In JUnit for example, I would use @ExtendWith to keep such responsibilities separated and easier to re-use and to avoid the use of static instances as seen here. But we didn\u2019t want to overhaul the complete test suite just to introduce this, better not to mix a big refactoring like that. The main goal at this point was to validate that traces could work.\nAs you see, we also passed the traceId to TestRail, more on that later.\npublic class Hooks extends BaseSteps {\r\n  private static final TestRailReporter testRailReporter =\r\n    TestRailReporterFactory.get();\r\n  private static final ScenarioTraceInstrumentation instrumentation =\r\n    new ScenarioTraceInstrumentation();\r\n\r\n  @Before\r\n  public void setup(Scenario scenario) {\r\n    instrumentation.startTrace(scenario);\r\n  }\r\n\r\n  @After\r\n  public void teardown(Scenario scenario) {\r\n    String traceId = instrumentation.endTrace();\r\n    testRailReporter.reportResultToTestRail(scenario, traceId);\r\n  }\r\n}\r\n\nFor every scenario we start a new span, including attributes to link it to the scenario. We also do some rudimentary logging for every scenario, making it easy to spot in the logs which scenario ran at which point and what the trace id was for that scenario run.\nimport static TraceInstrumentationInit.tracer;\r\n\r\npublic class ScenarioTraceInstrumentation {\r\n  private final ThreadLocalSpan spanState = new ThreadLocalSpan();\r\n\r\n  public void startTrace(Scenario scenario) {\r\n    Span span = spanState.start(\r\n      tracer.spanBuilder(scenario.getName())\r\n            .setSpanKind(SpanKind.CLIENT)\r\n    );\r\n    span.setAttribute(\"test.scenario_id\", scenario.getId());\r\n\r\n    System.out.println(\"|---------\\n\"\r\n      + \"| SCENARIO: \" + scenario.getId() + \" - \" + scenario.getName() + \"\\n\"\r\n      + \"| trace.id: \" + span.getSpanContext().getTraceId());\r\n  }\r\n\r\n  public String endTrace() {\r\n    String traceId = spanState.traceId();\r\n    spanState.end();\r\n    return traceId;\r\n  }\r\n}\nTo keep track of span state, I used a ThreadLocal, exposing a simple API to start and end the span that we could use from the ScenarioTraceInstrumentation and HttpClientTraceInstrumentation. As you see I did get lazy a bit here, using a Pair to store both the Span and Scope, using a record for that would make the code easier to read.\nclass ThreadLocalSpan {\r\n  private final ThreadLocal<Pair<Span, Scope>> state = new ThreadLocal<>();\r\n\r\n  Span start(SpanBuilder spanBuilder) {\r\n    Span span = spanBuilder.startSpan();\r\n    Scope scope = span.makeCurrent();\r\n    state.set(Pair.of(span, scope));\r\n    return span;\r\n  }\r\n\r\n  String traceId() {\r\n    return state.get().getLeft().getSpanContext().getTraceId();\r\n  }\r\n\r\n  void end() {\r\n    Pair<Span, Scope> pair = state.get();\r\n    pair.getRight().close();\r\n    pair.getLeft().end();\r\n  }\r\n}\r\n\nThe OpenTelemetry SDK also needs some initialization. Keeping it simple for this first experiment, and since some of the other code was static already, we just created two package scoped statics to be used from the ScenarioTraceInstrumentation and HttpClientTraceInstrumentation.\nclass TraceInstrumentationInit {\r\n  static final OpenTelemetry openTelemetry = initOpenTelemetry();\r\n  static final Tracer tracer = openTelemetry.getTracer(\"regressionTest\");\r\n\r\n  private static OpenTelemetry initOpenTelemetry() {\r\n    SdkTracerProvider sdkTracerProvider = SdkTracerProvider\r\n        .builder().build();\r\n    OpenTelemetrySdk sdk = OpenTelemetrySdk.builder()\r\n        .setTracerProvider(sdkTracerProvider)\r\n        .setPropagators(ContextPropagators.create(\r\n          W3CTraceContextPropagator.getInstance()\r\n        ))\r\n        .build();\r\n    Runtime.getRuntime().addShutdownHook(\r\n      new Thread(sdkTracerProvider::close)\r\n    );\r\n    return sdk;\r\n  }\r\n}\nAnd finally, we could put all the the pieces together in the HttpClientTraceInstrumentation, see below.\nWe create an additional span for the request [1] and then inject the traceheader into the request [2].\nThe OpenTelemetry SDK code is very generic, allowing you to inject headers into pretty much anything, from HTTP requests to Messages with all sorts of client libraries. The extremely generic code does make it a bit hard to read, but essentially you need to pass it a method that accepts two string arguments, the header name and header value. The HttpRequest from HttpClient has exactly such a method: setHeader [3].\nLast but not least, we also log every request being made [4], giving a nice overview in the test logs of what API calls are being made for each scenario.\nimport static TraceInstrumentationInit.openTelemetry;\r\nimport static TraceInstrumentationInit.tracer;\r\n\r\npublic class HttpClientTraceInstrumentation implements HttpProcessor {\r\n  private static final String W3C_TRACEPARENT_HEADER = \"Traceparent\";\r\n\r\n  private static final TextMapPropagator textMapPropagator =\r\n    openTelemetry.getPropagators().getTextMapPropagator();\r\n\r\n  private static final TextMapSetter setter = \r\n    HttpRequest::setHeader; // [3] http client setHeader method\r\n\r\n  private final ThreadLocalSpan spanState = new ThreadLocalSpan();\r\n\r\n  @Override\r\n  public void process(HttpRequest request, HttpContext context) {\r\n    Span span = spanState.start(\r\n      tracer.spanBuilder(\"/\")\r\n          .setSpanKind(SpanKind.CLIENT)\r\n    );\r\n    try (Scope ignored = span.makeCurrent()) {\r\n      // [1] add span details\r\n      span.setAttribute(HTTP_METHOD, request.getRequestLine().getMethod());\r\n      span.setAttribute(HTTP_URL, request.getRequestLine().getUri());\r\n      span.setAttribute(\"component\", \"http\");\r\n\r\n      injectTraceHeader(request);\r\n      logRequest(request);\r\n    }\r\n  }\r\n\r\n  @Override\r\n  public void process(HttpResponse response, HttpContext context) {\r\n    spanState.end();\r\n  }\r\n\r\n  private static void injectTraceHeader(HttpRequest request) {\r\n    // [2] let opentelemetry sdk propagate any required headers\r\n    textMapPropagator.inject(Context.current(), request, setter);\r\n  }\r\n\r\n  private static void logRequest(HttpRequest request) {\r\n    System.out.println( // [4]\r\n      \"|- \" + request.getRequestLine() + \"\\n\" +\r\n      \"|  ^- \" + request.getLastHeader(W3C_TRACEPARENT_HEADER));\r\n  }\r\n}\r\n\nEverything coming together\nNow when tests are run, their logs clearly show each of the scenarios, their trace id and the API calls being invoked. And for every API call, the complete traceparent header is shown. With these details being propagating it to our backend, we can now lookup all the calls for a scenario in Elastic, or even search there for the span id to locate the trace for a specific API call.\n|---------\r\n| SCENARIO: T2748705 - Validate if method PUT was added to controller and it's possible to use it\r\n| trace.id: d28a64332015de5d324cb3e0f0380eba\r\n|- PUT http://.../api/...\r\n| ^- traceparent: ...\r\n...\r\n...\r\n|---------\r\n| SCENARIO: ...\r\n| trace.id: ...\r\n|- GET http://.../api/...\r\n| ^- traceparent: ...\r\n...\r\n...\r\n\nIntegrate existing tools\nAt this client, the team used TestRail to centrally view reports of all test runs. To find the cause for a failing test, wouldn\u2019t it be nice if we could skip spitting through logs. As you saw before, we had the trace id available when we posted results to TestRail, so we added a link there to take you directly to the corresponding trace in Elastic.\n\nResults!\nWhen you follow that link to Elastic, it shows you all the API calls (transactions) that were part of that test scenario (trace).\nFrom there you can navigate to each API call and look at individual traces. Elastic shows a lot of useful info, like a latency distribution chart that makes it super easy to quickly select all slow calls or all failing calls.\n\nThere is also a dedicated errors section, which allows you to quickly locate details for any errors that occurred as part of the scenario traces.\n\nTip: If you want to take a closer look at what\u2019s in these kind of traces, there is a nice free Elastic demo environment where you can play around with traces in the APM section.\nConclusion\nWith just a little bit of code, we managed to reduce the amount of time needed to pinpoint test failures dramatically. Which shows again that optimizing for quick work loops and short feedback cycles are crucial for improving software deliver performance.\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 34178, "title": "Top 8 tips for visiting re:Invent 2023", "url": "https://www.luminis.eu/blog/top-8-tips-for-visiting-reinvent-2023/", "updated_at": "2022-12-13T14:50:57", "body": "AWS re:Invent 2022 will be an event I will never forget. From the sheer size of the event to the high-quality sessions, conversations, and swag, re:Invent has a lot to offer if you are at all working with AWS.\n\nThe event took place in Las Vegas, spanning multiple venues on the strip. AWS did their best to ensure that the event\u2019s magnitude was manageable for the visitoran app that shows everything that is happening that week and \u00a0by introducing helps you navigate between the venues.\u00a0\nEven with this app handy and all of the re:Invent employees ready to help you at any moment, there are still some things that are good to know before going to the event. In this blog, I will share 8 tips for re:Invent 2023 that I learned from my visit this year.\n#1: Reserve your sessions\nThe sessions scheduled for re:Invent will be made available much earlier than the event, and you will have the option to reserve your seat at these sessions. I highly recommend you take the time to do this and reserve the sessions that you want to go to.\n\nWhen you finally visit re:Invent, all sessions will have two lines. One for the reserved visitors and the other for the unreserved (walk-up) visitors. Having a reserved seat will not only guarantee you a spot at the session, but it also allows you to do other things and show up 15 minutes before the session and still enter. The people in the walk-up line usually start to gather about 45 minutes before the session and have to stand, and sometimes sit, in line until 10 minutes before the session. Don\u2019t let this happen to you. Reserve your seat!\n#2: Get Certified\nThere are sessions, swag, events, and even a special lounge for visitors that are AWS certified.\n\nI had the most interesting interactions and the most fun at these events. I had conversations with AWS builders around the world. We had discussions about the certification process, how they use AWS at the companies they work, what kind of costs they have, which AWS services they use, and a lot more. We also played games and had some drinks, and eventually added each other on LinkedIn to keep in touch. Don\u2019t miss out on this. Get certified! Any official AWS certification will do.\n\n#3: Join the AWS Community builders\nThere are also special perks for visitors that are part of the AWS Community Builders group. This gives you access to events and special SWAG. There is an open application twice a year to join. To get into the AWS community builders, you need to:\n\nWrite blogs about AWS\nSpeak at conferences about AWS\nGive workshops on AWS technology\nAnswer questions online about AWS technology\nContribute to AWS open source projects\n\u2026anything else that spreads the word and helps others learn about AWS\n\nBy joining, you get several benefits next to all of the special access and SWAG at re:Invent:\n\n500$ AWS credit\n1 year subscription at CloudAcademy\n1 AWS certified exam voucher\nAccess to Slack which gives you access to AWS community builders and Hero\u2019s\nre:Invent discount of 50%\nMore SWAG\nLots of exclusive access to resources (webinars, beta previews)\n\n\n#4: Leave room for SWAG\nAWS re:Invent has SWAG, a lot of it! Once you visit the Expo, where all the stands are of the event sponsors, you will be overwhelmed with the number of companies that have shown up and are eager to talk to you about their products. Most, if not all stands have some sort of SWAG to give to you. These come in all shapes and sizes. Leave some room in your suitcase!\n\n#5: Vary the type of session you visit\nThere are several types of sessions to choose from at re:Invent, ranging from typical breakout sessions to workshops. One fact to note is that a breakout session is recorded and can be viewed later on, while sessions such as Chalk or builders sessions do not. If you are struggling to decide which sessions to go to and some of them overlap, consider the session type and choose the non-breakout sessions. You can always watch the breakout sessions later.\nFor example, at one of the \u201cbuilder sessions\u201d I was sitting at a table with a Senior AWS architect and other re:Invent visitors to discuss how we can monitor costs on AWS. It was very insightful to see which tools were being used, either AWS native or third party, and also to see the massive number of costs some people are dealing with. One of the tools they discussed in this session is the CUDOS dashboard, which you can deploy via a CloudFormation template that can monitor various aspects of your costs and give you recommendations.\u00a0\nAnother type of session was the \u201cChalk\u201d session, which was very interactive. In one of the sessions, we designed a solution that included using Lambda extensions to debug them from your local development environment, decreasing the feedback loop during Lambda development.\nI also visited some breakout sessions, which are available now on Youtube. Some of my favorites include:\n\nUnleash developer productivity with infrastructure from code\n\nAn introduction to Infrastructure FROM code from the CEO of Ampt AWS services.\n\n\nA close look at AWS Fargate and AWS App Runner\n\nA walkthrough of the evolution from EC2 instances to AWS App Runner, given by a principal engineer at AWS that has been there from the start.\n\n\nA day in the life of a billion requests\n\nA look into how AWS optimized their authentication mechanism to handle half a billion requests per second to IAM. (spoiler: they use a lot of HMAC\u2019s)\n\n\nAre you integrating or building distributed applications?\n\nAn overview of choices you have to make when building distributed applications and makes clear that the words you use are important. Recommended for anyone in an Architect role.\n\n\nThe architect elevator: Connecting the boardroom and IT\n\nThis talk describes the impact you can have as an architect within a company and gives you tips on how to do this. Also highly recommended for anyone in an Architect role.\n\n\n\n#6: Take the venue\u2019s locations into account\nAs mentioned previously, re:Invent takes place at multiple venues on the Las Vegas strip. Take a moment to check where all the venues are and how long it takes to walk to each venue. AWS offers shuttles between venues, and in my experience, these work well. However, even with the shuttles, if you have to get from the Wynn to the Mandalay Bay, then you are probably going to miss the mark.\n\n#7: Book a hotel close to the Expo\nThe expo was held at the Venetian. This is the place you want to be when you are not in any other sessions or events. There is a lot to do and see there, such as speaking to people at the several stands about their AWS-related products, collect SWAG, play several types of games, win prizes, attend lightning talks, view product demo\u2019s and get some snacks.\nBooking a hotel close to the Expo gives you more opportunities to experience everything it offers.\n\n#8: Register and attend receptions\nThere is breakfast and lunch included on most days of re:Invent. However, try registering for as many receptions as possible for dinner. These will be announced in the AWS re:Invent app and will be hosted by several of the sponsors. This gives you a good chance to network with different kinds of people. Some of these will offer buffet dinners which is a nice plus.\u00a0\nFor some of these events you have to register on time or you won\u2019t get in, and in some cases, the registration starts well before re:Invent, so be sure to keep an eye out online and in your email inbox. If you get an invitation, register immediately as they are quickly full.\nConclusion\nI had a great time at re:Invent, and I hope these tips will help improve your re:Invent experience if you choose to attend in person. Of course, you can attend re:Invent virtually as well, for free. Some of these tips, such as getting certified or joining a community, will require you to take action now. It might seem like a lot of work. However, you will be happy once you reap the benefits at re:Invent next year.\n", "tags": ["aws", "AWS re:Invent 2022", "AWS re:Invent 2023"], "categories": ["Blog", "Cloud"]}
{"post_id": 34120, "title": "Cybersecurity Awareness | Phishing", "url": "https://www.luminis.eu/blog/cybersecurity-awareness-phishing/", "updated_at": "2022-11-30T11:59:53", "body": "In October 2004 U.S. Department of Homeland Security and the National Cybersecurity Alliance launched Cybersecurity Awareness month in effort to create awareness and helping individuals to protect themselves. Since 2012 this idea has been adopted by ENISA (European Union Agency for Cybersecurity) with an annual campaign dedicated to promoting cybersecurity among EU citizens and organizations through awareness raising activities and sharing of good practices.\nIn 2022 the theme is \u201cThink Before U Click!\u201d #ThinkB4UClick, which addresses two of the most common threads, namely Phishing and Ransomware.\nIn this blog we will dive into the thread called Phishing. What is it, how to recognize a phishing attack, prevention and what to do when receiving a phishing email and why it is important to create awareness within your organization.\nWhat is phishing?\nPhishing is a social engineering method, where an attacker\u2019s goal is to trick a person to reveille personal and/or sensitive information like credentials, PIN, phone numbers, financial information or any other information that could be used to perform the next step in the attack chain, like gaining access to systems.\nOne of the commonly used methods is email phishing. With this method the attacker sends an email to a person or organization and mostly requires the receiver to login into a website that looks familiar, but in fact is fraudulent.\nHow to recognize email phishing?\nEmail phishing attacks are evolving and become harder to recognize. Still there are some \u2018red flags\u2019 that could help you recognize a phishing email or at least raise suspicion when receiving one.\nA brief list of \u2018red flags\u2019 when comes to recognizing a phishing email;\n\nRequest to login\nThe email contains a link to a website where you need to login\nPoor grammar\nAlthough not always the case, a phishing email containing poor grammar and spelling errors should at least raise suspicion\nUrgency\nThe message contains some urgency. \u201cYou have limited time to click on the link\u201d\nMessage is addressed to a generic recipient\nPhishing emails are often addressed to a generic recipient. \u201cDear sir/mom\u201d\nFrom name and address do not match\nThe \u2018from\u2019 name does not match the address that is used in the email\nSubject unclear\nThe subject of the message is unclear. What is the purpose of the message you received?\nContact information missing\nThe contact information you could use to verify the purpose of the message is missing\n\nAn image of a suspicious email containing some of the red flags mentioned above\n\nPreventing phishing within your organization\nImplementing proper technology that automates the process of detection and prevents the delivery of suspicious emails to your users is the first (and necessary) step. Equally important is to keep your organization informed and \u2018up-to-date\u2019 through education and awareness on this subject. When an automated process fails to recognize a malicious email, the receiver is the next in the \u2018line of defense\u2019 and they should be able to recognize it based on some red flags. And this can only be achieved by creating awareness and training.\nWhat to do when receiving a suspicious message\nWhen you are suspicions about a message you received or identified one as phishing and it has any attachments, it is Important not to open it. An attachment may contain harmful software (known as malware) that could supply access to the attacker or even harm your system.\nA general rule to confirm if the sender intended to send you the message, is simply to contact the sender (if possible) and verify it. If contact information is missing, that should be a red flag and you should not take any actions requested in the message.\nIf you are not reassured after the previous step, you should always report the message as phishing with your mail client. This information is used by automatic detection processes to avoid further spreading of the message through the organization.\nIn case you (accidently) opened the attachment or followed up the instruction in the mail, then you should report this to your security department. Proper actions should be taken by the security department to prevent this spreading further within an organization.\nIn most cases a phishing attack is executed on several users within an organization. Therefore, it is important to inform others within your organization about a (possible) phishing attack, and it is recommended to use the internal communication channel.\nTo summarize the actions, you should (or should not) take as a user when a suspicious message reaches you:\n\nDo not open any attachments\nAn attachment may contain harmful software (known as malware) that could provide access to the attacker or even harm your system.\nContact the sender\nIf possible, contact the sender in a separate message to verify if they did send a message and request information about the purpose of the message\nReport it as phishing\nMost mail clients offer functionalities to report phishing. This helps (automated) systems to recognize phishing attacks and prevents them from spreading further within an organization.\nUse internal communication channels to inform others about a (possible) attack\n\nWhy it is important to create awareness\nPhishing and other attacks have increased, especially during the Covid period. Educating your employees and increasing awareness within your entire organization has become important, since the employees are \u2018the first line of defense\u2019 if an automatic process fails. Recognizing malicious attacks and taking proper actions to prevent an attacker from gaining access to sensitive data and increases the security maturity within your organization.\nIn conclusion\nEvery organization should be aware of a (possible) attack and ready to act if one occurs. Setting up processes, testing these and using the proper security tooling to mitigate and/or prevent attacks and create cybersecurity awareness through training and attack simulations. And of course, #ThinkB4UClick\n", "tags": [], "categories": ["Blog", "Security"]}
{"post_id": 33652, "title": "Using Azure Active Directory for your web apps: Thoughts from a software developer", "url": "https://www.luminis.eu/blog/using-azure-active-directory-for-your-web-apps-thoughts-from-a-software-developer/", "updated_at": "2022-09-27T13:58:49", "body": "With the emergence of identity as a service (iaaS) from Cloud service providers such as Azure and AWS, managing authorization and authentication to your web application in a secure manner has become much easier and less work for developers.\nFor most scenarios we no longer need to implement ASP.NET Core Identity user management ourselves, nor setup our own identity server (for single sign-on). Instead we can make use of the identity service provider called Azure Active Directory (AAD) which is maintained and developed by Microsoft. This is a good thing, as correctly implementing such software patterns is non-trivial and may incite insecure practices (such as the password-credentials flow) unless the developer knows exactly what he or she is doing.\nHowever, I have found that properly understanding and managing iaaS via Azure can be a confusing topic in itself and it took me some time to properly understand how to use and configure it. I would like to explain how you can use it for you own applications and clarify some concepts that were originally unclear to me in hopes of providing some illumination for others. For the scope of this article, I\u2019ll focus on one of the most basic scenarios for using Azure active directory as a software developer: How to retrieve access tokens for your SPA (Single Page Application) which communicates with your web API, and subsequently protect your web API against unauthorized access.\nFor the scenario in which you have a SPA that targets a web API, I have found that this documentation best covers the steps you need to take.\nThe specific example here is registering the API and the SPA with Azure AD B2C (which is AAD for customer defined identities), but the process should be the same for normal Azure AD (which is AAD for Microsoft identities).\nFor the scope of this article, I would like to expand the following topics in that piece of documentation.\nSpecifically, I\u2019ll cover three topics that I feel require additional emphasis beyond the official Microsoft documentation\n\nApp registration and MSAL.\nScopes and admin consent.\nId tokens vs Auth tokens.\n\nI\u2019ll additionally provide the official links that cover how to implement these things technically for convenience of the reader.\nApp registration and MSAL\nPicture the basic scenario that users will be logging into our SPA using a Microsoft AD account of employees from a single tenant. Our SPA needs to retrieve application specific information from a ASP.Net Core web API. We want to protect this API so that only a specific subset of the users within our tenant are allowed to retrieve data from the web API.\nSchematically we arrive at the following idea:\n\nReceiving and verifying the token is a bit more complex than this schema suggests but this suffices to convey the basic idea for now.\nLet\u2019s first look at how the SPA registers with AAD.\nOn the Azure portal side, you will have to create an app registration. The app registration is an entry inside Azure that contains all identity related information of an application that you register with AAD. Perhaps somewhat counter-intuitively, this registration typically has no relationship with the deployed instance of the website on Azure itself. Instead, the front-end code of the SPA uses a Javascript library called MSAL (\u201cMicrosoft Authentication Library\u201d) and a configuration file containing a TenantId, ClientId and redirect URL to find the app registration on Azure. Where the TenantId is the identifier of the organisation/tenant under which the app registration is created, the ClientID is the identifier of app registration instance on Azure itself and the redirect URL is the web address at which the SPA requests the access token to be returned. Upon successful retrieval of the token by the SPA via the logic from the MSAL library, the same token will be added as HTTP header to each request from your SPA to your web API. Note that the successful retrieval of the token is also generally the clue that the website user has successfully logged in for your front-end application logic.\nMSAL isn\u2019t just available as a JavaScript library, but also for .NET applications, Java applications, mobile applications and a few others. The beauty of it is that the library manages interaction with AAD for fetching tokens, fetching refresh tokens and expiring the token. Additionally, you don\u2019t have to search for a discovery document, specify the token endpoint or manage storage of the token in the web browser (etc.) yourself as the MSAL library will do these things for you. You only have to follow the implementation instructions for your specific platform and configure a settings file generally.\nOn the app registration side, we need to specify which root domains we white-list to request an access token. We also specify what type of consent the user requesting the token must give, what scopes the token is for and what type of tokens are handed out. The above-mentioned Microsoft documentation explains clearly how to do these things, but in order to understand what you are actually doing I feel some extra explanation is very welcome. In the rest of this blog, I\u2019ll elaborate on these things.\nNow that I have given an overview of how the SPA will interact with AAD, that leaves the topic of the web API. With each request to the web API from our SPA, the authorization token, which is usually a Json Web Token (JWT), is passed along in the authorization headers. Therefore, the process for integrating the web API with AAD is a bit simpler than for the SPA. The only thing we really need to do is register our web API with AAD via a separate app registration and subsequently verify the JWT with the AAD service upon each request entering the API. You\u2019ll usually want the Web API and the SPA to each have their own app registration, so you can finetune their scopes separately. The API can be granted admin consent for example, where the SPA is not (I\u2019ll discuss consent and scopes in the next section). In a typical .NET Core webapp, the entire implementation could be something as simple as this (when using the Microsoft.AspNetCore.Authentication Nuget package):\n\n\nFuther reading on code configuration\n\nScopes and admin consent\nThe concept of scope for a JWT may be a little confusing at first. Scopes are not part of the JWT itself but are sent alongside a request towards the authorization service to specify what type of information the application would like access to. If the requested Scope and previously granted access (either previously by the user, or from admin consent), \u00a0are not the same for the user identity matching to the requested JWT, the user will have to give consent for the app to fetch the additional information. Alternatively, if that user is not allowed to give this consent for a specific resource, he/she may be prompted to ask an admin within the organisation for consent. When the JWT is subsequently verified by the Web API, the scope that was agreed upon by that user, is again be checked via AAD (the previously granted scopes are stored on the AAD side for that User identity), to see if it encompasses the specific API information that the front-end tries to access.\n\nFurther reading on scopes\nFurther reading on consent prompts\n\nOftentimes you\u2019ll find your SPA will need a complex set of information from your web API and you don\u2019t want to bother the employees using your app with a consent pop-up. In these cases, AAD offers the option to provide tenant-wide admin consent. Basically, you login as an administrator to give an application or group of users in your tenant permanent consent for a specific scope without user interaction. This should be handled with some care and you should make sure the application is only granted access for the scopes that it actually needs. Granting admin consent can be done in the following Azure pane:\n\nIt is a powerful tool but the developer should take care to not abuse it in scenarios where user-consent would suffice.\nID tokens versus Auth tokens\nPerhaps one of the most obscure options the AAD portal interface provides to the developer that tries to setup AAD single sign on for one of his or her app for the first time is the concept of ID tokens and access tokens:\n\nThe wording here is extremely confusing so let me explain when and how to use this. Apparently AAD distinguishes between Access tokens and ID tokens. These are the two types of JWT that the authorization endpoint can hand out. Note that the Azure portal speaks about the AUTHORIZATION endpoint and specifically not the TOKEN endpoint. To understand the difference between the two, we need to cover the two OAuth based authorization flows that are commonly used. Namely the authorization code flow and the implicit flow.\nImplicit grant flow\nA popular authentication method that has been used for single page web applications (SPA\u2019s) in recent years, is the implicit flow (a.k.a. the \u00a0OpenId Connect (OIDC) implicit grant flow). However, the most commonly used way to implement this type of authentication and authorization nowadays, is via the so called authorization code flow. (Implicit flow has fallen out of favor since browsers started supporting cross origin resource sharing).\nBecause it\u2019s chronologically the more dated one, let\u2019s start by discussing the implicit flow:\n\nImplicit grant flow\n\nThe authentication library signals the browser to navigate to the access (bearer) token endpoint of the identity provider*.\nBecause the user is not authenticated, the browser is redirected to login screen for the authentication service.\nThe user enters his credentials and these are verified by the authentication service.\nThe browser is redirected to the access token endpoint.\nThe access token is returned as a URL/query parameter to a redirect URL that was pre-configured at the Authorization Server, ensuring that only the designated domain can request the token. Because the token is passed in the URL, it is extra important to always communicate over Https when using the implicit flow.\nThe access token is held in the browser storage by the authentication library. It is retrieved and added to the request headers by your SPA or mobile app.\nRequests made to your web API can now be accompanied by the access token.\nThe web API can now verify whether these requests are authentic with the identity provider, using this token.\n\n* Azure Active Directory in this specific case.\nOn a side-note: The Azure Active Directory Authentication Library (MsAdal) was the popular predecessor to MSAL for JavaScript applications until about a year ago. It was therefore an attractive option for implementing an authentication system for your front-end apps. However, this predecessor exclusively supported the implicit grant flow.\nAuthorization code flow\nThe authorization code flow is the currently recommended way to manage tokens from AAD and I\u2019ll try to describe it the best way I can with the following figure and step-by-step explanation.\n\nAuthorization code flow\n\nThe authentication library signals the browser to navigate to the authorization code endpoint of the identity provider*.\nBecause the user is not authenticated, the browser is redirected to login screen for the authentication service.\nThe user enters his credentials and these are verified by the authentication service.\nThe browser is redirected back to the Auth code endpoint. This time the authorization code can be returned.\nThe authorization code is returned as a URL/query parameter to a redirect URL that was pre-configured at the authorization server ensuring that only the designated domain can request the authorization code.\nThe authentication library does a POST request to the /token endpoint of the identity provider, providing the (PKCE protected) authorization code.\nThe access (bearer) token is retrieved to the return URL.\nThe access token is held in the browser storage by the authentication library. It is retrieved and added to the http request headers by your SPA or mobile app. (Refresh token can be fetched from the identity provider once needed and replace the access token).\nRequest made to your web API can now be accompanied by the bearer token.\nThe web API can now verify whether these requests are authentic with the identity provider, using this token.\n\n* Azure Active Directory in this specific case.\nAs we can see, the authorization code flow expands on the implicit grant flow by first retrieving an authorization code, before getting the access token with the help of that authorization code. The significant advantages of using the authorization code flow over the implicit grant flow are that (A) the access token is not as exposed and (B) the availability of refresh tokens. These advantages are possible because (A) the authorization code is short-lived and single-use, and that (B) the access token cannot be easily intercepted (due to PKCE and because it is never passed in the URL).\nBack to the Azure portal\nIn short, we can now understand that we would like to use the auth code flow in most scenarios. How to configure the auth code flow in AAD depends on whether additional user claims are needed. If we do not need additional user claims in our front-end, we only need an authorization code from the /authorize endpoint of AAD and therefore both the Access token and ID token checkboxes can be left unchecked. We get the access token from the /token endpoint of AAD and not from the /authorize endpoint after all. If we need more info for our front-end than just the token to send with the request headers to our web API (for example a set of claims that control whether our front-end does or does not show certain features and buttons) we should still use the auth code flow, but enable the checkbox for ID Tokens. This causes an ID Token to be sent alongside the authorization code to the redirect URL. This configuration is what Microsoft calls the \u2018hybrid\u2019 flow. Unlike the auth code and the access (bearer) token, the ID token contains additional information like user claims that the front-end may need for displaying purposes.\nIn modern web-apps you should basically never have to retrieve access tokens directly from the authorization endpoint, because all the MSAL variations now support the authorization code flow. Therefore, the access token checkbox should always be left empty unless you need to work specifically with the implicit grant flow for backward compatibility reasons.\nConclusion\nIn summary, I\u2019d recommend the following things in most scenarios when implementing AAD authentication for your web applications:\n\nUse MSAL.\nUse scopes.\nAlways use the authorization code flow.\nUse tenant-wide admin consent sparingly and responsibly.\n\nI hope this paper has provided the reader some insight on how and why and has contributed to a better understanding of how to implement OAuth based authentication with Azure Active Directory.\n", "tags": ["Azure", "Azure directory", "Webapi"], "categories": ["Blog", "Cloud"]}
{"post_id": 33528, "title": "Developing a (Cloud) Security strategy\u00a0", "url": "https://www.luminis.eu/blog/developing-a-cloud-security-strategy/", "updated_at": "2022-09-07T11:44:18", "body": "In an increasingly digital world, security has become an important subject for every organization, regardless of its size. With more organizations adopting cloud as their main provider, the need to implement a thorough cloud security strategy has become essential to every business. An ad-hoc security control and response approach is not enough to secure your organization.\nBut what are the best practices you need to consider, when implementing (cloud) security in you organization? And what are the subjects, you need to be aware of when developing a security strategy?\nIn a series of blogs, we will show you how we at Luminis address these challenges with our clients and partners.\nWhy do I need a security strategy?\nA well-developed security strategy adds maturity to your organization, helps you detect and prevent threats that accrue now and in the future. It enables organizations to become agile in terms of security and address threats and issues faster. It also helps you prevent overspending on security. When dealing with security, it is important to remember: \u201cSpent the right amount, not more\u201d!\nThe landscape around (cloud) security changes rapidly. Once your security strategy is developed, you need to regularly review it and follow up on your findings. This is vital for your organization in order to gain maturity in security strategy and keep your workload and data secure.\nSome subjects you should consider when developing a cloud strategy:\n\nSecurity is the responsibility of the entire organization, not just the security department/team\nIt should be proactive and not just a periodic check when a project is pushed to production\nIt should be repeatable\nIt should be cost effective\nIt should minimize risk\nIt should be documented\n\nShared responsibility\nWhen developing a cloud strategy, you first need to be aware of the responsibility you have as a cloud service cust0mer. Alle major cloud service providers apply a \u201cshared responsibility\u201d model. As can be seen in the chart below (AWS shared resposibility model), this differentiation of responsibility is commonly referred to as Security \u201cof\u201d the Cloud versus Security \u201cin\u201d the Cloud.\n\nAWS shared responsibility model\nIn this model, the cloud provider is responsible for protecting the infrastructure that hosts all of the offered services. Whereas the customer is responsible for securely configuring the cloud services they select.\nWith this in mind, the next logical step is to map the existing infrastructure and architecture.\nMapping\nYou cannot secure what you cannot see or do not know! Mapping your existing infrastructure and architecture is a vital part of a healthy and solid (cloud) security strategy. Visualizing your workloads, (key) assets, business objectives, critical information and systems should give you a clear overview of the possible attack surface you need to address, what components are in use, the priorities within your roadmap, how the components are related to each other, what tools you should consider using, etc.\nOnce you have gathered all this information, it will help you create a balanced roadmap.\nAssessment\nOnce these steps are taken, our suggestion would be to start with an assessment of workloads. This helps you understand the current state of your infrastructure and architecture. This assessment will also contribute to the general security awareness within the organization, which is arguably the most important step in keeping your organization secure.\nPolicies and compliance\nIdentifying the necessary mission and business needs, laws, executive orders, guidelines, regulations, policies, standards, guidelines and regulatory compliance for your business helps you understand the current and upcoming challenges. It can serve as a guideline for making the right choices when it comes to choosing a cloud provider, tooling, (security) frameworks and methods.\nPeople, tooling and methodologies\nWhen developing and implementing a security strategy, next to tooling and methodologies, people are equally important! By involving people from your organization, your create ownership and awareness about security. Everybody should be aware of the risks and should be \u2018part\u2019 of the general security mindset.\nSome of the subjects to consider:\n\nCIO/CISO\nWell-Architected framework\nDevSecOps\nIaC\nSCA, SAST, DAST\nEncryption (on transit & at rest)\nZero trust\nOWASP\nEmployee education \u2013 recurring!\nBackup & DR\n\nRoadmap\nOnce you have gathered alle the necessary information and documentation, it is good start drawing up a roadmap. This doesn\u2019t have to be in detail. The roadmap is a living document and you should treat it as such.\nThink big, start small!\nStart with a highover view of the findings from the previous steps, for example on the policy level and then work out the details. Further down the road you start to implement controls, methodologies and tooling.\nIn the next blog of this security series we will discuss DevSecOps. What are the considerations you need to take into account and what you need to know before applying DevOps methodology for your security?\n", "tags": ["cloud", "cloud security", "security"], "categories": ["Blog", "Security"]}
{"post_id": 33506, "title": "Luminis achieves growth ambitions with Private Equity firm M80", "url": "https://www.luminis.eu/blog/luminis-achieves-growth-ambitions-with-private-equity-firm-m80/", "updated_at": "2022-09-02T10:38:31", "body": "Brussels \u2013 September 2nd, 2022 \u2013 Luminis announces today that the private equity fund M80 has acquired a majority stake in the company. The Belgian M80 offers software and technology company Luminis the opportunity to further expand its strategy and market position. Luminis becomes part of a platform that also includes XPLUS, a Belgian expert in enterprise IT architecture. M80 plans to make selective acquisitions in Europe in the coming months with the aim of building a portfolio of best-in-class digital transformation specialists.\nHans Bossenbroek, CEO Luminis about accelerating the strategy: \u201cDue to the tight labor market, organic growth of Luminis proved to be a challenge. With the help of the private equity fund M80 it is now possible to realize our growth ambitions.\u201d\nAfter acquiring XPLUS in April 2022, M80 has now taken the next step in its development of a full-service digital transformation provider in Europe. By becoming part of this group of companies, Luminis can accelerate the broadening of its services and products portfolio in the field of digital transformation.\nHans Bossenbroek will lead this new group of companies and will take responsibility for the further development of the digital transformation services. \u201cThe rapid developments in the market and technology require a multidisciplinary approach based on customer value and innovation. We strive to be the full-service provider for our customers in the field of Cloud and data.\u201d With this step, Hans hands over the baton to Jeroen Bouvrie, who will take on the task of Managing Director Luminis.\nJeroen Bouvrie, Managing Director Luminis: \u201cI am convinced that together with XPLUS and other companies that will join our platform in the future, we can support customers even better in their digital transformation. Bringing parties together fits exactly in the line of thought and strategy of Luminis; we do not look for synergy afterwards, but aim to add value for our customers as quickly as possible.\u201d\nCarl Annicq, M80 Partners: \u201cM80 aims to bring together companies with superior capabilities to save customers the frustration of working with mediocre IT vendors \u2013 we are thrilled to have convinced a quality player like Luminis to join this initiative alongside XPLUS. We look forward to further steps to strengthen our group internationally.\u201d\n\nAbout Luminis\n\nLuminis was founded in 2002 and offers customers high-quality solutions in the field of Cloud and Data. Luminis has partnerships with Amazon Web Services (AWS) and Microsoft, among others.\nIn addition, Luminis is the initiator and driving force of the IT training program Accelerate with, among others, Thales, Bosch and the Dutch Tax Authorities.\nLuminis has 150 employees and has offices in Amersfoort, Amsterdam, Rotterdam, Arnhem and Apeldoorn and provides its services to, for example, Thales, Alliander, Huuskes, BDR Thermea, bol.com and The Learning Network.\nMore information on https://www.luminis.eu\n\nAbout M80\n\nM80 Partners is the management company of M80 Capital, a private equity fund established in November 2018, investing in companies in Belgium, France, the Netherlands and Luxembourg.\nFounded by Peter Maenhout, the investment team consists of seasoned private equity professionals, but also entrepreneurs, former CEOs and digital pioneers.\nThe company focuses on growth companies in healthcare, consumer, business services and industry. The M80 team invests in companies it can help digitally transform to accelerate sales and improve operations.\nMore information on\u202fhttps://m80partners.com/\n\nAbout XPLUS\u202f\n\nFounded in 2010 by Wim Vochten, XPLUS provides consulting services focused on digital transformation through the implementation of business-critical enterprise IT architecture and digital solutions in the BeNeFraLux.\nAmong its 100+ highly experienced consultants, 75% worked earlier as a principal consultant or partner with a top 5 consultancy firm.\nXPLUS serves blue chip clients in a variety of sectors, mainly in banking, insurance, telco and retail.\nMore information on\u202fhttps://www.xplus.eu\n\nContact:\nHans Bossenbroek \u2013 CEO: +31 622801230, hans.bossenbroek@luminis.eu\nCarl Annicq \u2013 M80 Partners: +32 495 58 10 27 \u2013 carl.annicq@m80partners.com\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 33242, "title": "Invoking an AWS Lambda function during a CDK deployment", "url": "https://www.luminis.eu/blog/invoking-an-aws-lambda-function-during-a-cdk-deployment/", "updated_at": "2022-07-25T16:35:43", "body": "In general, AWS Lambda functions are triggered by some sort event. Most common use cases are an event from EventBridge, SQS, or an event created by a call to API Gateway in case you have a REST/HTTP API based on an AWS Lambda function. However, the other day I was looking for an option to execute my Lambda function immediately after it was created and/or updated while deploying my Infrastructure as Code with AWS CDK. I wanted it to work without manually executing a CLI command or calling an HTTP endpoint. It needed to be based on the CDK / CloudFormation deployment. A couple of use cases we had was triggering an import process or running a liquibase/ flyway script to populate a database.\nLooking for options\nWhile researching options, I initially looked for a method on the Function CDK Construct. I wondered if had specific lifecycle methods, but that did not seem the case. Secondly I started looking at an EventBridge rule that could listen to AWS CloudFormation events, but it seems there are almost no events are coming out of CloudFormation into EventBridge.\nAWS CDK is based on CloudFormation, so I searched within the documentation for both technologies to see what kind of hooks or lifecycle events were available. First thing I found was CloudFormation Hooks, however that only seemed related to proactive validation and automatic enforcement at the pre-deployment phase. While searching I did find a suggestion to look into using a Custom Resource and that seemed like a good solution.\nUsing Custom Resources in CDK\nWhat are Custom Resource in CDK?\nAWS CloudFormation custom resources are extension points to the provisioning engine. When CloudFormation needs to create, update or delete a custom resource, it sends a lifecycle event notification to a custom resource provider.\nWith custom resources you can hook into the provisioning engine and create a handler for the create, update and delete events. This will allow you to:\n\nCreate AWS resources that are not (yet) supported by CDK/CloudFormation\nCreate Non AWS resources (remote managed databases like ElasticCloud or MongoDB Atlas)\nPerform all kinds of other operations as you can write your own custom logic ( database seeding, database migrations, API calls, SDK calls)\n\nAWS CDK supports Custom Resources and gives you two options to implement them:\n\nLeverage the Custom Resource Provider Framework \u2013 Create your own lambda functions to handle the cloud formation events\nLeverage the Custom Resources for AWS APIs \u2013 Use the AWSCustomResource construct and provide a single AWS SDK API call\n\nUsing a custom resource provider\nSo what is a custom resource provider in CDK / CloudFormation?\nWhen CloudFormation needs to create, update or delete a custom resource, it sends a lifecycle event notification to a custom resource provider. The provider handles the event (e.g. creates a resource) and sends back a response to CloudFormation. Providers are implemented through AWS Lambda functions that are triggered by the provider framework in response to lifecycle events.\nThe CDK documentation on Custom Resources has some extensive documentation on implementing such a Lambda function as a custom provider. At the minimum, you will need to define the onEvent handler, which is invoked by the provider framework for all resource lifecycle events (create, update and delete) and you need to return a result which is then submitted to CloudFormation.\nThe framework offers a high-level API which makes it easier to implement robust and powerful custom resources and includes the following capabilities:\n\nHandles responses to AWS CloudFormation and protects against blocked deployments\nValidates handler return values to help with correct handler implementation\nSupports asynchronous handlers to enable operations that require a long waiting period for a resource, which can exceed the AWS Lambda timeout\nImplements default behavior for physical resource IDs.\n\nThe following code shows how the Provider construct is used in conjunction with a CustomResource and a user-provided AWS Lambda function which implements the actual handler.\nFunction onEvent;\r\nFunction isComplete;\r\nRole myRole;\r\n\r\nProvider myProvider = Provider.Builder.create(this, \"MyProvider\")\r\n  .onEventHandler(onEvent)\r\n  .isCompleteHandler(isComplete) // optional async \"waiter\"\r\n  .logRetention(RetentionDays.ONE_DAY) // default is INFINITE\r\n  .role(myRole)\r\n  .build();\r\n\r\nCustomResource.Builder.create(this, \"Resource1\").serviceToken(myProvider.getServiceToken()).build();\r\n\nWhen writing such an eventHandler you can use the AWS Lambda PowerTools for Java Custom Resources utility library.\nA skeleton of such a function when used with Lambda PowerTools will look like:\nimport com.amazonaws.services.lambda.runtime.Context;\r\nimport com.amazonaws.services.lambda.runtime.events.CloudFormationCustomResourceEvent;\r\nimport software.amazon.lambda.powertools.cloudformation.AbstractCustomResourceHandler;\r\nimport software.amazon.lambda.powertools.cloudformation.Response;\r\n\r\npublic class ProvisionEventHandler extends AbstractCustomResourceHandler {\r\n\r\n    @Override\r\n    protected Response create(CloudFormationCustomResourceEvent createEvent, Context context) {\r\n        doProvisioning();\r\n        return Response.success();\r\n    }\r\n\r\n    @Override\r\n    protected Response update(CloudFormationCustomResourceEvent updateEvent, Context context) {\r\n        return null;\r\n    }\r\n\r\n    @Override\r\n    protected Response delete(CloudFormationCustomResourceEvent deleteEvent, Context context) {\r\n        return null;\r\n    }\r\n}\r\n\r\n\nAs you can see from the code snippet, Lambda power tools adds a level of abstraction for you so you don\u2019t have to handle events and offers direct methods for create, update and delete of a resource. The solution itself looks very powerful and flexible for a lot of different use cases. You can add multiple operations inside such a block which makes it a powerful solution for complex operations. Creating the code for such a Lambda function looks straight forward, but still it\u2019s quite a bit of work and more code to maintain, so after reading about the AWSCustomResource construct, I had the gut feeling it was all I needed and it looks much simpler to achieve my goal.\nUsing the AWSCustomResource construct\nSo what does the AWSCustomResource construct do?\nDefines a custom resource that is materialized using specific AWS API calls. These calls are created using a singleton Lambda function.\nYou can specify exactly which calls are invoked for the \u2018CREATE\u2019, \u2018UPDATE\u2019 and \u2018DELETE\u2019 life cycle events.\nThat sounds pretty cool! Besides the AWS CDK code it sounds like we don\u2019t have to write any code to be able to leverage this. So we don\u2019t have to write the Lambda function or manage the IAM policies. All we need to do is provide de Sdk call. The rest seems to be handled by the construct. Sweet!\nLet\u2019s first define the Lambda Function that will run our own business logic and needs to be triggered during the deployment.\nFunction function = new Function(this, \"java-based-function\", FunctionProps.builder()\r\n            .runtime(Runtime.JAVA_11)\r\n            .code(Code.fromAsset(\"../app/target/app.jar\"))\r\n            .handler(\"com.jeroenreijn.aws.samples.lambdatrigger.FunctionHandler\")\r\n            .memorySize(512)\r\n            .timeout(Duration.seconds(10))\r\n            .logRetention(RetentionDays.ONE_WEEK)\r\n            .build());\r\n\nNow that our business logic function is defined, we will need to define which AWS SDK call we want to make. In our case we want to invoke a Lambda function from inside our custom resource. Let\u2019s create the SDK call to the AWS Lambda service and provide our parameters.\nPhysicalResourceId physicalResourceId = PhysicalResourceId.of(\r\n        LocalDateTime\r\n                .now()\r\n                .toString()\r\n);\r\n\r\nAwsSdkCall lambdaExecutionCall = AwsSdkCall.builder()\r\n        .service(\"Lambda\")\r\n        .action(\"invoke\")\r\n        .physicalResourceId(physicalResourceId)\r\n        .parameters(Map.of(\r\n                \"FunctionName\", function.getFunctionName(),\r\n                \"InvocationType\", \"Event\",\r\n                \"Payload\",\r\n                \"{\" + \"\\\"body\\\":\\\"{\\\\\\\"message\\\\\\\": \\\\\\\"Hello World\\\\\\\"}\\\"\" + \"}\"\r\n        ))\r\n        .build();\r\n\nIf we look at the above snippet, we can see an example of how to invoke a specific lambda function by name. The Payload parameter is optional, so if your function is not expecting a payload you can leave that out.\nWith the AWS SDK call in place we wil need to create our AwsCustomResource construct. Since we want our function logic to happen when we create our update our CDK stack we will need to add our AWS SDK call to the onCreate and onUpdate handlers.\nLast but not least, to follow the least privilege principle we make sure that our Custom Resource can only call our specific function by adding it to the Policy.\nAwsCustomResourcePolicy policy = AwsCustomResourcePolicy.fromStatements(List.of(\r\n        PolicyStatement.Builder\r\n                .create()\r\n                .actions(List.of(\"lambda:InvokeFunction\"))\r\n                .effect(Effect.ALLOW)\r\n                .resources(List.of(function.getFunctionArn()))\r\n                .build()));\r\n\r\nAwsCustomResource lambdaTriggerResource = AwsCustomResource.Builder.create(this, \"custom-resource\")\r\n        .logRetention(RetentionDays.FIVE_DAYS)\r\n        .onCreate(lambdaExecutionCall)\r\n        .onUpdate(lambdaExecutionCall)\r\n        .timeout(Duration.minutes(1))\r\n        .policy(policy)\r\n        .installLatestAwsSdk(false)\r\n        .build();\r\n\nTo make sure our business function is deployed before making the call we can add an explicit dependency. By doing so, CDK / CloudFormation will know there is a specific order in which it needs to create our resources.\nlambdaTriggerResource.getNode().addDependency(function);\nWhen you deploy the above solution AWS CDK / CloudFormation will actually create a second Lambda function for us containing the SDK call to the function that holds our actual business logic. That was exactly what I was trying to do and CDK seems to make it really simple to implement this.\n\nSummary\nAs you can see Custom Resources in AWS CDK are quite powerful. It gives you a lot of flexibility and when you need more than a single API call you can leverage the Provider framework. For single API calls using the AwsCustomResource is quite straightforward and it allowed me to invoke my lambda function on deployment.\n", "tags": ["aws", "aws cdk", "lambda", "serverless"], "categories": ["Blog", "Cloud"]}
{"post_id": 33120, "title": "AWS Lambda Provisioned Concurrency AutoScaling with AWS CDK", "url": "https://www.luminis.eu/blog/aws-lambda-provisioned-concurrency-autoscaling-with-aws-cdk/", "updated_at": "2022-07-19T09:26:38", "body": "A couple of weeks ago I was working on some AWS CDK based code and I was trying to figure out how to configure auto-scaling for the provisioned concurrency configuration of an AWS Lambda function. We wanted to run some performance tests on our service and were wondering how scaling provisioned concurrency would impact our overall latency. We tried with the default configuration but also wanted to experiment with a bit more aggressive scaling policy so we had to provide our own metric configuration. In this post, I will explain what provisioned concurrency is and how to set up an auto-scaling configuration for it using AWS CDK. We\u2019ll be looking at using predefined metric configurations, but also how to do it with a custom metric.\nWhat is provisioned concurrency and how does it relate to latency and cost?\nProvisioned concurrency is a feature for AWS Lambda that got introduced in 2019. It helps you keep one or more AWS lambda function instances in a \u2018warm\u2019 state in case of sudden bursts of traffic. This is particularly useful if you have functions that take a bit of time to initialize while traffic is increasing and you want to keep the latency as low as possible. Without provisioned concurrency (and without autoscaling) you might end up having a cold starts when traffic increases, which will result in a high latency for your end consumer. This can happen with all languages, but in particular with Java or .Net based applications. As you\u2019re paying for these \u2018warm\u2019 execution environments you don\u2019t want to have too many of them running when they\u2019re not in use. To prevent this there are two ways of adjusting your concurrency setting:\n\nScheduled (very useful if you have very predictable load patterns)\nTarget tracking (scale based on the increase or decrease of load)\n\nWhen you want to calculate the required concurrency you can use the following formula Transactions per second * Function Duration (in seconds). The shorter your function runs, the less concurrency you will need. Sometimes you have unpredictable traffic patterns, so a target tracking auto-scaling strategy is the best option. With the target tracking policy attached to application auto-scaling it will make sure that the number of concurrently available lambdas will stay in line with the number of requests.\n\nNow let\u2019s see what this looks like from an AWS CDK perspective.\nConfiguring our Infrastructure as Code with AWS CDK\nNow in CDK, we will need to define and configure our Lambda function. To do so you can use the Function construct and reference the jar that contains your actual code.\nFunction javaBasedFunction =\r\nnew Function(this, \"java-based-function-id\", FunctionProps.builder()\r\n.runtime(Runtime.JAVA_11)\r\n.code(Code.fromAsset(\"../app/target/java-app.jar\"))\r\n.handler(\"com.jeroenreijn.aws.samples.scaling.FunctionHandler\")\r\n.memorySize(1024)\r\n.timeout(Duration.seconds(10))\r\n.logRetention(RetentionDays.ONE_WEEK)\r\n.build());\r\n\nProvisioned concurrency for a lambda function can only be configured if the lambda function has an alias or version. In this example, we will create an alias for our function and map that alias to the latest version of our function. When deploying the CDK stack a new version will be created and the alias will be referenced to the latest version. For our function, we will start with a single provisioned lambda function.\nAlias alias = Alias.Builder.create(this, \"auto-scaling-lambda-alias-id\")\r\n.aliasName(\"auto-scaling-lambda-alias\")\r\n.provisionedConcurrentExecutions(1)\r\n.version(function.getCurrentVersion())\r\n.build();\nSo far so good for defining our function and function alias.\nRegistering Lambda functions as scalable targets with Application Auto Scaling\nNow that we have everything in place for our Lambda function we will need to register our function as a scalable target.\nThrough the AWS CDK, there are two ways of configuring autoscaling for the provisioned concurrency configuration of our function.\n\nConfiguring scaling options directly on the alias\nConfiguring scaling options via application autoscaling\n\nSo let\u2019s explore both options.\nConfiguring scaling options directly on the alias\nThe Function Alias has a short-hand method for configuring provisioned concurrency scaling. You can do this by calling the .addAutoScaling method on the Alias.\nAutoScalingOptions autoScalingOptions = AutoScalingOptions.builder()\r\n.minCapacity(0)\r\n.maxCapacity(10)\r\n.build();\r\n\r\nIScalableFunctionAttribute iScalableFunctionAttribute =\r\nalias.addAutoScaling(autoScalingOptions);\r\n\r\niScalableFunctionAttribute.scaleOnUtilization(\r\nUtilizationScalingOptions\r\n.builder()\r\n.utilizationTarget(0.7)\r\n.build()\r\n);\nAdding a scaling strategy on the alias is pretty straight forward. You can use both scaling on utilization and scale by schedule. However, it does not seem to allow for scaling by a custom metric configuration.\nConfiguring scaling options via application autoscaling\nIn CDK we can leverage the constructs available for the application autoscaling service. In our case, we will be primarily using the ScalableTarget construct to define the target that we want to scale. It requires a couple of configuration options like; min and max capacity, the id of the resource we want to scale, and the dimension we want to scale. In Java this looks like this:\nScalableTarget scalableTarget =\r\nScalableTarget.Builder.create(this, \"auto-scaling-lambda-target-id\")\r\n.serviceNamespace(ServiceNamespace.LAMBDA)\r\n.minCapacity(1)\r\n.maxCapacity(10)\r\n.resourceId(String.format(\"function:%s:%s\", function.getFunctionName(), alias.getAliasName()))\r\n.scalableDimension(\"lambda:function:ProvisionedConcurrency\")\r\n.build();\nNow that we\u2019ve defined the target we still need to provide the scaling strategy. In our case, we want to scale on the utilization of our provisioned concurrency setting. CDK, by default, has a predefined setting to scale on the ProvisionedConcurrencyUtilization metric. Besides the metric, we also need to specify a target value for which application autoscaling will trigger.\nscalableTarget\r\n.scaleToTrackMetric(\"PCU\", BasicTargetTrackingScalingPolicyProps.builder()\r\n.predefinedMetric(\r\nPredefinedMetric.LAMBDA_PROVISIONED_CONCURRENCY_UTILIZATION\r\n)\r\n.targetValue(0.7)\r\n.build()\r\n);\nThe generated scaling policy will result in two Cloudwatch alarms:\n\nAn alarm for scaling up that requires 3 data points over 1 minute each\nAn alarm for scaling down that requires 15 data points over 15 minutes\n\nBoth of these alarms will use the ProvisionedConcurrencyUtilization metric in combination with the Average statistic by default, which means that on average over these 3 data points (minutes) the value needs to be above 0.7 to trigger a scale-up.\n\nSometimes you might have sudden spikes which only happen in the first minute and not in the second or third, which will result in no scaling action. Depending on your use case this could be a problem. An alternative could be to make this a bit more of an aggressive scaling policy and use the Maximum statistic, which causes the alarm to trigger if the maximum utilization was above 0.7 for only one out of three data points. To do that we need to define the metric ourselves and specify the Maximum statistic.\nMetric maximumMetric = Metric.Builder.create()\r\n.namespace(\"AWS/Lambda\")\r\n.metricName(\"ProvisionedConcurrencyUtilization\")\r\n.statistic(\"Maximum\")\r\n.dimensionsMap(\r\nMap.of(\r\n\"FunctionName\", function.getFunctionName(),\r\n\"Resource\", function.getFunctionName() + \":\" + alias.getAliasName()))\r\n.unit(Unit.COUNT)\r\n.period(Duration.minutes(1))\r\n.build();\nTake a good look at the .dimensionsMap method as I got this wrong the first time and my scale up and down did not happen. It\u2019s important to get these values right. To be able to use the custom metric we can specify it on our TargetTracking scaling policy.\nscalableTarget\r\n.scaleToTrackMetric(\"PCU\", BasicTargetTrackingScalingPolicyProps.builder()\r\n.customMetric(maximumMetric)\r\n.targetValue(0.7)\r\n.build()\r\n);\nTesting the application autoscaling configuration\nIt\u2019s important to validate that we\u2019ve set up everything in the right way. I can tell you from experience when it comes to writing CDK code you sometimes miss predefined static values for specific services that contain the correct format and pattern for instance like with the .dimensionsMap as you can see above. If you\u2019re using the AWS console, the UI will take care of these dimensions for you, but when you manually have to specify them you need to make sure to validate that they contain the correct values for application autoscaling to do its job.\nWe can test autoscaling on our custom metric with a simple load testing tool like Artillery or Apache Benchmark. In this example, I\u2019m using Apache Benchmark as it\u2019s quite simple to run from the command line and I already had it installed on my machine.\n$ ab -n 20000 -c 20 https://someid.execute-api.eu-west-1.amazonaws.com/test/\nIf everything is set up correctly the scaling policy should trigger application autoscaling to scale up the number of provisioned concurrent lambda functions.\nValidating application autoscaling\nYou should be able to see the value change in the AWS console under the Provisioned concurrency configuration, but I always prefer to use the AWS command-line interface to see if it\u2019s doing its job. To see the scaling activities for the AWS Lambda functions you can call the application autoscaling service and ask for scaling activities that happen within the Lambda namespace.\n$ aws application-autoscaling describe-scaling-activities --service-namespace=lambda\nNow if you did that correctly it should result in a response that describes all the scaling activities going on for your AWS Lambda functions. You can see specify --resource-ids=someid if you only want to see the activities for a specific function.\n{\r\n\"ScalingActivities\": [\r\n{\r\n    \"ActivityId\": \"52873ef2-2a0d-4af4-8f58-62467cd7b4ee\",\r\n    \"ServiceNamespace\": \"lambda\",\r\n    \"ResourceId\": \"function:AwsApigatewayLambdaStack-HelloWorldHandler30C22324-E6HowYd9st0b:hello-world-lambda-alias\",\r\n    \"ScalableDimension\": \"lambda:function:ProvisionedConcurrency\",\r\n    \"Description\": \"Setting desired concurrency to 5.\",\r\n    \"Cause\": \"monitor alarm TargetTracking-function:AwsApigatewayLambdaStack-HelloWorldHandler30C22324-E6HowYd9st0b:hello-world-lambda-alias-AlarmHigh-34b6d7bb-2ab4-4a9b-9d80-7110edcfe768     in state ALARM \r\ntriggered policy AwsApigatewayLambdaStackhelloworldlambdaasgPCU388B92B5\",\r\n    \"StartTime\": \"2022-07-02T11:57:31.400000+02:00\",\r\n    \"EndTime\": \"2022-07-02T11:59:13.059000+02:00\",\r\n    \"StatusCode\": \"Successful\",\r\n    \"StatusMessage\": \"Successfully set desired concurrency to 5. Change successfully fulfilled by lambda.\"\r\n},\r\n....\r\n]\r\n}\nIf everything is working as expected you should see the currency value change based on the increase or decrease of utilization.\nSome lessons learned while testing\nWe\u2019ve tested our function and have seen that the provisioned concurrency configuration for our lambda is scaling properly, so now is a good time to talk about some gotchas. While testing this it\u2019s important to keep in mind that if you run a very big load test against your lambda and your test is finished, you need to make sure that you either manually scale down in your dev/test environment or send some slow traffic to your services over a period of time. While testing this myself I noticed in our dev/test environment that after we did our tests, the provisioned concurrency number was not going down. As we\u2019ve seen in a previous section of the article, scaling up and down is based on a CloudWatch Alarm. The alarms, however, are by default configured with \u201cTreat missing data as missing\u201d, which means that if there is no traffic no alarms will be triggered and you can have quite some warm functions in your account which quickly starts to add up in unnecessary costs.\nSummary\nIn this post we looked at scaling the provisioned concurrency settings for lambda functions by means of configuring them via AWS CDK. As you\u2019ve seen it\u2019s relatively simple to configure and it can help you save cost and keep latency down.\n", "tags": ["aws", "aws cdk", "java", "lambda", "serverless"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 33036, "title": "The Amplify Series, Part 5: Uploading and retrieving images with Amplify Storage", "url": "https://www.luminis.eu/blog/cloud-en/the-amplify-series-part-5-uploading-and-retrieving-images-with-amplify-storage/", "updated_at": "2023-05-08T12:58:50", "body": "With our application in place, it is now time to start adding more functionality using some of the other Amplify categories. In this article, we will be adding Amplify Storage to upload and retrieve images from AWS in just a few steps.\nWe will start off by adding the Amplify Storage category to our project. We will follow up by using the Storage component to be able to upload and list all of our images. Finally, we will create some UI that will make use of this functionality. At the end of this article, you will have a better understanding of Amplify Storage and can use it in any scenario regarding file uploads and retrieval.\nAdding Amplify Storage to our project\nWe will continue in the repository where we left off in the last blog post. From this point, we will run amplify add storage with the following options:\n\nService: content.\nFriendly name: amplifyappimages.\nBucket name: amplifyappimages\nAccess: create, update, read, and delete, for authorized users only.\n\nThe Amplify CLI output will look similar to this:\n\r\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add storage\r\n\r\n? Select from one of the below mentioned services: Content (Images, audio, video, etc.)\r\n\r\n\u2714 Provide a friendly name for your resource that will be used to label this category in the project: \u00b7 amplifyappimages\r\n\r\n\u2714 Provide bucket name: \u00b7 amplifyappimages\r\n\r\n\u2714 Who should have access: \u00b7 Auth users only\r\n\r\n\u2714 What kind of access do you want for Authenticated users? \u00b7 create/update, read, delete\r\n\r\n\u2714 Do you want to add a Lambda Trigger for your S3 Bucket? (y/N) \u00b7 no\r\n\r\n\u2705 Successfully added resource amplifyappimages locally\r\n\r\n\u26a0\ufe0f If a user is part of a user pool group, run \"amplify update storage\" to enable IAM group policies for CRUD operations\r\n\u2705 Some next steps:\r\n\"amplify push\" builds all of your local backend resources and provisions them in the cloud\r\n\r\n\"amplify publish\" builds all of your local backend and front-end resources (if you added hosting category) and provisions them in the cloud\r\n\nWe want users to be able to only see their own uploads. Therefore it is important that we select that only authenticated users can make use of this resource and that we give them all permissions so that they can upload, view and delete images. This is yet another useful example of how the Amplify Auth category automatically links with other categories to provide authentication and authorization functionality.\u00a0\nThis command adds some changes to our repo. In the next step, we will start using the new\u00a0category.\nWe will separate this functionality into three components:\n\nImage uploader: A component responsible for uploading the image and giving the user feedback about the upload.\n\n\nImage album: A component responsible for listing all uploaded images.\n\n\nImage viewer: A component that will show only one image in a modal.\n\nAdding an image overview page\nWe will first start by adding a new Angular component that will be used to show all images our users have uploaded::\nng generate component components/categories/storage\nThis will generate the expected files for our component. We will then link up routing to be able to render this component. This is all pretty standard Angular stuff that we have done in the previous blog, please refer to this commit for details.\nAdding image upload functionality\nWe will now add a component that will contain the functionality to upload an image:\nng generate component components/categories/storage/image-upload\nThis will generate the expected Angular files. Inside our storage.component.html we will make sure to add the newly generated image-upload:\n<app-image-upload></app-image-upload>\r\n\r\n\nInside the image-upload.component.html, we will add:\n<input\r\n\u00a0 type=\"file\"\r\n\u00a0 id=\"imageUpload\"\r\n\u00a0 name=\"imageUpload\"\r\n\u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 (change)=\"imageSelected($event)\"\r\n/>\nThis will give us an input that we can use to select images from our device. We have to add logic to react to whenever a user selects an image in our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 selectedFile: File | undefined = undefined;\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {}\r\n\r\n\u00a0 imageSelected = (e: Event) => {\r\n\u00a0 \u00a0 const input = e.target as HTMLInputElement;\r\n\r\n\u00a0 \u00a0 if (!input.files?.length) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 this.selectedFile = input.files[0];\r\n\u00a0 };\r\n}\nWith this change we set the variable selectedFile to contain the contents of the file we selected. Now that we have this, we can add an upload button that uploads the selected image to the AWS cloud, which we previously configured.\u00a0\nWe will start off by adding a button in image-upload.component.html:\n<input\r\n\u00a0 type=\"file\"\r\n\u00a0 id=\"imageUpload\"\r\n\u00a0 name=\"imageUpload\"\r\n\u00a0 accept=\"image/png, image/jpeg\"\r\n\u00a0 (change)=\"imageSelected($event)\"\r\n/>\r\n\r\n<button class=\"aws-button\" (click)=\"uploadImage()\"> <!---NEW!-->\r\n\u00a0 Upload\r\n</button>\nOnce this button is clicked the uploadImage function is called. We will define this function in the following way in our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify'; // <--- add this\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 selectedFile: File | undefined = undefined;\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {}\r\n\r\n\u00a0 //Add this function!\r\n\u00a0 uploadImage = async () => {\r\n\u00a0 \u00a0 if (!this.selectedFile) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 await Storage.put(this.selectedFile.name, this.selectedFile, {\r\n\u00a0 \u00a0 \u00a0 \u00a0 contentType: 'image/*',\r\n\u00a0 \u00a0 \u00a0 \u00a0 level: 'private'\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 } catch (error) {\r\n\u00a0 \u00a0 \u00a0 console.log('Error uploading file: ', error);\r\n\u00a0 \u00a0 }\r\n\u00a0 };\r\n\r\n\u00a0 //other code\r\n}\nWe first import the Storage component from aws-amplify. We then define the uploadImage function that is used by our new button. This function uses the Amplify Storage component and calls the put method, which gets the file name, the file contents, and some properties.\nIn this case, the options include the type of file we want to accept and the authorization level. Setting the level to private means that only the user that uploaded this image will have access to it.\u00a0\nOnce we have this in place we can select an image with our input and upload it using the new button. While we don\u2019t see any feedback yet, if we go into our AWS console to S3 and search for our bucket, we should see that there is a new directory called \u201cprivate\u201d which contains a directory with the cognitoID of the user we used to upload the image. Inside this directory we can see the image that was uploaded:\n\nAdding file upload progress feedback\nIn this step we want to show the user some feedback about the image upload process. We will start by adding an enum for the possible upload states and will set a variable in our component to have the correct state given the situation of the upload process. On our UI we will then show some text based on this state.\n\u00a0\nWe will first make these changes to our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify';\r\n\r\n//Add this!\r\nenum UploadState {\r\n\u00a0 IDLE,\r\n\u00a0 UPLOADING,\r\n\u00a0 UPLOAD_COMPLETE,\r\n\u00a0 ERROR\r\n}\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 selectedFile: File | undefined = undefined;\r\n\u00a0 //Add these 3:\r\n\u00a0 uploadStates = UploadState;\r\n\u00a0 uploadState: UploadState = UploadState.IDLE;\r\n\u00a0 progressText: string = '';\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {}\r\n\r\n\u00a0 uploadImage = async () => {\r\n\u00a0 \u00a0 this.uploadState = UploadState.UPLOADING; // <--- Add this\r\n\u00a0 \u00a0 if (!this.selectedFile) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 await Storage.put(this.selectedFile.name, this.selectedFile, {\r\n\u00a0 \u00a0 \u00a0 \u00a0 contentType: 'image/*',\r\n\u00a0 \u00a0 \u00a0 \u00a0 level: 'private',\r\n\u00a0 \u00a0 \u00a0 \u00a0 progressCallback: this.progressCallback // <---- Add this\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 \u00a0 this.uploadState = UploadState.UPLOAD_COMPLETE; // <--- Add this\r\n\u00a0 \u00a0 } catch (error) {\r\n\u00a0 \u00a0 \u00a0 this.uploadState = UploadState.ERROR; // <---- Add this\r\n\u00a0 \u00a0 \u00a0 console.log('Error uploading file: ', error);\r\n\u00a0 \u00a0 }\r\n\u00a0 };\r\n\r\n\u00a0 //Add this function!\r\n\u00a0 progressCallback = (progress: any) => {\r\n\u00a0 \u00a0 this.progressText = `Uploaded: ${(progress.loaded / progress.total) *\r\n\u00a0 \u00a0 \u00a0 100} %`;\r\n\u00a0 };\r\n\r\n\u00a0 imageSelected = (e: Event) => {\r\n\u00a0 \u00a0 this.uploadState = UploadState.IDLE; // <---- Add this\r\n\u00a0 \u00a0 const input = e.target as HTMLInputElement;\r\n\r\n\u00a0 \u00a0 if (!input.files?.length) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 this.selectedFile = input.files[0];\r\n\u00a0 };\r\n}\nThe upload state will start in IDLE, switch to UPLOADING and will either go into UPLOADING_COMPLETE or ERROR depending on how the call to Amplify Storage goes. The Storage.put function also allows for the definition of a progressCallback function in order to track the progress of the upload.\u00a0\nOnce these are set in place, we add the following to our image-upload.component.html to be able to see the feedback:\n<!---Existing code-->\r\n\r\n<p\r\n\u00a0 *ngIf=\"\r\n\u00a0 \u00a0 uploadState === uploadStates.UPLOADING ||\r\n\u00a0 \u00a0 uploadState === uploadStates.UPLOAD_COMPLETE\r\n\u00a0 \"\r\n>\r\n\u00a0 {{ progressText }}\r\n</p>\r\n<p *ngIf=\"uploadState === uploadStates.UPLOAD_COMPLETE\">\r\n\u00a0 Upload complete!\r\n</p>\r\n\r\n<p class=\"error-text\" *ngIf=\"uploadState === uploadStates.ERROR\">\r\n\u00a0 Something went wrong with the file upload\r\n</p>\nWe will now get some feedback when uploading images.\nThe complete set of changes for this step can be found in this commit.\nAdding an image overview\nNow that we can upload images, we want to view all our uploaded images. We start by creating an image-album component:\nng generate component components/categories/storage/image-album\nNow we are going to update the image-album.component.ts to contain logic to get all of the images for our user:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify';\r\n\r\nexport interface Image {\r\n\u00a0 key: string;\r\n\u00a0 url: string;\r\n}\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-album',\r\n\u00a0 templateUrl: './image-album.component.html',\r\n\u00a0 styleUrls: ['./image-album.component.css']\r\n})\r\nexport class ImageAlbumComponent implements OnInit {\r\n\u00a0 images: Image[] = [];\r\n\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 ngOnInit(): void {\r\n\u00a0 \u00a0 this.getAllImages();\r\n\u00a0 }\r\n\r\n\u00a0 getAllImages = () => {\r\n\u00a0 \u00a0 Storage.list('', { level: 'private' })\r\n\u00a0 \u00a0 \u00a0 .then(result => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 result.forEach(async imageObject => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const objectKey = imageObject.key;\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (objectKey !== undefined) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const signedURL = await Storage.get(objectKey, {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 level: 'private',\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 download: false\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.images.push({ key: objectKey, url: signedURL });\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 \u00a0 })\r\n\u00a0 \u00a0 \u00a0 .catch(err => console.log(err));\r\n\u00a0 };\r\n}\nSome important things to note here is to set the levels to private in both the Storage.list and Storage.get functions. The reason we need to use two functions is because the Storage.list only gives us some information about the objects, such as the unique key in S3. However it does not give use a URL with which we can actually view the object.\nFor this we need a signed URL. Because we are logged in and we call the Storage.get function with the private level, the Amplify Storage component will check which user is logged in and verify that this user may view this object and create a signed url.\nWe also created an Image interface which we will use to contain the image key and signed url retrieved from S3. We will store these in an array on the component and loop through these to show the images in image-album.component.html:\n<div class=\"container\">\r\n\u00a0 <div class=\"row\" *ngFor=\"let image of images\">\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"></div>\r\n\u00a0 \u00a0 <div class=\"col-lg-8\">\r\n\u00a0 \u00a0 \u00a0 <img class=\"album-image\" src=\"{{ image.url }}\" />\r\n\u00a0 \u00a0 </div>\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"></div>\r\n\u00a0 </div>\r\n</div>\nThis simply loops through the imageUrls and shows an image for each. We also add a bit of styling to add the width and a bit of margin in our image-album.component.css:\n.album-image {\r\n\u00a0 width: 100%;\r\n\u00a0 margin: 8px;\r\n}\nThis results in the images being viewed on the page like this:\n\nFor all the code changes, including the wiring the new component, see this commit.\nAdding image removal functionality\nIn this section we are going to add a button to remove images that we have uploaded. We will first add a removeImage function in our image-album.component.ts:\nremoveImage = async (key: string) => {\r\n\u00a0 \u00a0 await Storage.remove(key, { level: 'private' });\r\n\u00a0 \u00a0 this.images = [];\r\n\u00a0 \u00a0 this.getAllImages();\r\n\u00a0 };\nWe reassign the images array here to force Angular to rerender the image-album component so that we can see that our image as indeed been removed. We then update our image-album.component.html to contain an \u201cx\u201d button for removal next to each image:\n<div class=\"container\">\r\n\u00a0 <div class=\"row\" *ngFor=\"let image of images\">\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"></div>\r\n\u00a0 \u00a0 <div class=\"col-lg-8\">\r\n\u00a0 \u00a0 \u00a0 <img class=\"album-image\" src=\"{{ image.url }}\" />\r\n\u00a0 \u00a0 </div>\r\n\u00a0 \u00a0 <div class=\"col-lg-2\"> <!--NEW! -->\r\n\u00a0 \u00a0 \u00a0 <button class=\"remove-image-button\" (click)=\"removeImage(image.key)\">\r\n\u00a0 \u00a0 \u00a0 \u00a0 x\r\n\u00a0 \u00a0 \u00a0 </button>\r\n\u00a0 \u00a0 </div>\r\n\u00a0 </div>\r\n</div>\nAnd we add a bit of styling to the button:\n.remove-image-button {\r\n\u00a0 border: 0px;\r\n\u00a0 background: transparent;\r\n}\nAnd now we have a button that allows us to remove images, which will look something like this:\n\nAs always, the changes for this step can be found in this commit.\nSetting the maximum number of uploads per user\nThe final step we want to add for our image upload is to add a maximum number of images per user so that a user does not flood our S3 buckets. We don\u2019t have a property for this in the Amplify Storage component, so we need to build this ourselves. Also, note that the solution here is only a frontend check, there is no actual check on the bucket itself per user.\u00a0\nWe make the following changes to our image-upload.component.ts:\nimport { Component, OnInit } from '@angular/core';\r\nimport { Storage } from 'aws-amplify';\r\n\r\nconst MAX_NUMBER_OF_IMAGES_PER_USER: number = 10;\r\n\r\nenum UploadState {\r\n\u00a0 IDLE,\r\n\u00a0 UPLOADING,\r\n\u00a0 UPLOAD_COMPLETE,\r\n\u00a0 ERROR,\r\n\u00a0 MAX_REACHED // <--- add this!\r\n}\r\n\r\n@Component({\r\n\u00a0 selector: 'app-image-upload',\r\n\u00a0 templateUrl: './image-upload.component.html',\r\n\u00a0 styleUrls: ['./image-upload.component.css']\r\n})\r\nexport class ImageUploadComponent implements OnInit {\r\n\u00a0 //Existing code...\r\n\r\n\u00a0 uploadImage = async () => {\r\n\u00a0 \u00a0 this.uploadState = UploadState.UPLOADING;\r\n\u00a0 \u00a0 if (!this.selectedFile) {\r\n\u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 try {\r\n\u00a0 \u00a0 \u00a0 //Add this part!\r\n\u00a0 \u00a0 \u00a0 const userImages = await Storage.list('', { level: 'private' });\r\n\u00a0 \u00a0 \u00a0 if (userImages.length >= MAX_NUMBER_OF_IMAGES_PER_USER) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 this.uploadState = UploadState.MAX_REACHED;\r\n\u00a0 \u00a0 \u00a0 \u00a0 return;\r\n\u00a0 \u00a0 \u00a0 }\r\n\r\n \u00a0 \u00a0 //Existing code...\r\n\u00a0 \u00a0 } catch (error) {\r\n\u00a0 \u00a0 \u00a0 //Existing code ...\r\n\u00a0 \u00a0 }\r\n\u00a0 };\r\n\r\n\u00a0 //Existing code...\r\n}\nWe first add a new state called MAX_REACHED which we will set when this is the case. Inside the uploadImage function we will add a new check where we will first list the objects of the user and check if the length is equal or greater than the max. If this is the case we set the upload state to MAX_REACHED and stop the upload.\nIn our image-upload.component.html, we add a new error message that is shown in this upload state:\n<!---Existing code...-->\r\n\r\n<p class=\"error-text\" *ngIf=\"uploadState === uploadStates.MAX_REACHED\">\r\n\u00a0 Reached maximum amount of uploads. Please remove an image before trying again.\r\n</p>\nNow when we try to add the 11th image, we get the following message:\n\nChanges for this step can be found here.\nUp next: AI and ML with Amplify Predictions\nIn this blog, we have used the Amplify Storage category to update our existing application to contain image upload, viewing, and removal functionality per user. In the next article, we will look at using the power of AI and Machine Learning with Amplify Predictions.\n\u00a0\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30665, "title": "Pull requests for the designer: How to implement the workflow for a multi-platform application on Azure DevOps", "url": "https://www.luminis.eu/blog/development-en/pull-requests-for-the-designer-how-to-implement-the-workflow-for-a-multi-platform-application-on-azure-devops/", "updated_at": "2023-01-27T12:41:32", "body": "Enabling designers to review design implementations early on in the development process is a great way to improve the flow of getting work done in a project. Plus, it increases the collaboration of designers, engineers, testers, and business stakeholders. In this post, I\u2019ll explain how to technically implement this workflow, using a typical multi-platform application project as an example.\nThis blog post is the technical counterpart to the article explaining the conceptual side of the pull-requests-for-the-designer equation. If you want to know how to improve user experience and project flow by involving designers early in the review process, go read that one first. Then, come back here and learn how to set up the workflow yourself. Onto the implementation details!\nProject and workflow setup\nWhile we will use a Xamarin-powered multi-platform application \u2014 specifically: the FlowSuite app we built for our client Bronkhorst \u2014 and Azure DevOps as an example, the principles of the project and workflow setup can be applied to basically any user-facing software application setup.\nThe idea is simple: developers work on a single codebase that contains generic business and data binding logic for all platforms (in our case: iOS, Android, and Windows), and platform-specific code (mostly view-related). This enables us to build several functionally equivalent applications with the minimum amount of code duplication and effort.\nAt the project workflow level, a designer user interface designs and user experience descriptions aimed at the specific platforms, related to user stories. Engineers get to work on the implementation, trying their hardest to follow the specifications. Once ready for review, they submit their code, create a pull request for it, and wait for review. At that point, a build pipeline picks up the changes, builds all application variants and the implemented design is ready for review by the designer, who can leave comments, optionally ask for rework, and finally approve the change.\n\nSetting up the build pipeline\nTo enable the described workflow, we have set up an Azure Pipeline in Azure DevOps, tied to our trunk-based code repository. For every commit on a pull-request, release or main branch, a pipeline is created, and for every change to it, a pipeline run is triggered, resulting in several application builds.\n\nBelow, you can see the high-level overview of our pipeline definition, written in YAML. It contains the build trigger, necessary build meta information and the build stages. As you can see, we have a non-platform specific build stage for the application foundation, and three separate build stages, one for each platform. Each successful build is then released to our designers via AppCenter, enabling quick delivery and short process times for the review (and optional rework) stage.\ntrigger:\r\n  - main\r\n  - release/*\r\n\r\nvariables:\r\n  isRelease: $[startsWith(variables['Build.SourceBranch'], 'refs/heads/release')]\r\n  androidArtifactName: \"drop_android\"\r\n  iosArtifactName: \"drop_ios\"\r\n  uwpArtifactName: \"drop_uwp\"\r\n\r\nname: $(GITVERSION.FULLSEMVER)\r\n\r\nstages:\r\n  - stage: \"Build\"\r\n   ...\r\n\r\n  - stage: \"BuildAndroid\"\r\n    dependsOn: Build\r\n    jobs:\r\n      - template: /build-android.yml\r\n\r\n  - stage: \"BuildIos\"\r\n    dependsOn: Build\r\n    jobs:\r\n      - template: /build-ios.yml\r\n\r\n  - stage: \"BuildUwp\"\r\n    dependsOn: Build\r\n    jobs:\r\n      - template: /build-uwp.yml\r\n\r\n  - stage: \"Production_Releases\"\r\n    ...\r\n\nEnabling iOS builds and distributions\nFor iOS, some specifics are worth mentioning. First of all, we use two separate provisioning profiles to build our iOS app: one for review builds distributed via AppCenter (the scope of this article), and one for end-user release builds ending up in the Apple App Store. Secondly, we need to be able to identify builds so we can tie them to change requests. For this, we use GitVersion, but any uniquely ID that is traceable to the commit or pull request level would suffice.\nIn our pipeline configuration we use a simple \u2018isRelease\u2019 variable to trigger the applicable build and release task:\njobs:\r\n  - job: Build_iOS\r\n    pool:\r\n      vmImage: \"macos-11\"\r\n    steps:\r\n      - task: GitVersion@5\r\n        ..\r\n      - task: NuGetToolInstaller@1\r\n\r\n      - task: NuGetCommand@2\r\n        ...\r\n\r\n      - task: InstallAppleCertificate@2\r\n        inputs:\r\n          certSecureFile: \"BronkhorstDistri.p12\"\r\n          certPwd: \"*****\"\r\n          keychain: \"temp\"\r\n\r\n      - task: DownloadSecureFile@1\r\n        name: BronkhorstDistri # The name with which to reference the secure file's path on the agent, like $(mySecureFile.secureFilePath)\r\n        inputs:\r\n          secureFile: BronkhorstDistri.p12\r\n\r\n      - task: InstallAppleProvisioningProfile@1\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          provisioningProfileLocation: \"secureFiles\"\r\n          provProfileSecureFile: \"Ad_Hoc_Bronkhorst_FlowControl.mobileprovision\"\r\n\r\n      - task: InstallAppleProvisioningProfile@1\r\n        condition: eq(variables.isRelease, true)\r\n        inputs:\r\n          provisioningProfileLocation: \"secureFiles\"\r\n          provProfileSecureFile: \"Bronkhorst_Flowcontrol_distribution.mobileprovision\"\r\n\r\n      - task: UpdateiOSVersionInfoPlist@1\r\n        ...\r\n\r\n      - task: CopyFiles@2\r\n        ...\r\n\r\n      - task: XamariniOS@2\r\n        inputs:\r\n          solutionFile: \"**/*.sln\"\r\n          configuration: \"Release\"\r\n          # This value is automatically set by the InstallAppleCertificate task\r\n          signingIdentity: $(APPLE_CERTIFICATE_SIGNING_IDENTITY)\r\n          # This value is automatically set by the InstallAppleProvisioningProfile task\r\n          signingProvisioningProfileID: $(APPLE_PROV_PROFILE_UUID)\r\n          packageApp: true\r\n          args: /p:IpaPackageDir=\"$(Build.ArtifactStagingDirectory)\"\r\n\r\n      - task: PublishBuildArtifacts@1\r\n        inputs:\r\n          PathtoPublish: \"$(Build.ArtifactStagingDirectory)\"\r\n          ArtifactName: \"$(iosArtifactName)\"\r\n          publishLocation: \"Container\"\r\n\r\n      - task: AppCenterDistribute@3\r\n        condition: eq(variables.isRelease, false)\r\n        displayName: iOS group release\r\n        inputs:\r\n          serverEndpoint: \"BronkhorstAppCenter\"\r\n          appSlug: \"Luminis/Bronkhorst.FlowControl.App.iOS\"\r\n          appFile: \"$(Build.ArtifactStagingDirectory)/Bronkhorst.FlowControl.iOS.ipa\"\r\n          buildVersion: \"$(GITVERSION.FULLSEMVER)\"\r\n          releaseNotesOption: \"input\"\r\n          releaseNotesInput: \"$(GITVERSION.FULLSEMVER)\"\r\n          destinationType: \"groups\"\r\n          distributionGroupId: \"XXXX-XXXX-XXXX-XXXX-XXXX\"\r\n\nBefore this can run we need to setup a few things in AppCenter, most importantly the distributionGroupId that is used in the AppCenterDistribute task.\n\nA few more things of note: to be able to distribute the app to our designers and other testers, we need to set up a few things in AppCenter. Most importantly, we need to have a \u2018distributionGroupId\u2019 and use that in our\u00a0 \u2018AppCenterDistribute\u2019 task. Next, we\u2019ve setup a Feature Tester group that will automatically resign the app for new devices; the certificate used in the build should match the one that is used to build the app in Azure DevOps. Finally, any tester that is added to our group must set up their device once using AppCenter, after which they will be able to receive subsequential updates.\nEnabling Android builds and distributions\nOur Android Pipeline setup looks very similar: we have separate build tasks for testing and production, which will either produce an Android Package (APK) that is distributed using AppCenter, or an Android App Bundle (AAB) that can be released to Google Play. Our configuration looks something like this:\njobs:\r\n  - job: Build_Android\r\n    pool:\r\n      vmImage: \"macos-latest\"\r\n    steps:\r\n      - task: GitVersion@5\r\n        ...\r\n\r\n      - task: NuGetToolInstaller@1\r\n\r\n      - task: NuGetCommand@2\r\n        ...\r\n\r\n      - task: DownloadSecureFile@1\r\n        name: \"androidKeystore\"\r\n        inputs:\r\n          secureFile: \"bronkhorst.flowcontrol.keystore\"\r\n          \r\n      - task: PowerShell@2\r\n        displayName: Set the variable of the Android version Code\r\n        ...\r\n\r\n      - task: UpdateAndroidVersionManifest@1\r\n        ...\r\n\r\n      - task: Bash@3\r\n        name: \"BuildAndroid_aab\"\r\n        condition: eq(variables.isRelease, true)\r\n        inputs:\r\n          targetType: \"inline\"\r\n          script: \"msbuild $(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.Droid/*.csproj /t:SignAndroidPackage -p:AndroidPackageFormat=aab /p:OutputPath=$(Build.ArtifactStagingDirectory) /p:Configuration=release /p:JavaSdkDirectory=$(JAVA_HOME_8_X64) -p:AndroidKeyStore=True -p:AndroidSigningKeyStore=$(androidKeystore.secureFilePath) -p:AndroidSigningStorePass=$(androidKeystore.password) -p:AndroidSigningKeyPass=$(androidKeystore.releaseKeyPassword) -p:AndroidSigningKeyAlias=release\"\r\n\r\n      - task: Bash@3\r\n        name: \"BuildAndroid_apk\"\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          targetType: \"inline\"\r\n          script: \"msbuild $(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.Droid/*.csproj /t:SignAndroidPackage -p:AndroidPackageFormat=apk /p:OutputPath=$(Build.ArtifactStagingDirectory) /p:Configuration=release /p:JavaSdkDirectory=$(JAVA_HOME_8_X64) -p:AndroidKeyStore=True -p:AndroidSigningKeyStore=$(androidKeystore.secureFilePath) -p:AndroidSigningStorePass=$(androidKeystore.password) -p:AndroidSigningKeyPass=$(androidKeystore.releaseKeyPassword) -p:AndroidSigningKeyAlias=release\"\r\n\r\n      - task: PublishBuildArtifacts@1\r\n        inputs:\r\n          PathtoPublish: \"$(Build.ArtifactStagingDirectory)\"\r\n          ArtifactName: \"$(androidArtifactName)\"\r\n          publishLocation: \"Container\"\r\n\r\n      - task: AppCenterDistribute@3\r\n        displayName: Android group release\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          serverEndpoint: \"BronkhorstAppCenterAndroid\"\r\n          appSlug: \"Luminis/Bornkhorst.FlowControl.App.Android\"\r\n          appFile: \"$(Build.ArtifactStagingDirectory)/com.bronkhorst.flowcontrol-Signed.apk\"\r\n          buildVersion: \"$(GitVersion.FULLSEMVER)\"\r\n          symbolsOption: \"Android\"\r\n          releaseNotesOption: \"input\"\r\n          releaseNotesInput: \"$(GITVERSION.FULLSEMVER)\"\r\n          destinationType: \"groups\"\r\n          distributionGroupId: \"be0bc1dc-26d2-40c4-90e9-c8ac1fdfae05\"\r\n\nAfter building the APK we can publish it to the AppCenter Test group.\n\nWe once again use an AppCenter Test group to onboard our designers and other reviewers. Unlike iOS, signing of the app is not needed, as long as our reviewers allow installations from unknown sources. The AppCenter Android app can be used to easily install new versions.\nEnabling Windows builds and distributions\nIn our case, we also release a Windows application. The process is once again very similar. To prevent unnecessary wait times, we opted to only test the x86 release version of our app, reducing our build time by half compared to building for more targets. Our setup, roughly:\njobs:\r\n  - job: Build_UWP\r\n    pool:\r\n      vmImage: \"windows-latest\"\r\n    steps:\r\n      - task: GitVersion@5\r\n        ...\r\n\r\n      - task: NuGetToolInstaller@1\r\n\r\n      - task: NuGetCommand@2\r\n        ...\r\n\r\n      - task: DownloadSecureFile@1\r\n        name: \"uwpCert\"\r\n        inputs:\r\n          secureFile: \"Bronkhorst.FlowControl.UWP.pfx\"\r\n\r\n      - task: PowerShell@2\r\n        displayName: \"Set Package.appxmanifest version to GitVersion\"\r\n        ...\r\n\r\n      - task: VSBuild@1\r\n        condition: eq(variables.isRelease, false)\r\n        name: \"BuildReleaseTestVersion\"\r\n        displayName: \"Build Release Test Version\"\r\n        inputs:\r\n          platform: \"x86\"\r\n          solution: \"$(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.UWP/*.csproj\"\r\n          configuration: \"Release\"\r\n          msbuildArgs: '/p:AppxBundlePlatforms=\"x86\"\r\n                        /p:AppxPackageDir=\"$(Build.ArtifactStagingDirectory)\"\r\n                        /p:AppxBundle=Always\r\n                        /p:UapAppxPackageBuildMode=StoreUpload\r\n                        /p:AppxPackageSigningEnabled=true\r\n                        /p:PackageCertificateThumbprint=\"\"\r\n                        /p:PackageCertificateKeyFile=\"$(uwpCert.secureFilePath)\"\r\n                        /p:PackageCertificatePassword=\"$(uwpCert.password)\"'\r\n\r\n      - task: VSBuild@1\r\n        condition: eq(variables.isRelease, true)\r\n        name: \"BuildReleaseVersion\"\r\n        displayName: \"Build Release Version\"\r\n        inputs:\r\n          platform: \"x86\"\r\n          solution: \"$(Build.SourcesDirectory)/Bronkhorst.Flowcontrol.UWP/*.csproj\"\r\n          configuration: \"Release\"\r\n          msbuildArgs: '/p:AppxBundlePlatforms=\"x86|x64|ARM\"\r\n                        /p:AppxPackageDir=\"$(Build.ArtifactStagingDirectory)\"\r\n                        /p:AppxBundle=Always\r\n                        /p:UapAppxPackageBuildMode=StoreUpload\r\n                        /p:AppxPackageSigningEnabled=true\r\n                        /p:PackageCertificateThumbprint=\"\"\r\n                        /p:PackageCertificateKeyFile=\"$(uwpCert.secureFilePath)\"\r\n                        /p:PackageCertificatePassword=\"$(uwpCert.password)\"'\r\n\r\n      - task: CopyFiles@2\r\n        displayName: \"Copy Output Files to: $(Build.ArtifactStagingDirectory)\"\r\n        inputs:\r\n          SourceFolder: \"$(system.defaultworkingdirectory)\"\r\n          Contents: '**\\bin\\Release\\**'\r\n          TargetFolder: \"$(Build.ArtifactStagingDirectory)\"\r\n\r\n      - task: PublishBuildArtifacts@1\r\n        inputs:\r\n          PathtoPublish: \"$(Build.ArtifactStagingDirectory)\"\r\n          ArtifactName: \"$(uwpArtifactName)\"\r\n          publishLocation: \"Container\"\r\n\r\n      - task: AppCenterDistribute@3\r\n        displayName: \"UWP group release $(GitVersion.AssemblySemVer)\"\r\n        condition: eq(variables.isRelease, false)\r\n        inputs:\r\n          serverEndpoint: \"BronkhorstAppCenterUWP\"\r\n          appSlug: \"Luminis/Bronkhorst-FlowSuite-UWP\"\r\n          appFile: \"$(Build.ArtifactStagingDirectory)/Bronkhorst.FlowControl.UWP_$(GitVersion.AssemblySemVer)_Test/Bronkhorst.FlowControl.UWP_$(GitVersion.AssemblySemVer)_x86.msixbundle\"\r\n          buildVersion: \"$(GITVERSION.AssemblySemVer)\"\r\n          symbolsOption: \"UWP\"\r\n          releaseNotesOption: \"input\"\r\n          releaseNotesInput: \"$(GITVERSION.FULLSEMVER)\"\r\n          destinationType: \"groups\"\r\n          distributionGroupId: \"XXXX-XXXX-XXXX-XXXX-XXXX\"\r\n\nUnsigned app bundles cannot be installed on Windows, so we need to sign our app using a certificate. If you use one that is not trusted by Windows, your reviews must first install the certificate as a root certificate before they can install the test application versions.\nConclusion\nI hope that Mike and I have managed to convince you that improving user experience and project flow is possible with the right attitude, process, and technology. In our case, Azure DevOps and AppCenter helped us a great deal in reducing lead times and thus delivering customer value early and often. Our setup is specific, but the principles of our solutions should be easily applicable to other technology stacks and similar use cases. We\u2019d love to hear how you did it!\n", "tags": ["Android", "AppCenter", "Azure DevOps", "CI/CD", "Design", "iOS", "Mobile", "Windows"], "categories": ["Blog", "Development"]}
{"post_id": 30750, "title": "Pull requests for the designer: How to improve the development process", "url": "https://www.luminis.eu/blog/concepting-ux-en/pull-requests-for-the-designer-how-to-improve-the-development-process/", "updated_at": "2023-01-27T12:43:49", "body": "At Luminis we see user experience and usability as crucial factors in developing successful mobile and web apps.\u00a0We are always looking for ways to improve the way we as a team collaborate and work towards delivering those great applications. One of those improvements is putting the designer into the technical workflow of the developers.\nOur approach\nIn short, our approach goes something like this. After the product definition phase, the designer starts creating the various screens and interactions to accommodate the use cases as described on the backlog. This is done in collaboration with the Product Owner (PO) and the development team. In doing so, we design the right solution (checked by PO) and the design is implementable (checked by the developers).\nWhen the designs are done, the development team can still involve the designer for assistance during implementation, sometimes resulting in updated designs. This flexibility and close contact helps us as a team to implement the best possible solution within 1 sprint.\nThe problem\nThis close contact between the PO, the designer and the developers makes us agile and helps us to quickly catch possible issues and lose little time by preventing rework. But what did not work well enough for us, is the moment after the developers are done with implementing those designs.\u2028\u2028The developer finishes the functionality, sets the backlog item to done (after being reviewed by a colleague developer of course), and that functionality is then presented in the demo at the end of sprint. In our experience, this process was lacking for two reasons. \u2028One, during this demo the designer sometimes would notice a detail or interaction that would not work well or as intended. This can be the result of a mistaken implementation of the design or an unexpected issue caused by something else. As a consequence, this issue would be put in the next sprint to fix. \u2028The second reason is that the demo shows the functionality in a limited way, often as one specific use case as described in the original user story. That is quite different from experiencing the app on your own device, going through several use cases. You\u2019ll experience stuff like loading times, and how an interaction feels in the bigger picture of the whole application.\nThe fix\nSo there was a weak link between finishing the code by the developers, and demoing this work at the end of the sprint. How did we fix this? We involve the designer as soon as possible, when the functionality has been built but before this is shown at a demo. Using the existing development process with its tools, it is quite easy for the designer to test the new functionality.\u00a0And that is done by involving the designer in approving pull requests as well. (What is a pull request, you ask? It is basically a developer asking the team to review his or her code, before it become part of the main code.)\u00a0The developer is then not only sending a pull request to the developers but also the designer. The designer can download the right build of the app to experience and review this new functionality.\n\nThis was the fix we needed in our process. As a designer I get notified when a PR is available, I download the build and test the functionality. I am able to verify if the design and the PO\u2019s wishes have been implemented well, and I could act quickly (with suggestions or updated designs) if something was off. For our development process, this gives us multiple advantages:\n\nA designer can check the functionality and help to improve it before the end of the sprint, so the functionality is definitely finished at that point. And that results in a satisfied PO at the demo;\nWith a review of the PR when it becomes available, the developer has the code and solution still fresh in his or her memory. So discussing or changing the solution, takes less time and effort for the developer, in comparison to fixing an issue in the next sprint.\u00a0This will more then compensate the extra time the designer puts in to do the PR work;\nA designer becomes a semi-tester, by putting the new functionality to the test and trying various inputs and interactions.\n\nIn a recent project, we applied this process when developing a mobile app. My colleague Frank explains in this post how you could set this is up for your own projects.\nConclusion\nIf user experience and usability of an application are important, it makes sense to involve the designer sooner and more often during the development process to review newly built functionalities. The best way to do this is to use the already existing process of pull requests, and sending the designer a PRs as well. Then not only the code will be reviewed, but the user-facing solution will be tested as well, resulting in better and more user-friendly applications.\n", "tags": ["Design"], "categories": ["Blog", "Concepting &amp; UX", "Development"]}
{"post_id": 32408, "title": "Creating a simple API stub with API Gateway and S3", "url": "https://www.luminis.eu/blog/cloud-en/creating-a-simple-api-stub-with-api-gateway-and-s3/", "updated_at": "2022-05-20T10:27:46", "body": "A while ago my team was looking to create a stub for an internal JSON HTTP based API. The to-be stubbed service was quite simple. The service exposed a REST API endpoint for listing resources of a specific type. The API supported paging and some specific request/query parameters.\nGET requests to the service looked something like this:\n/items?type=sometype&page=2\nWe wanted to create a simple stub for the service within our AWS account which we could use for testing. One of the tests we wanted to perform is to test if the service was down. If so our application would follow a specific code path. We could of course not easily test this with the real services. of the remote API, we tried to keep things as simple as possible.\nCreating the API with API Gateway, Lambda, and S3\nAs most of the services within our domain are based on Amazon API Gateway and AWS Lambda, we started looking into that direction at first. As our stub was read-only and we didn\u2019t have to modify the items, we chose to create an initial export of the dataset from the remote API into JSON files, which we could store in S3. For storing the files we chose to use a file name pattern that resembled our type and page parameter.\n{TYPE}_{PAGE_NUMER}(.json)\nThis resulted in a bucket like this:\n\nSo the simplified design of the API stub was going to be as follows:\n\nAn example version of what our code looked like was something similar to this:\n\nAs you can see in the above snippet, we\u2019re essentially calculating the path to the object in S3 based on some request parameters. When the path is resolved we just fetch the file from S3, and convert it to a string for the reply to API Gateway, which in turn returns the file to the consumer. Our lambda function was just acting as a simple proxy and we were wondering if we could get rid of the lambda function at all. Maintaining code, dependencies, etc takes a burden, so if we don\u2019t need it we would like to get rid of it.\nCreating the API with just API Gateway and S3\nAPI Gateway has great support for direct integration with other AWS services, so we started exploring our options. The solution we hoped for was something similar to the image below.\n\nWhile going through the documentation for API Gateway we found a pretty good example of how to use API Gateway as a proxy for S3. The provided solution lets API Gateway mirror the folder and file structure(s) in S3. Useful, but it did not cover our use case.\nOne of the other options that looked promising was configuring mapping templates. We had used that before to transform an incoming request body to a different format for the remote backend. In case you\u2019re unfamiliar with mapping templates in API Gateway:\nA mapping template is a script expressed in Velocity Template Language (VTL)and applied to the payload using JSONPath expressions.\nAfter digging through the API Gateway documentation we also discovered that mapping templates can be used to alter the query string, headers, and path.\n\nModifying the path was exactly what we wanted, so we tried that and it worked like a charm. Let\u2019s take a look at the resulting setup for our API Gateway GET request.\n\nAs you can see in the above section we\u2019ve added a GET method to the root of our API. The GET method has a method request which defines both query parameters; page and type.\n\nFor the integration request, we define the remote service we want to integrate with and we specify the bucket and a dynamic fileName.\n\nIn the path override of the integration request there are two important things to notice:\n\nAt the start of the Path override parameter, we provide the S3 bucket name\nAs the second argument, we provide a dynamic value named fileName\n\nThe path override will therefor be {bucket-name}/{fileName}.\nNow in our mapping template, we will fill the fileName parameter so that API Gateway knows which file to get from S3.\nLet\u2019s take a look at the mapping template.\n\nAs you can see we\u2019ve set up a template for requests for content-type application/json. Now when a GET request arrives with the type and page query parameters, it will assemble the resulting fileName variable in the path override.\n\nWhen the file in that specific bucket is found it will return the corresponding JSON. When the file is not found it will throw a 404 response body. We can also map the response code with a mapping template to produce some nice-looking error messages.\nSome last thoughts\nWhile researching this solution I also came across a post by Tom Vincent. Tom wrote a nice post about what he calls Lambdaless. Lambdaless is in essence an integration from API Gateway to another AWS Service without the use of AWS Lambda. I like the term and it resonates well with what we also tried to achieve in this post. In essence it\u2019s similar to what Peter wrote when he wrote his articles on creating a REST API on top of DynamoDB.\nI think this post shows a nice way of using Mapping Templates in API Gateway to transform the path and create a simple stub. Do keep in mind we use this for testing only and we don\u2019t run production workloads with this setup. I also think that if the API would have been more complex we would not have taken this approach. Nevertheless, it was simple to set up and use for our stub.\n", "tags": ["aws", "cloud"], "categories": ["Cloud", "Development"]}
{"post_id": 32359, "title": "Production-Ready CDK \u2013 Bootstrapping", "url": "https://www.luminis.eu/blog/production-ready-cdk-bootstrapping/", "updated_at": "2023-01-27T12:55:25", "body": "\nIn the previous posts, we set up the project, wrote our first construct, and implemented a CI/CD Pipeline using CDK Pipelines. If you haven\u2019t already done so, Bootstrapping AWS CDK is one of the first things we need to do in our AWS Accounts.\n\n\n\n\n\nWhat is AWS CDK Bootstrapping?\nTo use CDK and deploy our CDK app to our AWS environment(an AWS account plus a region), we need to provision the resources required for deployment. These resources are mainly an S3 bucket to store files and IAM Roles to have permissions related to deployment. We call the process of setting these resources bootstrapping.\nAccording to the documentation, you need bootstrapping if you use Assets and if your generated app template file size is more than 50 kilobytes. However, you will almost always cover these two cases, so you can start doing it without thinking more.\nWhy is it worth discussing?\nYou might be wondering why even I write about bootstrapping. Because in many tutorials, it says you need to use the cdk bootstrap command, and you are good to go, which is right most of the time.\nBut when you are using a company AWS account, the command will probably fail because of the permissions boundary. In that case, you need to consider Custom Bootstrapping, which might take some time if you don\u2019t know how to do it. The following information will help you do it in a few minutes.\nIf the cdk bootstrap command works on the first try, you most likely don\u2019t have the permissions boundary setup(you should seriously consider implementing it).\n\n\n\n\nSide note: CDK v2 uses the modern bootstrapping template by default, unlike CDK v1. In CDK v1, the default template is the legacy one, and the modern one is optional as you need to use the context flag:\n\u201c@aws-cdk/core:newStyleStackSynthesis\u201d: \u201ctrue\u201d.\nIf you have the legacy template in your environment, I recommend updating it and this post might be relevant to you. You can check the differences here.\n\n\n\n\nAlright, how do we\u00a0proceed?\nHere is the documentation for the bootstrapping. Again, using CDK v2, I would try cdk bootstrap with my local credentials and profile (example: including --profile dev), and it would fail(hopefully)!\nHaving purposeful guardrails in the cloud is essential. It might first seem they are restricting us, developers. But, if we use the guardrails well, they will save us from future headaches.\nNow, let\u2019s say we have already tried the command and it didn\u2019t work, because a role creation failed due to the permissions boundary. We need to think of Custom Bootstrapping, which is straightforward.\nAs the next step, you can download the bootstrapping template using the command cdk bootstrap --show-template > bootstrap-template.yaml. Then use the command cdk bootstrap --template bootstrap-template.yaml to deploy it. But it will fail again because we haven\u2019t changed anything related to the permissions boundary.\nPermissions Boundary\u00a0Changes\nWe need to provide a permissions boundary to every role in the bootstrapping template to resolve the problem. To achieve that quickly, we can get the Amazon Resource Name(ARN) of the boundary as an input parameter and then use it.\nIn the example below, I omitted and dotted some parts for brevity. We can ignore those lines.\nYou can see the boundary parameter with the name PermissionsBoundaryArn. After defining the input parameter, we use it in the IAM Role declaration on line 54.\n\r\nDescription: This stack includes resources needed to deploy AWS CDK apps into this\r\n  environment\r\nParameters:\r\n  .\r\n  .\r\n  .\r\n  .\r\n  .\r\n  PublicAccessBlockConfiguration:\r\n    Description: Whether or not to enable S3 Staging Bucket Public Access Block Configuration\r\n    Default: 'true'\r\n    Type: 'String'\r\n    AllowedValues: ['true', 'false']\r\n  PermissionsBoundaryArn:\r\n    Description: ARN of the Permissions Boundary\r\n    Type: 'String'\r\nConditions:\r\n  HasTrustedAccounts:\r\n    Fn::Not:\r\n      - Fn::Equals:\r\n          - ''\r\n          - Fn::Join:\r\n              - ''\r\n              - Ref: TrustedAccounts\r\n  .\r\n  .\r\n  .\r\n  .\r\nResources:\r\n  .\r\n  .\r\n  .\r\n  .\r\n  FilePublishingRole:\r\n    Type: AWS::IAM::Role\r\n    Properties:\r\n      AssumeRolePolicyDocument:\r\n        Statement:\r\n          - Action: sts:AssumeRole\r\n            Effect: Allow\r\n            Principal:\r\n              AWS:\r\n                Ref: AWS::AccountId\r\n          - Fn::If:\r\n              - HasTrustedAccounts\r\n              - Action: sts:AssumeRole\r\n                Effect: Allow\r\n                Principal:\r\n                  AWS:\r\n                    Ref: TrustedAccounts\r\n              - Ref: AWS::NoValue\r\n      RoleName:\r\n        Fn::Sub: cdk-${Qualifier}-file-publishing-role-${AWS::AccountId}-${AWS::Region}\r\n      PermissionsBoundary: !Ref PermissionsBoundaryArn\r\n      Tags:\r\n        - Key: aws-cdk:bootstrap-role\r\n          Value: file-publishing\r\n\nHere is the complete bootstrapping template. If you want to deploy from the CloudFormation Console, you don\u2019t need to change anything(the template version is v12 by the time I wrote this). You only need to provide the ARN as input.\nTo deploy from your CLI, you need to provide the ARN value for PermissionsBoundaryArn in the template. Then you can use the command cdk bootstrap --template bootstrap-template.yaml.\nMore Customization\nDepending on the industry & size of your company, your organization might have specific Compliance & Security requirements, such as adding Tags to the resources or having encryption for S3 Buckets. Depending on those requirements, you can keep modifying the template and deploy it in the same way.\nFor the other CLI command options, you can recheck the documentation. I recommend using flags:\n\n\u2012 \u2012 tags: it is a best practice to tag all resources you can.\n\u2012 \u2012 termination-protection: and have it enabled. Because you hardly need to delete the bootstrapping stack, you better have it.\n\u2012 \u2012 trust: in case you need to deploy from your dev environment to higher environments.\n\n\n\n\n\nIn this part of the series, we focused on a shorter topic, bootstrapping, and especially how to do this in a custom way.\nIn the next chapter, we will continue with Aspects to overcome the Permissions Boundary. This time, it is for our CDK Pipeline roles and all the other roles/resources we will provision.\nSee you in the next ones. Ciao!\n\n\n\n\n", "tags": ["aws", "aws cdk", "cloud", "devops"], "categories": ["Blog", "Cloud"]}
{"post_id": 32212, "title": "Luminis partners with AWS in Cloud Lab at Cupola XS", "url": "https://www.luminis.eu/blog/luminis-partners-with-aws-in-cloud-lab-at-cupola-xs/", "updated_at": "2022-05-05T13:18:16", "body": "Haarlem \u2013 Luminis and AWS are working together to accelerate the adoption of cloud technology by SMEs. Luminis has joined the Cloud Lab set up by Amazon Web Services (AWS) at the Cupola XS innovation center to help organizations innovate faster and scale their innovations more easily. As an AWS Advanced Consulting Partner, Luminis offers direct access to its knowledge and expertise in cloud and data technology in the domed building that used to be Haarlem prison.\nIn partnering at the Cloud Lab, AWS and Luminis share the ambition of accelerating the adoption of cloud technology by SMEs and their employees. AWS is a global pioneer in cloud services and has set up the Cloud Lab on the third floor at the Cupola XSinnovation center. Netherlands-based cloud-native consultancy CloudNation has also joined the lab.\nSharing knowledge and expertise\nThe Cloud Lab will actively contribute to knowledge events in an inspiring setting that used to be Haarlem prison. Luminis will share its knowledge and expertise in business and digital transformation at these events at least four times a year. Together with AWS, Luminis shows how cloud technology enables innovation.\u00a0\n\u201cLuminis is looking forward to actively contributing to the Cupola XS community. Besides attending the knowledge events and presentations, my Luminis colleagues and I will be present in person at the Cloud Lab to assist organizations and their employees with their digital needs and innovations,\u201d said Richard M. de Wolf, commercial partner lead at Luminis.\nLuminis as AWS Partner\nLuminis is an AWS Advanced Consulting Partner . Luminis spends a considerable amount of time gaining and maintaining knowledge and experience of AWS products and services. Luminis colleagues work with AWS technology daily and use this knowledge to assist the company\u2019s customers. This joint expertise will also be available at the physical Cloud Lab as part of the intensive collaboration between Luminis and AWS.\n", "tags": ["aws", "cloud labs", "cupola", "news"], "categories": ["Blog", "News"]}
{"post_id": 31828, "title": "The Amplify Series, Part 4: Developing and deploying a cloud-native application with AWS Amplify", "url": "https://www.luminis.eu/blog/the-amplify-series-part-4-developing-and-deploying-a-cloud-native-application-with-aws-amplify/", "updated_at": "2023-05-08T12:59:24", "body": "In this article, we will be using AWS Amplify to create a cloud-native application. We will cover the Authentication, GraphQL API, and Hosting categories of AWS Amplify. We will be using Angular for the frontend. However, the steps taken here should be similar for other frameworks.\nWe have learned what Amplify is, how Amplify works behind the scenes, and Amplify use cases throughout this series. It is time to see this in action and get hands-on experience using\u00a0 Amplify. At the end of this article, you will have a cloud-native application.\nPrerequisites\nBefore getting started, we need to make sure we have some things set. You need to have the following to be able to follow these instructions and build your application:\n\nAn active AWS account.\nNodeJS: I used version 14.17.5.\nAngular CLI: I used version 13.1.4.\nGit: Necessary to check out the example project.\nAmplify CLI: I used version 7.6.22.\n\nConfiguring Amplify CLI\nBefore we can start using the CLI, we need to configure the AWS Amplify CLI to use our AWS account. Once you have configured your CLI, we can get started.\nInstalling dependencies & launching the app\nWe have created an example AWS Amplify Angular app that you can clone. Check out the start_here branch and start from there. To ensure everything is working, run npm run start. You should see a simple application called \u201cThe Amplify App\u201d:\n\nThis app is currently a simple Angular app with no associated backend yet. From this point, we will start using the CLI to initialise our project, generate backend resources, and connect them to the frontend.\nInitialising the backend\nThe first command we will run is amplify init. You should see output similar to this:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify init\r\nNote: It is recommended to run this command from the root of your app directory\r\n? Enter a name for the project theamplifyapp\r\nThe following configuration will be applied:\r\n\r\nProject information\r\n| Name: theamplifyapp\r\n| Environment: dev\r\n| Default editor: Visual Studio Code\r\n| App type: javascript\r\n| Javascript framework: angular\r\n| Source Directory Path: src\r\n| Distribution Directory Path: dist/theamplifyapp\r\n| Build Command: npm run-script build\r\n| Start Command: ng serve\r\n\r\n? Initialize the project with the above configuration? Yes\r\nUsing default provider awscloudformation\r\n? Select the authentication method you want to use: AWS profile\r\n\r\nFor more information on AWS Profiles, see:\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\r\n\r\n? Please choose the profile you want to use default\r\n\u2834 Initializing project in the cloud...\r\n\nOnce this is done, our Amplify project is created, and we can see several new files in our repository. If you need a refresher of what these files are and how Amplify works, check out part 2 of this blog series: how does AWS amplify work?\nIf you are creating a public project, it is recommended to add the amplify/team-provider-info.json to the .gitignore, as it contains identifiers to the resources we are using for Amplify. This does not create a security risk since your project is secured with your credentials. If multiple developers are working on the same project, they all need this file.\nIf everything is working, you should see about 7 changed files.\nConfiguring the frontend\nNow we need to install the relevant Amplify frontend libraries. In this case, we run:\nnpm install --save aws-amplify @aws-amplify/ui-angular\nThis will install the amplify library and Angular specific UI elements that we will be using later. After this, we want to configure the Amplify library with our aws-exports.js by adding the following to our src/main.ts:\nimport Amplify from 'aws-amplify';\r\nimport aws_exports from './aws-exports';\r\nAmplify.configure(aws_exports);\nYour IDE might complain about the ./aws-exports not having a type definition. An easy for now is to add allowJs:true\u00a0to our tsconfig.json.\nFinally, we also need to add the following to src/polyfill.ts:\n(window as any).global = window;\r\n(window as any).process = {\r\nenv: { DEBUG: undefined }\r\n};\nWe need to add these since they are used by Amplify but are not present by default since Angular 6.\nYou should see around 5 file changes after this step.\nAdding Amplify Authentication\nWe are now going to add user authentication functionality to our application.\u00a0\nGenerating backend resources for authentication\nThe first thing we will do is generate backend resources that will help us with authentication by running amplify add auth. You should see output similar to this:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add auth\r\nUsing service: Cognito, provided by: awscloudformation\r\n\r\nThe current configured provider is Amazon Cognito.\r\n\r\nDo you want to use the default authentication and security configuration? Default configuration\r\nWarning: you will not be able to edit these selections.\r\nHow do you want users to be able to sign in? Username\r\nDo you want to configure advanced settings? No, I am done.\r\n\u2705 Successfully added auth resource theamplifyappfdeaa7e5 locally\r\n\r\n\u2705 Some next steps:\r\n\"amplify push\" will build all your local backend resources and provision it in the cloud\r\n\"amplify publish\" will build all your local backend and frontend resources (if you have hosting category added) and provision it in the cloud\nWe should now see some new files added to our project, such as files under amplify/backend/auth, which represent our authentication choices. However, nothing has happened in the backend yet. We need to run amplify push to push our changes to AWS and have CloudFormation create the resources for us:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify push\r\n\r\n\u2714 Successfully pulled backend environment dev from the cloud.\r\n\r\nCurrent Environment: dev\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Category \u2502 Resource name \u2502 Operation \u2502 Provider plugin \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Auth \u2502 theamplifyappfdeaa7e5 \u2502 Create \u2502 awscloudformation \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n? Are you sure you want to continue? Yes\r\n\u280b Updating resources in the cloud. This may take a few minutes...\nAdding the authentication component to the frontend\nWe will be using the Amplify Authenticator UI component to implement authentication in the frontend. We already installed this previously however now, we still need to add it to src/app/app.module.ts:\n//Other imports\r\nimport { AmplifyAuthenticatorModule } from '@aws-amplify/ui-angular'; // <-- Add this\r\n\r\n@NgModule({\r\ndeclarations: [\r\n//Existing declarations\r\n],\r\nimports: [\r\n//other imports\r\nAmplifyAuthenticatorModule // <-- Add this\r\n],\r\n// Other stuff\r\n})\r\nexport class AppModule {}\nWe are going to start off by simply protecting our entire application with the Amplify authenticator. To do this, add the following to src/app/app.component.html:\n<amplify-authenticator><!-- <--Add this -->\r\n<app-header></app-header>\r\n<div class=\"content-container\">\r\n<router-outlet></router-outlet>\r\n</div>\r\n<app-footer></app-footer>\r\n</amplify-authenticator> <!-- <--Add this -->\nAnd we also need to import the default styling in our src/app/styles.css:\n@import \"@aws-amplify/ui-angular/theme.css\";\r\n\r\n/* other styles */\nIf you run your application now, you should see the Amplify authenticator:\n\nYou can use this to create an account and log in to see the application. It is a fully-featured authenticator with registration, sign-in, forgot password, and email activation. It also gives feedback when errors occur.\u00a0\nThere are several options to customize this UI. You can find out more about the Amplify Authenticator in the documentation.\nOnce you have created an account, you should be able to log in to the AWS console, navigate to the Cognito service and see a user pool with the name of your project. Inside that user pool, you should see your new account:\n\nAdding signout functionality\nWe also want to be able to sign out of our application. First we add the actual button in header.component.html:\n<nav>\r\n    <!-- other existing code -->\r\n    <button class=\"aws-button\" (click)=\"signOut()\">Sign out</button> <!-- \u00a0<-- Add this -->\r\n</nav>\nAnd then, we add our sign out logic, which we get directly from Amplify, in our header.component.ts:\nimport { Auth } from 'aws-amplify'; // <-- Add this\r\n\r\nexport class HeaderComponent implements OnInit {\r\n\r\n// Other existing code\r\n\r\n    signOut() { // <-- Add this\r\n      Auth.signOut();\r\n    }\r\n}\nAnd now, we have a sign-out button which we can use to sign out and go back to the Amplify Authenticator.\nUser data\nWe will add a Welcome <<username>> text in the header. We can use the Amplify Auth library to get this data. We will begin by creating an Angular service which we will call UserService, which will handle all of our logged-in user\u2019s needs.\u00a0\nIn the terminal, run the following at the root of your project:\nng generate service services/shared/user\nWe will update the new service in the following way:\nimport { Injectable } from '@angular/core';\r\nimport { Auth } from 'aws-amplify';\r\n\r\nexport interface UserInfo {\r\n\u00a0 email: string;\r\n\u00a0 username: string;\r\n}\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class UserService {\r\n\u00a0 constructor() {}\r\n\r\n\u00a0 async getCurrentUserInfo(): Promise {\r\n\u00a0 \u00a0 const userInfoResponse = await Auth.currentUserInfo();\r\n\u00a0 \u00a0 return {\r\n\u00a0 \u00a0 \u00a0 email: userInfoResponse.attributes.email,\r\n\u00a0 \u00a0 \u00a0 username: userInfoResponse.username\r\n\u00a0 \u00a0 };\r\n\u00a0 }\r\n}\nIt makes use of the Amplify Auth library to get information about the current user. We then make some changes in our header.component.ts file to call this function:\nimport { UserService, UserInfo } from '../../services/shared/user.service'; //<--- Add this\r\n\r\nexport class HeaderComponent implements OnInit {\r\n\u00a0 //other code\r\n\u00a0 currentUserName: string = ''; //<--- Add this\r\n\r\n\u00a0 constructor(public router: Router, private userService: UserService ) { //<--- Add userService\r\n\u00a0 \u00a0 //other code\r\n\u00a0 }\r\n\r\n\u00a0 ngOnInit(): void { //<--- Add this \u00a0 \u00a0 this.userService \u00a0 \u00a0 \u00a0 .getCurrentUserInfo() \u00a0 \u00a0 \u00a0 .then((userInfo: UserInfo) => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 this.currentUserName = userInfo.username;\r\n\u00a0 \u00a0 \u00a0 })\r\n\u00a0 \u00a0 \u00a0 .catch(error => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 console.log('Error while obtaining user: ', error);\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 }\r\n}\nAnd now that we have variable in the header component with the current username, we can display this in the header.component.html:\n<nav>\r\n<!-- other code -->\r\n    <span class=\"welcome-text\">Welcome {{ currentUserName }}</span> <!-- <--- add this -->\r\n    <button class=\"aws-button\" (click)=\"signOut()\">Sign out</button>\r\n</nav>\nAnd finally we add some styling in header.component.css so that we can actually see the text on the dark background:\n.welcome-text {\r\n  color: white;\r\n  margin-right: 16px;\r\n}\nAnd now we have a text that shows us the username of the user that is logged in:\n\nCheck out the Amplify Auth documentation\u00a0for details.\nWe\u2019ve changed several things now:\n\nWe added the Amplify Authenticator UI.\nWe added a signout button.\nWe added user details to the header.\n\nConnecting the backend with GraphQL\nIn this section, we will add a GraphQL API for our application, which will replace the current in-memory mock data being used for the API page in our application. If you click through the API tab, you will see that we have a list of Posts that can all have a list of Likes and Comments. We will be modeling this in GraphQL.\nGenerating backend resources for the API\nTo start, we need to add a resource endpoint:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add api\r\n? Select from one of the below-mentioned services: GraphQL\r\n? Here is the GraphQL API that we will create. Select a setting to edit or continue Authorization modes: API key (default, expiration time: 7 days from now)\r\n? Choose the default authorization type for the API Amazon Cognito User Pool\r\nUse a Cognito user pool configured as a part of this project.\r\n? Configure additional auth types? No\r\n? Here is the GraphQL API that we will create. Select a setting to edit or continue Continue\r\n? Choose a schema template: One-to-many relationship (e.g., \u201cBlogs\u201d with \u201cPosts\u201d and \u201cComments\u201d)\r\n\r\n\u26a0\ufe0f WARNING: Some types do not have authorization rules configured. That means all create, read, update, and delete operations are denied on these types:\r\n- Blog\r\n- Post\r\n- Comment\r\nLearn more about \"@auth\" authorization rules here: https://docs.amplify.aws/cli/graphql/authorization-rules\r\nGraphQL schema compiled successfully.\r\n\r\nEdit your schema at /Users/evertsoncroes/Documents/development/private/theamplifyapp/amplify/backend/api/theamplifyapp/schema.graphql or place .graphql files in a directory at /Users\r\n/evertsoncroes/Documents/development/private/theamplifyapp/amplify/backend/api/theamplifyapp/schema\r\n\u2714 Do you want to edit the schema now? (Y/n) \u00b7 yes\r\n\u2705 Successfully added resource theamplifyapp locally\r\n\r\n\u2705 Some next steps:\r\n\"amplify push\" will build all your local backend resources and provision it in the cloud\r\n\"amplify publish\" will build all your local backend and frontend resources (if you have hosting category added) and provision it in the cloud\nIt is important to change the authentication method from API key to Amazon Cognito User Pool to use the Auth resources we created in the last section.\nWe now have some new files generated for us by the Amplify CLI. We are going to start by editing the amplify/backend/api/schema.graphql to look as follows:\ntype Post\r\n@model\r\n@auth(rules: [{ allow: owner }, { allow: private, operations: [read] }]) {\r\nid: ID!\r\ntitle: String!\r\ndescription: String\r\nlikes: [Like] @hasMany\r\ncomments: [Comment] @hasMany\r\n}\r\n\r\ntype Like\r\n@model\r\n@auth(rules: [{ allow: owner }, { allow: private, operations: [read] }]) {\r\nid: ID!\r\npost: Post @belongsTo\r\n}\r\n\r\ntype Comment\r\n@model\r\n@auth(rules: [{ allow: owner }, { allow: private, operations: [read] }]) {\r\nid: ID!\r\npost: Post @belongsTo\r\ncontent: String!\r\n}\nIn this example, we have defined 3 GraphQL Object types for Post, Likes, and Comments. We don\u2019t specify a date field because we get a createdAt and updatedAt field for each type by default.\u00a0\nThe Amplify specific parts of this schema are the directives, which you can notice by the @ prefix. The directives used here are:\n\n@model: This creates a DynamoDB table to back this model. In this case, 3 tables will be created.\n@auth: These are authorization rules for the type. The rules described here are:\n\nThe owner (creator) of a record is allowed to perform all operations on that record\nAn \u201cowner\u201d field will be added to each item in the DB, which contains the username of the logged-in user that created the item\nOnly logged in users (private) can read the records for this type\n\n\n@hasMany: This creates a one-to-many relationship with another type. In our example above, Posts have many comments and likes.\n@belongsTo: This creates a many-to-one relationship with another type. In our example above, comments belong to one post, and likes belong to one post\n\nYou can learn more about directives or authorization rules in the Amplify documentation.\nAfter customizing our schema we run amplify push and select yes. We are then asked a few questions related to graphql:\n? Do you want to generate code for your newly created GraphQL API Yes\r\n? Choose the code generation language target angular\r\n? Enter the file name pattern of graphql queries, mutations and subscriptions src/graphql/**/*.graphql\r\n? Do you want to generate/update all possible GraphQL operations - queries, mutations, and subscriptions Yes\r\n? Enter maximum statement depth [increase from default if your schema is deeply nested] 2\r\n? Enter the file name for the generated code src/app/API.service.ts\nThis will generate a lot of front-end code specific to our schema so thatuse we can easily \u00a0our GraphQL backend.\nConnecting the frontend to the API\nA few files have been generated that are important to cover. First of all, there is the API.service.ts. This file contains all generated types and statements that can be used with the GraphQL API and injected as an Angular Service into Angular components and services.\nSecondly, we also have a directory now called graphql in the root of our project. This file contains generated GraphQL Queries, Mutations and Subscription, and a JSON representation of our GraphQL schema. These files are used as input to generate API.service.ts. We will look at these files later in the article.\nFilling the data source\nWe will start by updating our post.service.ts to save new posts to our Amplify backend instead of in the mockObject. We will make the following changes:\n//Other imports\r\nimport { APIService } from 'src/app/API.service'; // <--- Add this\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class PostService {\r\n\u00a0 //other code\r\n\r\n\u00a0 constructor(private api: APIService) { // <--- Add api to constructor\r\n\u00a0 }\r\n\r\n\u00a0 addPost(title: string, description: string) { // <--- replace current addPost with new code\r\n\u00a0 \u00a0 this.api.CreatePost({\r\n\u00a0 \u00a0 \u00a0 title,\r\n\u00a0 \u00a0 \u00a0 description\r\n\u00a0 \u00a0 });\r\n\u00a0 }\r\n\r\n\u00a0 //other code\r\n}\nHere we import the generated API service and use one of the generated functions, CreatePost, to create a new post item to our backend. If you search for the definition of the CreatePost function in src/app/API.service you can see which parameters it expects. In this case adding a title and description is enough.\u00a0\nNow when we add posts, they will be persisted.\u00a0\nRetrieving data from the backend\nThe next step is to retrieve all our posts from the backend. We will make the following changes to our post.service.ts:\n// other imports\r\nimport { APIService, Post as AmplifyPost } from 'src/app/API.service'; // <--- Add this new import\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class PostService {\r\n\u00a0 private posts: BehaviorSubject<Post[]> = new BehaviorSubject<Post[]>([]); // <--- remove mockposts and its usage completely\r\n\u00a0 private postsData: Post[] = [];\r\n\r\n\u00a0 constructor(private api: APIService, private userService: UserService) {\r\n\u00a0 \u00a0 this.setOnPostCreateSubscription();\r\n\u00a0 }\r\n\r\n\u00a0 // Other code\r\n\u00a0 \r\n\u00a0 //Update this function\r\n\u00a0 getAllPosts(): Observable<Post[]> {\r\n\u00a0 \u00a0 this.api.ListPosts().then(response => {\r\n\u00a0 \u00a0 \u00a0 const responsePosts: Post[] = [];\r\n\u00a0 \u00a0 \u00a0 response.items.forEach(item => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 const post = this.convertToPost(item as AmplifyPost);\r\n\u00a0 \u00a0 \u00a0 \u00a0 responsePosts.push(post);\r\n\u00a0 \u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 \u00a0 this.posts.next(responsePosts);\r\n\u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 return this.posts.asObservable();\r\n\u00a0 }\r\n\r\n\u00a0 //Add this function\r\n\u00a0 private convertToPost(amplifyPost: AmplifyPost): Post {\r\n\u00a0 \u00a0 const {\r\n\u00a0 \u00a0 \u00a0 id,\r\n\u00a0 \u00a0 \u00a0 title,\r\n\u00a0 \u00a0 \u00a0 description,\r\n\u00a0 \u00a0 \u00a0 owner,\r\n\u00a0 \u00a0 \u00a0 createdAt,\r\n\u00a0 \u00a0 \u00a0 likes,\r\n\u00a0 \u00a0 \u00a0 comments\r\n\u00a0 \u00a0 } = amplifyPost;\r\n\r\n\u00a0 \u00a0 const likesItems = likes ? likes.items : undefined;\r\n\u00a0 \u00a0 const commentsItems = comments ? comments.items : undefined;\r\n\r\n\u00a0 \u00a0 return {\r\n\u00a0 \u00a0 \u00a0 id,\r\n\u00a0 \u00a0 \u00a0 title,\r\n\u00a0 \u00a0 \u00a0 description: description ? description : '',\r\n\u00a0 \u00a0 \u00a0 author: owner ? owner : '',\r\n\u00a0 \u00a0 \u00a0 date: new Date(createdAt),\r\n\u00a0 \u00a0 \u00a0 likes: likesItems ? likesItems.length : 0,\r\n\u00a0 \u00a0 \u00a0 comments: commentsItems ? commentsItems.length : 0\r\n\u00a0 \u00a0 };\r\n\u00a0 }\r\n}\nWe are doing a few things here:\n\nImporting the generated Post interface and giving it an alias since Post already exists and is used for our Post component UI model\nRemoving the mock data completely since this service will now be serving data from the Amplify backend\nUsing the listPosts generated function to get all posts in our backend.\nConverting the posts retrieved from the amplify backend to the Post model we want to use in our UI. A significant difference here is that we want to count the number of likes and comments and return numbers.\u00a0\n\nIf we run our app now, we should now see new data retrieved from the backend. If we add a new post and refresh the page ,we should see the latest post in the list.\nSubscribing to data\nIt would be nice to have the new post show up in the list immediately after it is added. One way to achieve this is to call getAllPosts after the addPosts was successful. There is a better way, though: Amplify GraphQL also allows us to subscribe to certain events.\u00a0\nIn this case, we can subscribe to the event of a post being created by making the following changes to our post.service.ts:\nimport { UserService, UserInfo } from '../shared/user.service'; // Add this import\r\n\r\n@Injectable({\r\n\u00a0 providedIn: 'root'\r\n})\r\nexport class PostService {\r\n\u00a0 private postsData: Post[] = []; // <-- add this\r\n\r\n\u00a0 constructor(private api: APIService, private userService: UserService) {\r\n\u00a0 \u00a0 this.setOnPostCreateSubscription(); // <-- add this\r\n\u00a0 }\r\n\r\n\u00a0 //Add this function\r\n\u00a0 private async setOnPostCreateSubscription() {\r\n\u00a0 \u00a0 const userInfo: UserInfo = await this.userService.getCurrentUserInfo();\r\n\r\n\u00a0 \u00a0 if (userInfo) {\r\n\u00a0 \u00a0 \u00a0 this.api.OnCreatePostListener(userInfo.username).subscribe(response => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 const responseData = response.value.data;\r\n\u00a0 \u00a0 \u00a0 \u00a0 if (responseData && responseData.onCreatePost) {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const post = this.convertToPost(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 responseData.onCreatePost as AmplifyPost\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 );\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.postsData.push(post);\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.posts.next(this.postsData);\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 });\r\n\u00a0 \u00a0 }\r\n\u00a0 }\r\n\r\n\u00a0 getAllPosts(): Observable<Post[]> {\r\n\u00a0 \u00a0 this.api.CustomListPosts().then(response => {\r\n\u00a0 \u00a0 \u00a0 const responsePosts: Post[] = [];\r\n\u00a0 \u00a0 \u00a0 response.items.forEach(item => {\r\n\u00a0 \u00a0 \u00a0 \u00a0 const post = this.convertToPost(item as AmplifyPost);\r\n\u00a0 \u00a0 \u00a0 \u00a0 responsePosts.push(post);\r\n\u00a0 \u00a0 \u00a0 });\r\n\r\n\u00a0 \u00a0 \u00a0 this.postsData = responsePosts; // <-- add this\r\n\u00a0 \u00a0 \u00a0 this.posts.next(responsePosts);\r\n\u00a0 \u00a0 });\r\n\u00a0 \u00a0 return this.posts.asObservable();\r\n\u00a0 }\r\n}\nWe made these changes:\n\nImport the UserService we created in the last section. This is needed because of a limitation in Amplify GraphQL subscription (see below).\nCreated an array that holds the state of the latest data retrieved. We need this since the subscription only returns the new post added. We need to add the newly created post to this list and return it to the component.\nWe call the onCreatePostListener with the username of the currently logged in user and we convert the response of this subscription to our Post model object, similarly to how we did this for the query.\n\nAmplify GraphQL limitation\nCurrently it is not possible to have the owner auth rule in your GraphQL schema and have a subscription on an object without supplying the owner username. If you want to be able to have a subscription on all objects of a type, regardless of who created it, you need to remove the owner auth rule in your schema. However, this means that you will no longer get the owner field in all of your items in the database and that any logged in user can also update and delete posts that don\u2019t belong to them.\u00a0\nFor the sake of having a working subscription example, we have created a subscription for the current user, meaning you will only get updated for posts that you post yourself. Hopefully in the future we will be able to have more flexibility regarding subscriptions. Nevertheless, this example should still show how easy it is to setup subscriptions and there are still several use cases where this might be useful.\u00a0\nLikes and comments\nFor the Like and Comment services we followed steps similar to those followed for the Post. You can view all of these steps in the Git commits we will list at the end of this section. However, there are some key points we still want to cover that we encounter when migrating likes and comments to the Amplify backend.\u00a0\nGraphQL query with filters and pagination\nWhen we want to query the likes or comments, we only want to query them if they belong to the Post that we are currently looking at in the frontend. Luckily, the generated functions we use to query our backend come with built-in filtering and pagination functionality.\u00a0\nIf we look at the definition of the ListLikes function in the API.service.ts, we can see the following signature:\nasync ListLikes(\r\n\u00a0 \u00a0 filter?: ModelLikeFilterInput,\r\n\u00a0 \u00a0 limit?: number,\r\n\u00a0 \u00a0 nextToken?: string\r\n\u00a0 ): Promise<ListLikesQuery> {\r\n\r\n //code\r\n\r\n }\nThe ModelLikeFilterInput contains options to filter the results based on several conditions. Click through them to see all of the possibilities. We can also see a limit and a nextToken parameters which are used for pagination. Since we did not build pagination in this application yet, we will refer to the pagination documentation.\nTo query only the likes that belong to a certain post, we use the following code:\ngetLikesForPostId(postId: string) {\r\n\u00a0 \u00a0 this.api.ListLikes({ postLikesId: { eq: postId } }).then(response => {\r\n\u00a0 \u00a0 //other code\nCustom GraphQL Queries, Mutations and Subscriptions\nIf you follow the patterns we have shown up to now and have posts, likes and comments using the Amplify backend, you will notice that all posts still show 0 likes and comments on the post overview. This is a bug, and has to do with the values that are retrieved in the generated GraphQL query used in the listPosts function.\u00a0\nIf we check the definition of the query in the src/graphql/queries.graphl we will see the following:\nquery ListPosts(\r\n\u00a0 $filter: ModelPostFilterInput\r\n\u00a0 $limit: Int\r\n\u00a0 $nextToken: String\r\n) {\r\n\u00a0 listPosts(filter: $filter, limit: $limit, nextToken: $nextToken) {\r\n\u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 title\r\n\u00a0 \u00a0 \u00a0 description\r\n\u00a0 \u00a0 \u00a0 likes {\r\n\u00a0 \u00a0 \u00a0 \u00a0 nextToken\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 comments {\r\n\u00a0 \u00a0 \u00a0 \u00a0 nextToken\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 createdAt\r\n\u00a0 \u00a0 \u00a0 updatedAt\r\n\u00a0 \u00a0 \u00a0 owner\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 nextToken\r\n\u00a0 }\r\n}\nWe can see that for the likes and comments, only the nextToken field is being retrieved. The items field is not being retrieved at all, which is why our frontend is defaulting to 0. Thankfully, we can define our own queries.\nWe can create a new file called src/app/custom-queries.graphql with the following content:\nquery CustomListPosts(\r\n\u00a0 $filter: ModelPostFilterInput\r\n\u00a0 $limit: Int\r\n\u00a0 $nextToken: String\r\n) {\r\n\u00a0 listPosts(filter: $filter, limit: $limit, nextToken: $nextToken) {\r\n\u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 title\r\n\u00a0 \u00a0 \u00a0 description\r\n\u00a0 \u00a0 \u00a0 likes {\r\n\u00a0 \u00a0 \u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 comments {\r\n\u00a0 \u00a0 \u00a0 \u00a0 items {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 id\r\n\u00a0 \u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 }\r\n\u00a0 \u00a0 \u00a0 createdAt\r\n\u00a0 \u00a0 \u00a0 updatedAt\r\n\u00a0 \u00a0 \u00a0 owner\r\n\u00a0 \u00a0 }\r\n\u00a0 \u00a0 nextToken\r\n\u00a0 }\r\n}\nWe now query the items for likes and comments and only retrieve the ids, since we only want to count them. Once we add this file, we can run the following in the terminal:\namplify codegen\nThis will generate new code that you can use in your frontend without having to run amplify push. Now we can update our post.service.ts to make use of this new query:\ngetAllPosts(): Observable<Post[]> {\r\n\u00a0 \u00a0 this.api.CustomListPosts().then(response => { // <-- updated here\r\n\u00a0 \u00a0 \u00a0 //code\nIf we run the application now we should see the correct counters for likes and comments.\nCustom GraphQL Resolvers\nIn our current example we retrieve likes and comments and we count them in the frontend to show the counters. We could also update our GraphQL schema for Post to include a likesCount and commentsCount fields and define custom logic for these fields.\u00a0\nThis would let the backend calculate these numbers for us and return them to the frontend. A demo of this would be too long for this article, so we will refer to the documentation for AWS Lambda resolver configuration, which explains how to set this up.\nWe\u2019ve introduced quite some changes:\n\nWe added an API endpoint.\nWe updated the GraphQL schema.\nWe pushed the changes to the backend.\nWe added Posts to the GraphQL backend.\nWe retrieved Posts from the GraphQL backend.\nWe added subscriptions for Posts.\nWe added Likes \u00a0to the GraphQL backend.\nWe retrieved Likes from the GraphQL backend.\nWe \u00a0fixed Posts subscriptions.\nWe created a custom query to list Posts.\nWe added a subscription for likes.\nWe added Comments to the GraphQL backend.\nWe retrieve Comments from the GraphQL backend.\nWe added subscriptions for Comments.\n\nHosting our application\nNow that we have our application working locally, it is time to host it on AWS so that it is available online. We are going to do this by adding the Amplify hosting category. Before we do that, we need to do these steps:\n\nLog in to the AWS console with your default browser.\nPush all your code to Git.\nUpdate the initial budget in angular.json to 2mb.\n\nWe will run the following command to add hosting:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify add hosting\r\n? Select the plugin module to execute Hosting with Amplify Console (Managed hosting with custom domains, Continuous deployment)\r\n? Choose a type Continuous deployment (Git-based deployments)\r\n? Continuous deployment is configured in the Amplify Console. Please hit enter once you connect your repository\nThis will open the AWS console that will enable us to connect our repository to the CI/CD pipeline:\n\nClick on the Hosting environments tab and select your Git repository:\n\nClick on connect branch. Once you are authenticated with your Git repository, select the repository and branch where the latest version of your code exists and click on Next.\nOn the next page it is important that you select an environment to use for this deployment. In this case we have only created a dev environment, so we will use that:\n\nIn case you don\u2019t have an existing service role for Amplify projects, click on Create new role to generate one and click on the refresh button to see it appear as an option.\nIf you scroll to the bottom you will see the default build settings. For now these will work, however if in the future you want to extend the build to include more phases or steps you can always change this.\nWhen we click on next we will get a summary of the options we selected. Review them and click on Save and deploy.\nFrom this point on, everytime you push to the develop branch a build will start and the live application will get updated. This is what it looks like when it is building:\n\nYou can click on any of the phases, such as provision to see the detailed logging of what is happening:\n\nWhen it is done building and deploying, we can see that all steps have passed and that there is a link to test out our app:\n\nConfiguring redirects for our SPA\nIf you click on the link you should see your application where you can log in and should see the main page. However, if you click on API you will get an access denied error. For single page applications we need to add an extra setting in the Amplify console in our project related to rewrites and redirects:\n\nThe settings are:\n\nSource address: </^[^.]+$|\\.(?!(css|gif|ico|jpg|js|png|txt|svg|woff|woff2|ttf|map|json)$)([^.]+$)/>.\nTarget address: /index.html.\nType: 200 (Rewrite)\nCountry: (leave empty).\n\nFor details, refer to the Amplify documentation for single page application redirects.\n(Extra) Linking a domain name to your deployment\nOne extra step you could take is to link a domain name to one of your deployments. In this case we are going to create a new environment for our project, link it to the master branch and then link the domain name \u201ctheamplifyapp.com\u201d to that deployment.\nTo start off, I will run the following command in the terminal:\nEvertsons-MBP:theamplifyapp evertsoncroes$ amplify init\r\nNote: It is recommended to run this command from the root of your app directory\r\n? Do you want to use an existing environment? No\r\n? Enter a name for the environment prod\r\n? Choose your default editor: Visual Studio Code\r\nUsing default provider\u00a0 awscloudformation\r\n? Select the authentication method you want to use: AWS profile\r\n\r\nFor more information on AWS Profiles, see:\r\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\r\n\r\n? Please choose the profile you want to use default\r\nAdding backend environment prod to AWS Amplify app: d1ekyrd95b627y\r\n\u2834 Initializing project in the cloud...\nOnce that is done we can run amplify push and follow the steps we made for the dev environment.\nAfter that, we can go into the Amplify console, click on General, scroll to the bottom and click on connect branch. There we will select the master branch and the prod environment:\n\nThen we click on next\u201d and save and deploy and wait for the build to be successful.\nAfter that is done, we can go to Route53 in the AWS console and register a domain name. Note that this will cost money! In this case I have registered the domain theamplifyapp.com.\nBack in the Amplify console, click on the Domain management tab and on Add domain. We should find our registered domain in the available domains and select it. Then we click on Configure domain and set it to the master branch build:\nOnce we click on Save the process should get started. When it is complete, we can visit our application on our brand new domain!\nWe made two major changes:\n\nWe\u2019ve added Amplify hosting.\nWe\u2019ve updated our Angular budget to 2Mb.Updated Angular budget to 2mb\n\nUp next\u2026\nWith just a few Amplify commands and code changes we have:\n\nCreated a cloud-native application\u2026\nProtected with authentication\u2026\u00a0\nBacked by a GraphQL backend\u2026\nWith a fully configured CI/CD pipeline\u2026\nHosted on the AWS cloud\u2026\nConnected through a registered domain name\n\nFor each of the categories taken there are still plenty of customization options to explore. However, hopefully it has become clear how powerful Amplify can be.\u00a0\nIn the next installment of this series we will continue working on our application and we will add the Amplify Storage category. We will use this to create an image library where we can upload images.\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 31593, "title": "The Evolution of AWS from a Cloud-Native Development Perspective: Serverless, Event-Driven, Developer-Friendly, Sustainable", "url": "https://www.luminis.eu/blog/the-evolution-of-aws-from-a-cloud-native-development-perspective-serverless-event-driven-developer-friendly-sustainable/", "updated_at": "2023-04-20T13:42:59", "body": "Recent AWS announcements feature fewer service additions and instead underline the efforts the cloud giant is undertaking to increase the strength of its massive portfolio of building blocks. AWS CEO Adam Selipsky has mapped out a course that intensifies AWS\u2019s focus on serverless technologies, event-driven architectures, an improved developer experience, and sustainability.\n\nIn this blog post, we outline the direction cloud-native engineering is heading in, how you can learn to leverage AWS\u2019s portfolio of managed services and primitives to your advantage, and how cloud technology is reshaping the field of software engineering.\nIf you prefer watching a video, we got you covered! We recently broadcasted a cloud-native development focussed AWS re:Invent re:Cap featuring most of this article\u2019s themes. You can find the recording at the end of this post.\nWhat AWS is telling us\nWith so many services in its portfolio already, this year\u2019s AWS re:Invent didn\u2019t hold any massively surprising announcements. With the yearly conference reaching its 10-year milestone \u2014 and AWS being around for 15 years \u2014 the AWS platform is maturing while at the same time evolving to meet market demands.\nAWS leadership\u2019s re:Invent keynotes\nFreshly at the helm of AWS, Adam Selipsky held his first re:Invent Keynote. Selipsky\u2019s primary message: AWS is shifting from merely offering infrastructure primitives to embracing the idea of being a so-called platform of platforms. Therefore, AWS\u2019s offerings will increasingly become an interesting piece of organizational value chains.\nWhile Amazon CTO Werner Vogels took the stage after an introduction referencing Fear and Loathing in Las Vegas, his keynote wasn\u2019t as spectacular. Dr. Vogels took the time to make a case for Well-Architected applications, which we interpret as a good signal for builders. Other highlights were the release of Construct Hub and the CDK Patterns library.\n\nPrimitives, not frameworks\nAn important theme these last few months was the expansion and refinement of AWS as a cloud-native platform. You can view AWS as a data center containing compute resources and seemingly limitless storage at the other end of the internet, or maybe as a big bunch of infrastructural primitives. The way we look at the platform is by grouping its services into three categories:\n\nInfrastructural primitives: storage, networking, and compute (a.k.a. Infrastructure as a Service, IaaS).\nMiddleware-oriented services: web servers, container orchestrators, databases, message queues (a.k.a. Platform as a Service, PaaS).\nServerless: take what you need, pay-as-you-go.\n\nBuilding something on a platform containing 200+ services might seem a bit overwhelming. Therefore, from the perspective of cloud-native development, we use this rule of thumb: start building from the most modern collection of services first and, if needed, complement the solution using more primitive building blocks.\nFrequently, you will end up with a serverless solution that is rapidly and almost infinitely scalable and highly cost-effective. However, sometimes there are missing pieces of the puzzle, and you can\u2019t complete it with serverless services. We can then turn to platform primitives or infrastructure services like containers, RDS, virtual machines, or even complete SaaS solutions. Don\u2019t forget to update your solutions from time to time, as the platform never stops evolving.\nReduce risk, improve performance, grow revenue, increase efficiency\nEffectively adopting the cloud means making the cloud work for your money. It\u2019s not just a virtualized data center, or at least: that\u2019s not the mindset that enables you to accelerate your organization towards its goals. Cloud-native development requires understanding cloud-native best practices in terms of performance, resiliency, cost optimization, and security. Luckily, AWS keeps investing in adoption frameworks, its Well-Architected Framework, and certification and training programs.\n\nThe ongoing evolution of serverless: commoditize all the things\nYou can immediately tell the serverless category from above is something special. Its services allow you to focus on functional value without worrying about what is needed to run it. You can compare a serverless service to a commodity, like electricity: you use it anytime you need it, without much second thought. For a while, serverless was synonymous with functions (FaaS) or Lambda, but its reach rapidly extends beyond compute to data stores and application integration.\nServerless technology keeps gaining traction, as we can see in its continuous evolution. Let\u2019s look at AWS\u2019s current serverless portfolio and see how it evolves.\nThree categories of building blocks\nRoughly speaking, we can divide AWS\u2019s serverless building blocks into three categories:\n\nCompute: running code without provisioning servers (Lambda, Fargate).\nData Storage: storing objects, documents, relational and other data (Simple Storage Service (S3), DynamoDB, Relational Database Service Proxy, Aurora Serverless).\n(Application) integration: EventBridge, Step Functions, Simple Queue Service (SQS), Simple Notification Service (SNS), API Gateway, AppSync.\n\nCombining these building blocks, you can quickly create valuable solutions at scale.\n\nServerless expansion areas\nWhile SaaS is powerful and often offers excellent value for money, it has significant constraints in terms of flexibility. To win in the marketplace, we need more flexibility for our differentiating technology. These are the lines along which serverless is evolving. Its journey started at compute, but the serverless philosophy quickly moved into other AWS categories.\nBig data and streaming\nAmazon MSK will get a serverless counterpart, which is great news for organizations running Kafka workloads. The same goes for AWS\u2019s big data platform Amazon EMR and its data warehouse offering Amazon Redshift. Amazon Kinesis is also evolving its data streaming service, exemplified by the new Data Streams On-Demand feature. Last but certainly not least: AWS Data Exchange for APIs makes it a breeze to use third-party data while leveraging AWS-native authentication and governance.\nApplication integration\nApart from getting extra ephemeral storage, AWS Lambda now supports partial batch responses for SQS and event source filtering for Kinesis, DynamoDB, and SQS. EventBridge added Amazon S3 Notifications. On the surface, that might sound like a small change. But as Matt Coulter puts it: code is a liability, not an asset. Anything you can push to the cloud vendor gains you some security and an increased ability to focus on value. You can now control Amazon Athena workflows using AWS Step Functions in that same vein.\nAre applications, infrastructure, and integration separable?\nNow that serverless is moving beyond compute into the data and integration layers, we might ask ourselves: are applications, infrastructure, and integration separable? Is programming still mostly about functions and logic, and where does it bleed into the domain of operations? The advent of serverless is pushing boundaries everywhere and blurring previously clear and stable lines.\nThat\u2019s excellent news, in our opinion: the more control and flexibility our teams have, the faster we can innovate. Gregor Hohpe, an Enterprise Strategist at AWS, says: \u201cI firmly believe that we are just starting to realize the true potential of cloud automation. Modern cloud automation isn\u2019t just about reducing toil. (\u2026) it can help us blur the line between application and automation code.\u201d We agree wholeheartedly.\n\nThe future (of cloud) is event-driven\nAnother direction cloud technology developments are pointing towards is that of event-driven architectures. Not only is the cloud itself highly event-driven, but applications built on cloud platforms also tend to be fitting most naturally when using the power of events. Why is that?\nAgility in architecture\nMarket forces are driving organizations to become more agile. As Jeff Bezos puts it: \u201cThe only sustainable advantage you can have over others is agility, that\u2019s it.\u201d Organizations must learn to leverage technology to respond to and drive change to fulfill this business requirement. Event-driven architectures present the necessary characteristics:\n\nLoosely coupled: heterogeneous services and share information effectively while hiding implementation details.\nScalable: multiple types of consumers can work in parallel, massively if needed.\nIndependent: evolution, scaling, and failing can happen in isolation, especially when using buffers like event routers.\nAuditable: if events are routed and stored centrally, we can quickly implement auditing, policy enforcement, and information access control.\nFrugal: if we push changes when available, we don\u2019t need to waste resources continuously polling for changes.\n\nEvent-driven systems enable us to focus on system behavior and temporality within a business domain, freeing us from the chains of rigid structures incapable of dealing with changing needs and requirements. There is a trade-off (of course): agile systems are complex and thus force us to learn to manage \u2014 seeming, but not actual \u2014 chaos.\n\nAWS building blocks for event-driven architectures\nNow, how is AWS helping us compose these systems? Whether you are building event-sourced systems, leverage CQRS, or are just notifying components, AWS has the building blocks you need:\n\nProducers: most AWS primitives, especially serverless ones, produce events. For example, we can feed Lambda with events from API Gateway, CloudWatch, DynamoDB, S3, Kinesis, and many more services. Besides that, we can use custom apps, SaaS solutions, and microservices as event producers.\nConsumers: notable consumers are AWS Lambda, Amazon SQS, Amazon Kinesis Data Firehose, and Amazon SNS. We can also use APIs from SaaS offerings and custom apps as event consumers.\nRouters: we can fulfill most integration needs using Amazon EventBridge and Amazon SNS.\nWell-Architected: the AWS Well-Achitected Framework describes best practices, which we can assess and monitor using the AWS Well-Architected Tool.\n\nCombining these AWS services and using them to integrate SaaS and self-built products gives us the leverage we miss when only using off-the-shelf products.\nHow are event-driven cloud architectures evolving?\nThe answer to this question is short and straightforward: along the same lines as serverless. Last December, AWS mostly announced incremental improvements. But they quickly add up, accumulating in an ever-more powerful platform to build and evolve our solutions on. And the announcements don\u2019t stop when re:Invent is done: AWS is working to improve its event-driven primitives year-round.\nAmazon Eventbridge is an excellent example of this continuous investment. It was introduced in mid-2019 and has gained a lot of features since then: a schema registry, archiving and replaying of events, and a long list of event destinations. Its evolving event pattern matching, filtering, and routing options embody the \u2018code is liability\u2019 philosophy, enabling us to focus on value delivery. Recently, S3 Event Notifications were added to EventBridge\u2019s features, giving us more direct, reliable, and developer-friendly options when responding to S3 object changes. For the web-scale organizations among us, AWS introduced cross-region event routing.\nWe mentioned Lambda\u2019s new message filtering features above but want to highlight them again since they underline AWS\u2019s continued investment in this area. Lastly, AWS IoT TwinMaker deserves some attention: it utilizes events as data sources and enables developers to create digital twins of real-world systems, which is nothing less than fantastic.\n\nImproving the developer experience\nAWS provides us with many building blocks, but using them effectively is not straightforward. Luckily for us, AWS seems to understand this problem and is steadily improving in that area.\nProgrammable everything: APIs, SDKs, and the AWS CDK\nIn 2002, Jeff Bezos\u2019s so-called API Mandate forced all Amazon teams to expose their data and functionality through service interfaces. Amazon has built AWS around the same principles: every service is programmatically controllable, from starting a virtual machine to accessing a satellite. While this is an essential property of an effective cloud platform, it is not necessarily developer-friendly.\nBy now, AWS has incrementally and significantly improved in this space. Besides using their APIs, we can control services and infrastructure with a unified Command Line Interface (CLI), Software Development Kits (SDK), CloudFormation, and, since July 2019, the AWS Cloud Development Kit (CDK).\nThe CDK had a massive impact on developer productivity and satisfaction. While teams could already access and control services and infrastructure using the AWS SDK and their favorite programming language, infrastructure was primarily defined using incredible amounts of mostly punctuation marks and whitespace, also known as YAML or JSON. CDK \u2014 initially only flavored TypeScript and Python \u2014 finally gave developers an AWS-native means to define Infrastructure as actual Code. Since its introduction, AWS has added support for more languages, like Java, C#, and Go.\nMetrics, toolsets, and communities\nThe lines between infrastructure and logical code have thus been blurred, creating more opportunities to increase business agility. And AWS provides more tools to increase development productivity, like metrics and purpose-built toolsets. New developments in this space are:\n\nCommunity support platforms like the newly announced re:Post and Construct Hub.\nMonitoring improvements like CloudWatch Real-User Monitoring and Metrics Insights.\nA Secrets Detector addition to the software engineer\u2019s best non-human programming buddy, Amazon CodeGuru.\n\n\nReducing complexity\nAnother way to increase (business) agility is by reducing complexity. AWS seems very aware of the complexity we add when developing cloud-native solutions. It has released several offerings and improvements to its platform over the last period in reaction to this.\nLess handcrafted code\nWe can reduce the amount of hand-crafted code we deploy in several areas (once more: code is a liability!). Business analysts can kickstart machine learning workloads using Amazon SageMaker Canvas, while others can visually create full-stack apps using Amplify Studio. More incremental improvements are enhanced dead-letter queue management for SQS and the Lambda and S3 enhancements mentioned earlier.\nFeature flags and dark launches\nReducing operational complexity is a way to increase productivity. Modern organizations have learned to leverage feature flags and dark launches to test changes without introducing much operational overhead. Our colleague Nico Krijnen took the time to write down how Amazon CloudWatch Evidently and AWS AppConfig Feature Flags can help us in this regard.\nMigration automation\nLastly, a powerful \u2014 but often not so straightforward \u2014 way to reduce complexity is to migrate existing workloads from the old data center to the cloud. AWS has been investing a lot of effort in this space and continues to do so, as evidenced by the recent releases of AWS Migration Hub Refactor Spaces, AWS Mainframe Modernisation, and AWS MicroService Extractor for .NET.\n\nSustainability of the cloud, sustainability in the cloud\nSoftware engineering has more dimensions than speed and complexity. With the advent of DevOps, FinOps, and the increased awareness of IT infrastructure\u2019s impact on our environment, teams are increasingly responsible for more than just developing and deploying code.\nAdrian Cockcroft, Amazon\u2019s newly appointed VP of Sustainability Architecture, recently headed a very insightful talk on architecting for sustainability. Amazon has committed to net-zero carbon by 2040, and AWS aims to use 100% renewable energy by 2025. AWS needs their customer\u2019s help to decrease their footprint, so AWS has introduced several sustainability tools and improvements to their platform.\nThe Customer Carbon Footprint Tool was released earlier this month, enabling AWS users to calculate the carbon emissions their workloads produce now and in the future. Reducing them is the next logical step, which is why AWS added a new Sustainability Pillar to their Well-Architected Framework. More concretely, teams optimize resource usage by choosing several new CPU instances for EC2 or Lambda or by analyzing the enhanced AWS Compute Optimizer infrastructure metrics and acting accordingly.\n\nSurvival of the fittest\nAWS\u2019s cloud platform is becoming more mature, stable, developer-friendly, and sustainable while at the same time evolving rapidly to meet emerging business needs. By closely following the needs of its users and experimenting and growing with them, AWS keeps delivering the platform of the future.\nFor us cloud-native developers, that\u2019s excellent news. Increasing business agility by creating and evolving event-driven, serverless, sustainable systems that can turn on a dime in response to customer needs: it\u2019s what we need now and going forward.\n\nLearn from our experts:25 Jan 2021-Piet van DongenTraining: AWS overviewIn this training we will take you through the main services of AWS (including Lambda, EC2, S3, Elastic Beanstalk). Not only do we explain what these services mean, but also in short demos we show what possible applications are and...\n", "tags": ["aws", "cloud", "evolution", "future"], "categories": ["Blog", "Cloud"]}
{"post_id": 31377, "title": "Production-Ready CDK \u2013 CDK Pipelines", "url": "https://www.luminis.eu/blog/production-ready-cdk-cdk-pipelines/", "updated_at": "2023-01-27T12:55:14", "body": "We initiated our AWS CDK project in the previous chapter and focused on the project structure. Now, we can leverage CI/CD to speed up the technical value stream. Besides, as the project gets bigger, it becomes more challenging to automate; therefore, why not do it initially?\n\nThe most powerful feature of Cloud Development Kits is abstracting complex cloud applications and, as a result, making the cloud more straightforward. AWS CDK does this also for CI/CD pipelines by offering a module called CDK Pipelines.\nCDK Pipelines save us from writing a lot of code, configuration, and wiring. I have used and tried many CI/CD tools, and CDK Pipelines way is one of the most painless ways of implementing CI/CD. For a more detailed introduction, you can check the documentation. We won\u2019t recap it, but we will focus on real-life scenarios in production-like environments.\nIf you tried CDK Pipelines in the past and did it with the old ways, you might disagree with this statement. I should say: I agree with you. And to clarify, there was another construct that was not the most intuitive in the past. But that one is deprecated. So from now on, we only use this CodePipeline construct from the CDK Pipelines library, which I think works as it should be. The code is compact and opinionated in a beautiful way.\nLet\u2019s start by taking a step back and thinking about the positioning before jumping into the code.\n\u00a0\nDon\u2019t use \u2018cdk deploy\u2019 in your pipelines\n\u201ccdk deploy command is convenient, I always use it for my local CDK code, and I can also use it at my CI/CD pipelines. First, I use the CI/CD tool I want and do the CI part, then just deploy with a single command, easy peasy.\u201d\u200a\u2014\u200aprobably someone who doesn\u2019t care about CI/CD\nWell, you can, but should you?\nI see this in many CI/CD pipelines used for CDK projects, from simple PoCs to production environments in enterprises. cdk deploy command is quick and straightforward, but the intended purpose is fast development, not CI/CD pipelines. Why? Because as your application gets more extensive, you will have more CI/CD steps, more CloudFormation stacks, and cross-account or multi-region deployments. It might seem easy, but it is the dirty way in the long run.\nSecond, when using cdk deploy in a CI/CD pipeline, you must give deployment-related IAM permissions. What happens if your CI/CD tool is compromised? They will have access to your AWS account to deploy stacks or, maybe even if you don\u2019t handle permissions right, destroy existing stacks. It sounds improbable, but it is not the well-architected way. We should be reducing permissions continuously.\nI am not arguing that we shouldn\u2019t be using the command at all, only saying it has a specific purpose which is not secure or complex pipeline scenarios.\n\u00a0\nRestricting to the\u00a0Minimum\nOkay, cdk deploy is out of the equation. So what is the right way?\nWe need to give the least privileges for controlling AWS resources from the CI/CD tooling. One way to do that is by providing only an S3 file upload permission to the IAM role used by the CI/CD tool and creating a deployment pipeline on AWS for the deployment step. We can implement more sophisticated controls in this way, like adding more checks or having manual approvals at different stages.\nFurthermore, since only the S3 file upload action can be compromised, it becomes harder to deploy/destroy resources.\nWe can achieve this in two ways:\n\nUse Git to upload artifacts to S3, then do CI and CD on AWS using CDK Pipelines.\nBuild artifacts with a CI tool, then upload artifacts to S3, finally do CD part on AWS using CDK Pipelines.\n\nLet\u2019s see what the first one looks like:\nWay 1: CDK Pipeline for both Continuous Integration and Continuous Deployment\n\u00a0\nIf you are not starting your software development from scratch for yourself or your company, you should already have at least one CI/CD tooling in use. As a result, you will have CI steps already figured out and implemented before. Good news and this takes us to the second way. We can use the CI tooling and still use CDK Pipelines for CD. This way is the way I use most at my projects:\nWay 2: CDK Pipeline for only Continuous Deployment\n\u00a0\nIntegrating Github and\u00a0AWS\nWe discussed keeping things at a minimum with S3 upload permission. For simplicity, we will skip the CI and go similar to the first way. We will push changes from Git to AWS directly and then deploy them as CloudFormation Stacks.\nLuckily, we can simplify more and skip having an S3 Bucket part. Since we use Github, we can utilize the Github-AWS integration, namely CodeStar Connection. In this way, we use only CodeStar permission to deploy stacks (instead of S3 upload permission). It looks like this at the end:\nWay 3: Git-CDK Pipeline integration using CodeStar Connections\nYou can do it in a minute by using the API for it. Or even easier, you can go to one of the Developer Tooling Services of AWS like CodePipeline, click Settings, click Connections, and finally click Create Connection. Follow the steps and give the permissions you need.\n\n\u00a0\nVoil\u00e0, ready for the CDK Pipeline!\n\u00a0\nAdding CDK Pipeline to the\u00a0Project\nIf the stack from the previous article still exists, you should start over by destroying it using cdk destroy or npx projen destroy as I explained before.\nLet\u2019s start by separating the Lambda Stack from src/main.ts. We create a new file with the path src/lambda-stack.ts:\nimport { Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\n\r\n// example cdk app stack\r\nexport class LambdaStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    new lambda.Function(this, 'ExampleFunction', {\r\n      functionName: 'example-lambda',\r\n      code: lambda.Code.fromAsset('lambda'),\r\n      handler: 'hello.handler',\r\n      runtime: lambda.Runtime.NODEJS_14_X,\r\n    });\r\n  }\r\n}\r\n\r\n\nThen implement the pipeline at a new file with the path src/cdk-pipeline-stack.ts.\nimport { Stack, StackProps, Stage } from 'aws-cdk-lib';\r\nimport { CodePipeline, CodePipelineSource, ShellStep } from 'aws-cdk-lib/pipelines';\r\nimport { Construct } from 'constructs';\r\nimport { LambdaStack } from './lambda-stack';\r\n\r\n// 3a. We define a Lambda Stage that deploys the Lambda Stack. \r\nexport class LambdaStage extends Stage {\r\n  constructor(scope: Construct, id: string) {\r\n    super(scope, id);\r\n    new LambdaStack(this, 'LambdaStack');\r\n  }\r\n}\r\n\r\nexport class CdkPipelineStack extends Stack {\r\n  constructor(scope: Construct, id: string, props?: StackProps) {\r\n    super(scope, id, props);\r\n\r\n    // 1. We import the CodeStar Connection for Github-CDK Pipeline integration. Therefore, \r\n    // you only need to provide the ARN of the Connection.\r\n    const codePipelineSource = CodePipelineSource.connection('cagingulsen/prod-ready-cdk','main', { \r\n      connectionArn: 'arn:aws:codestar-connections:eu-west-1:YOUR_ACCOUNTI_D:connection/YOUR_CONNECTION_ID'\r\n      },\r\n    );\r\n\r\n    // 2. We define the CDK Pipeline using the source from the first step and \r\n    // use three commands for the synth step. We install dependencies from the yarn.lock file \r\n    // with yarn install --frozen-lockfile command to have deterministic, fast, and repeatable builds. \r\n    // The following two lines, we already know.\r\n    const cdkPipeline = new CodePipeline(this, 'CdkPipeline', {\r\n      pipelineName: 'lambda-stack-cdk-pipeline',\r\n      synth: new ShellStep('Synth', {\r\n        input: codePipelineSource,\r\n        commands: [\r\n          'yarn install --frozen-lockfile',\r\n          'npx projen build',\r\n          'npx projen synth',\r\n        ],\r\n      }),\r\n    });\r\n\r\n    // 3b. Then we add this to the CDK Pipeline as a pipeline stage.\r\n    cdkPipeline.addStage(new LambdaStage(this, 'dev'));\r\n  }\r\n}\r\n\nHere we see three things happening; please check the comments in the code above.\nThen, of course, we also need to change the src/main.ts, because we moved the Lambda Stack to a separate file, and the starting stack of the CDK App is from now on the CDK Pipeline Stack.\nimport { App } from 'aws-cdk-lib';\r\nimport { CdkPipelineStack } from './cdk-pipeline-stack';\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nnew CdkPipelineStack(app, 'CdkPipelineStack', { env: devEnv });\r\n\r\napp.synth();\r\n\r\n\nAnd finally, we need to update the only test by renaming main.test.ts to lambda-stack.test.ts without changing the test. But again, we are testing if our Lambda Stack has exactly one Lambda Function.\nimport * as cdk from 'aws-cdk-lib';\r\nimport { Template } from 'aws-cdk-lib/assertions';\r\nimport { LambdaStack } from '../src/lambda-stack';\r\n\r\ntest('Lambda created', () => {\r\n  const app = new cdk.App();\r\n  const stack = new LambdaStack(app, 'LambdaStack');\r\n  const template = Template.fromStack(stack);\r\n\r\n  template.resourceCountIs('AWS::Lambda::Function', 1);\r\n});\r\n\r\n\nWe need to run cdk deploy or npx projen deploy only once to deploy our stacks. Then, it will deploy the CDK Pipeline, and we can see the pipeline at the CodePipeline service. From now on, for every commit you have on the main branch, the CDK pipeline will pick it up. No more deploy commands. We only push to the main branch to deploy.\nNeat, isn\u2019t it?\n\u00a0\nCDK Pipeline deployed and working\nHere is the code with the CDK Pipeline.\n\u00a0\nOther Cool\u00a0Features\nAs you saw, we only used the most basic way to use CDK Pipelines. We can always configure it more by:\n\nAdding more stacks. We could have a different stack like API Gateway Stack and deploy it in the same pipeline. Or use Lambda Stack again but deploy another version of it with a different configuration.\n\ncdkPipeline.addStage(new LambdaStage(this, 'dev'));\r\ncdkPipeline.addStage(new APIGatewayStage(this, 'dev'));\r\n\nor\ncdkPipeline.addStage(new APIGatewayStage(this, 'dev'));\r\ncdkPipeline.addStage(new APIGatewayStage(this, 'acceptance'));\r\n\n\u00a0\n\nDeploying stacks to multiple regions or accounts:\n\nexport class LambdaStage extends Stage {\r\n  constructor(scope: Construct, id: string, appRegion: string) {\r\n    super(scope, id);\r\n    new LambdaStack(this, 'LambdaStack', {\r\n      env: {\r\n        account: process.env.CDK_DEFAULT_ACCOUNT,\r\n        region: appRegion,\r\n      },\r\n    });\r\n  }}\r\ncdkPipeline.addStage(new LambdaStage(this, 'dev1', 'eu-west-1'));\r\ncdkPipeline.addStage(new LambdaStage(this, 'dev2', 'us-east-1'));\r\n\n\u00a0\n\nAdding other types of stages, like a ShellStep or CodeBuildStep:\n\ndeclare const cdkPipeline: pipelines.CodePipeline; \r\nconst preprod = new APIGatewayStage(this, 'PreProd');\r\ncdkPipeline.addStage(preprod, {   \r\n  post: [     \r\n    new pipelines.ShellStep('Validate Endpoint', {       \r\n      commands: ['curl -Ssf https://my.webservice.com/'],     \r\n    }),\r\n   ],\r\n });\r\n\n\u00a0\n\nRunning pipeline stages in parallel using Waves.\n\ndeclare const cdkPipeline: pipelines.CodePipeline;\r\n  \r\nconst wave = cdkPipeline.addWave('MyWave'); \r\nwave.addStage(new APIGatewayStage(this, 'Stage1')); \r\nwave.addStage(new APIGatewayStage(this, 'Stage2'));\r\n\n\u00a0\n\nAdding manual approvals between pipeline stages:\n\ndeclare const cdkPipeline: pipelines.CodePipeline; \r\nconst preprod = new APIGatewayStage(this, 'PreProd'); \r\nconst prod = new APIGatewayStage(this, 'Prod');\r\ncdkPipeline.addStage(preprod, {   \r\n  post: [     \r\n    new pipelines.ShellStep('Validate Endpoint', {       \r\n      commands: ['curl -Ssf https://my.webservice.com/'],     \r\n    }),\r\n   ],\r\n });\r\ncdkPipeline.addStage(prod, {\r\n   pre: [\r\n     new pipelines.ManualApprovalStep('PromoteToProd'),\r\n   ],\r\n });\r\n\n\u00a0\n\nUsing the (default) self mutation feature. If you add new application stages in the source code or new stacks to LambdaStage, the pipeline will automatically reconfigure itself to deploy those new stages and stacks.\n\n\u00a0\n\n\n\n\nAwesome!\nWe will use some of the features we mentioned here in the following chapters and improve our pipeline.\n\n\nIn this blog, we continued building our project by adding a CI/CD pipeline using the CDK Pipelines module of AWS CDK. The next topics are Bootstrapping and Aspects. We will tackle a few problems we see when we try to use AWS CDK in AWS platforms. See you soon in the next one, cheers!\n\n\n", "tags": ["aws", "aws cdk", "cdk pipelines", "CI/CD", "cloud", "devops"], "categories": ["Blog", "Cloud"]}
{"post_id": 31138, "title": "Production-Ready CDK \u2013 Project Structure", "url": "https://www.luminis.eu/blog/cloud-en/production-ready-cdk-project-structure/", "updated_at": "2023-01-27T12:55:00", "body": "In my last post, I announced I am starting a new blog series on Cloud Development Kits. You can find more about the purpose & plan here.\nIn the first chapter of this series, we will begin building our cdk project hands-on while explaining the tooling and the decisions used.\n\nThe primary tool of this post is Projen! In the CDK community, it is a popular tool these days. But for others, it might be the first time they hear about it.\nProjen is a project configuration management tool. To make it more concrete, what AWS CDK is to AWS is, Projen is to your Git Project. So as they call it: it is a CDK for software projects. We can manage all project configurations from a simple, single Javascript file and synthesize project files such as package.json, .gitignore, .eslintrc.json.\nThe concept sounds familiar. We have all seen and used other utility tools like Cookiecutter or Yeoman before. The main issue with those tools is they are for templating, only for one use. After weeks/months of using those tools, your projects will look very different, and there is nothing to do about it. Whereas with Projen, we can create the project and keep managing and configuring it actively since it is not one-time use.\nWhy is it popular with the CDK community, and how did the project start? Because Mr. Elad Ben-Israel, the leading creator of AWS CDK, started Projen and showcased it\u00a0at the first CDK Day in 2020. Then the project grew quickly and almost became the new standard for the CDK projects. While writing this, I saw that it even became an AWS project.\nMy personal experience and why I prefer it\nI have observed that after people start using AWS CDK, the number of AWS CDK projects usually increases sharply after some time. I once worked in an environment where we had +50 AWS CDK repositories. The configurations, pipelines, versioning were all over the place. We fixed it and made them look similar, but not all projects were developed or maintained at the same rate. As time passed, we had the same issue, and we didn\u2019t have a clever and consistent way of managing our projects.\nI also tried templating engines, mostly Cookiecutter, years ago. But unfortunately, the template becomes obsolete rapidly, and almost always, people are not on the same page regarding the project configuration. Besides that, I tried the Bedrock Pattern but didn\u2019t find it applicable to my projects.\nPlus, it is hard to a correct and consistent project structure. There are so many things to think about. To make it more concrete, here is the list of files/features we usually need from a Typescript project, which is a lot:\n\nThe heart of the project: package.json\nTypescript Compiler configuration: tsconfig.json\nDependencies\nLinter\nUnit testing & coverage\nVersion bumps & changelog\nCI builds\nAutomated releases\nSecurity patches\nLicense\nNpm workflow scripts\n\nLuckily the opinionated projects that come ready with Projen contain months of experience, trial, and error. For me, Projen solved the problems I mentioned and made our configuration management much more straightforward, thanks to these.\nLastly, although I highly recommend it, I should warn you it might sometimes be challenging to fix errors because it is a pretty new tool, and the community is not at its peak yet. So we need a bit of patience, that\u2019s all.\nImplementation\nEnough with the story; let\u2019s start with the implementation. Here are the prerequisites to be able to use AWS CDK and Projen:\n\nAWS Account & IAM User or Role that you can assume\nAWS CLI\nNode.js: recommend version 16; version 14 should also be fine\nIDE of your choice\nGit\n\nI assume you configured all and AWS CLI & git & npm(from Node.js installation) working as expected. So let\u2019s execute the following commands to create the project:\n$ mkdir prod-ready-cdk && cd prod-ready-cdk\r\n$ git init\r\n$ npx projen new awscdk-app-ts\nProjen file\nNow, we have a project ready to be detailed. First, we will be working with the\u00a0.projenrc.js file to configure the project. It should look like this first:\nconst { AwsCdkTypeScriptApp } = require('projen');\r\nconst project = new AwsCdkTypeScriptApp({\r\n  cdkVersion: '1.95.2',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'prod-ready-cdk',\r\n\r\n  // cdkDependencies: undefined,  /* Which AWS CDK modules (those that start with \"@aws-cdk/\") this app uses. */\r\n  // deps: [],                    /* Runtime dependencies of this module. */\r\n  // description: undefined,      /* The description is just a string that helps people understand the purpose of the package. */\r\n  // devDeps: [],                 /* Build dependencies for this module. */\r\n  // packageName: undefined,      /* The \"name\" in package.json. */\r\n  // release: undefined,          /* Add release management to this project. */\r\n});\r\nproject.synth();\nSee, it comes up with AWS CDK v1. We need to change the CDK version to v2 and provide more fields. (Warning: I needed to change the first two lines as well)\nconst { awscdk } = require('projen');\r\nconst project = new awscdk.AwsCdkTypeScriptApp({\r\n  authorAddress: 'kemal.gulsen@luminis.eu',\r\n  authorName: 'Kemal Cagin Gulsen',\r\n  cdkVersion: '2.8.0',\r\n  defaultReleaseBranch: 'main',\r\n  name: 'prod-ready-cdk',\r\n  description: 'A CDK project for my blog posts',\r\n  repositoryUrl: 'https://github.com/cagingulsen/prod-ready-cdk.git',\r\n  keywords: [\r\n    'AWS CDK',\r\n    'projen',\r\n    'Typescript',\r\n    'Deployment',\r\n  ],\r\n\r\n  // cdkDependencies: undefined,  /* Which AWS CDK modules (those that start with \"@aws-cdk/\") this app uses. */\r\n  // deps: [],                    /* Runtime dependencies of this module. */\r\n  // description: undefined,      /* The description is just a string that helps people understand the purpose of the package. */\r\n  // devDeps: [],                 /* Build dependencies for this module. */\r\n  // packageName: undefined,      /* The \"name\" in package.json. */\r\n  // release: undefined,          /* Add release management to this project. */\r\n});\r\nproject.synth();\nFor changes to take effect, we need to rerun Projen:\n$ npx projen\nYou can see the changes in the\u00a0package.json, mainly for the AWS CDK dependencies. With CDK v2, we don\u2019t need to add dependencies per AWS Service,\u00a0\u201caws-cdk-lib\u201d:\u00a0\u201c^2.8.0\u201d\u00a0is all we need.\nFurthermore, you might need to bootstrap AWS CDK again using the\u00a0cdk bootstrap\u00a0command since CDK v2 uses the modern bootstrap stack. This modern way will help us because the modern bootstrap stack is a prerequisite for CDK Pipelines.\nNext, we synthesize the CDK app using the command:\n$ npx projen synth\ninstead of\u00a0cdk synth. But, we see that it doesn\u2019t work since we changed the CDK version. So, let\u2019s update the dependencies and add a Hello World Lambda while on\u00a0src/main.ts.\nTip: instead of using npx projen \u2026\u00a0every time, we can have an alias like\u00a0alias pj=\u201dnpx projen\u201d to make it shorter.\nAWS CDK App: A Hello World Lambda Function\nLet\u2019s use src/main.ts for our Lambda stack for now and refactor it in the next episodes. After I fix the imports and add the lambda function, it looks like this:\nimport { App, Stack, StackProps } from 'aws-cdk-lib';\r\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\r\nimport { Construct } from 'constructs';\r\n\r\nexport class LambdaStack extends Stack {\r\n  constructor(scope: Construct, id: string, props: StackProps = {}) {\r\n    super(scope, id, props);\r\n\r\n    new lambda.Function(this, 'ExampleFunction', {\r\n      functionName: 'example-lambda',\r\n      code: lambda.Code.fromAsset('lambda'),\r\n      handler: 'hello.handler',\r\n      runtime: lambda.Runtime.NODEJS_14_X,\r\n    });\r\n  }\r\n}\r\n\r\n// for development, use account/region from cdk cli\r\nconst devEnv = {\r\n  account: process.env.CDK_DEFAULT_ACCOUNT,\r\n  region: process.env.CDK_DEFAULT_REGION,\r\n};\r\n\r\nconst app = new App();\r\n\r\nnew LambdaStack(app, 'lambda-stack-dev', { env: devEnv });\r\n// new LambdaStack(app, 'lambda-stack-prod', { env: prodEnv });\r\n\r\napp.synth();\nAnd of course, add our Lambda source code,\u00a0lambda/hello.js.\nexports.handler = function(event, context) {\r\n  console.log('Hello, Cloudwatch!');\r\n  context.succeed('Hello, World!');\r\n};\nThen finally, after we\u00a0npx projen synth (or shorter, pj synth), we will have the smallest AWS CDK App ready to be deployed. You know the drill; then we do\u00a0npx projen deploy\u00a0and check the lambda created on AWS.\nTesting\nFinal touches, let\u2019s add our first unit test.\nimport * as cdk from 'aws-cdk-lib';\r\nimport { Template } from 'aws-cdk-lib/assertions';\r\nimport { LambdaStack } from '../src/main';\r\n\r\ntest('Lambda created', () => {\r\n  const app = new cdk.App();\r\n  const stack = new LambdaStack(app, 'LambdaStack');\r\n  const template = Template.fromStack(stack);\r\n\r\n  template.resourceCountIs('AWS::Lambda::Function', 1);\r\n});\nTo run tests, we can use the command\u00a0npx projen test.\nWatch mode\nWatch mode is a feature that can be very handy when writing CDK code. AWS CDK Team introduced this mode with AWS CDK v2. Every time we save a file and change the synthesized cdk output, the watch mode calls cdk deploy command. Therefore, our stacks deployed on AWS reflect our code, and we don\u2019t need to use cdk/projen commands every time we deploy. As a result, we save time, and deployments are faster for the development environment.\nTo enable it, we can use\u00a0npx projen watch.\nFor other commands, you can check\u00a0package.json. And for the list of options, you can check the API Reference. However, these days we have a more good-looking option in the Construct Hub.\nGithub Project\nAfter configuring the Projen file and AWS CDK App, we can create a new repository on Github and push the code to the repository. I have mine: cagingulsen/prod-ready-cdk. Then used the following commands:\n$ git remote add origin https://github.com/cagingulsen/prod-ready-cdk.git\r\n$ git push -u origin main\nThen we can commit the latest changes and push them to Github. You can check the code here.\nWe made the introduction for our CDK journey and mainly focused on Projen. But of course, this is not the final version of our cdk project configuration. We will add more settings and use more Projen features in the future to ramp up. And indeed, we will have more CDK Constructs than just a Lambda Function.\nThank you for your time, and see you in the upcoming post on CDK Pipelines. Cheers!\nReferences:\nhttps://github.com/projen/projen\nhttps://youtu.be/SOWMPzXtTCw\nhttps://aws.amazon.com/blogs/developer/increasing-development-speed-with-cdk-watch/\n\u00a0\n", "tags": ["aws", "aws cdk", "cdk", "cloud", "infrastructure as code", "projen"], "categories": ["Blog", "Cloud"]}
{"post_id": 30000, "title": "Cross-account AWS resource access with AWS CDK", "url": "https://www.luminis.eu/blog/cloud-en/cross-account-aws-resource-access-with-aws-cdk/", "updated_at": "2023-06-16T08:21:19", "body": "So here is the case: you have S3 buckets, DynamoDB tables, relational tables on several AWS accounts and want to share the data with other AWS accounts. To create a data lake for example. And you are not using the AWS Lake Formation, which provides cross account usage out of the box.\nOr, you are in the middle of a migration from accounts.\nOr, you want to have communication over multiple accounts, for example an API Gateway which needs to be used by different lambdas over multiple accounts.\nOr, you want to deploy with code pipeline to multiple environments which exists in multiple accounts.\nWhere to start\nAlthough AWS describes this topic quit nicely, I want to demonstrate how to do it with CDK.\nFor this example we will have two accounts, the original, source Account ID is 11111 and the new, target Account ID is 22222.There are actually two ways of using resources in cross accounts, namely by identity-based policy and resource-based policy. What is the difference then?\nWell, the first (and for us most important) difference is, not all resources do support a resource-based policy. For example, DynamoDB does not, as can be found in these tables right here.\n\nWith an identity-based policy, you will kind of create a \u201cproxy role\u201d to get to the other account and resources. For a resource-based policy, a policy will be directly attached to the resource itself, where you can attach the account IDs you want to give access to. For an identity-based policy the new account will need to assume a role temporarily, which then only gives permissions for that specific role instead of the original permissions, while a resource-based policy will give both permissions at the same time.\nGiven that not all resources support identity-based policy, we will provide a solution for both below.\n\u00a0\nResource-based policy cross account usage\nWe are going to add some code in our existing CDK script for the source account (11111):\nconst bucket = new s3.Bucket(this, 'SourceBucket', {\r\n    bucketName: 'source-bucket'\r\n});\r\n\r\nbucket.addToResourcePolicy(new iam.PolicyStatement({\r\n    actions: ['s3:Get*', 's3:List*'],\r\n    resources: [bucket.arnForObjects('*')],\r\n    principals: [new iam.AccountPrincipal('22222')]\r\n}))\r\n\nHere we are defining the resource-based policy for the new target account, 22222 for our original bucket, so that it will have direct access. The service in the target account just has to reference the bucket (by arn, most of the times) and it will work! At least, the actions that you gave permissions for. \ud83d\ude42\nThe target CDK script could contain the bucket if needed like so:\nconst bucket = s3.Bucket.fromBucketName(this, 'SourceBucket', 'source-bucket');\nBut more likely is it, that you will have to use it in your application, for example with the Java S3Client (v2):\nvar s3Client = S3Client.builder().build();\r\nvar getObjectRequest = GetObjectRequest\r\n   .builder()\r\n   .bucket(\"source-bucket\")\r\n   .key(\"filename\")\r\n   .build();\r\nvar response = s3Client.getObject(getObjectRequest);\nA bit more advanced approach which you will probably need when you have > 10 accounts that need to use the S3 bucket, is by using AWS Organizations. Because if you will have to add all those accounts separately, you can miss the overview quite fast!\nThe CDK script will change slightly, but still is easy configurable as long as you are managing your accounts correctly by Organizations:\nbucket.addToResourcePolicy(new iam.PolicyStatement({\r\n   actions: ['s3:Get*', 's3:List*'],\r\n   resources: [bucket.arnForObjects('*')],\r\n   principals: [new iam.OrganizationPrincipal('organizationId')]\r\n}));\nAll accounts under this organizationId will get the access you just defined.\nIdentity-based cross account usage\nIf you want to enable cross account usage for DynamoDB for example, we are going to need to use the identity-based policy option. This will involve some more steps than for the resource-based policy.\nFirst, we need to ensure that the original account will allow the new account to perform the action of assuming a role. This is done by creating a role in the 11111 account with a policy to allow this actions. Because we trust the new account fully, we will use 22222:root as principal. You could always put this to a specific user if you want to.\n//create role to assume the new principal account to\r\nconst role = new iam.Role(this, 'CrossAcountRole', {\r\n    assumedBy: new iam.ArnPrincipal('arn:aws:iam::22222:root'),\r\n    roleName: 'cross-account-role'\r\n});\r\n\r\n//add statement to allow assumerole action for this account\r\nconst assumeStatement = new iam.PolicyStatement();\r\nassumeStatement.addActions('sts:AssumeRole')\r\n\r\n//add under the principal policy\r\nrole.addToPrincipalPolicy(assumeStatement);\r\n\r\n//add statement to allow reading and putting into dynamodb table in original account\r\nconst resourceStatement = new iam.PolicyStatement();\r\nresourceStatement.addResources('arn:aws:dynamodb:eu-west-1:11111:table/tableName');\r\nresourceStatement.addActions('dynamodb:DescribeTable', 'dynamodb:GetItem', 'dynamodb:PutItem');\r\n\r\n//add as new policy\r\nrole.addToPolicy(resourceStatement);\r\n\nThis will create a role with an arn. Keep this arn for the next step.\nAfter this, we can go on to the CDK part of the new account. Here, we need to allow the task, lambda or any computing service to let it assume a role to the original account; the changing to the proxy role. This is done by adding a policy to the related role of the service. Here we need the arn of the role we just created.\n//for an ecs task\r\nnew ecs.TaskDefinition(this, 'TaskDefinition', {...})\r\n\t.addToTaskRolePolicy(\r\n    new iam.PolicyStatement({\r\n        resources: ['arn:aws:iam::111111:role/cross-account-role'],\r\n        actions: ['sts:AssumeRole'],\r\n    })\r\n);\r\n//for a labmda function\r\nnew lambda.Function(this, 'Function', {...})\r\n\t.addToRolePolicy(\r\n    new iam.PolicyStatement({\r\n        resources: ['arn:aws:iam::111111:role/cross-account-role'],\r\n        actions: ['sts:AssumeRole'],\r\n    })\r\n);\r\n\nIn CDK that\u2019s it!\nNow, we need to get the credentials correctly set up in the application itself (we are using a Java application as example, which uses AWS SDK v1); meaning that we get temporary credentials (the proxy role) we can use to hook into the original account, instead of the actual account, which will normally happen when you are creating a resource client like for DynamoDB.\nWhen looking into the STSAssumeRoleSessionCredentialsProvider class, it becomes clear we can use this to keep a temporary role for the original account we want. In order to use this, we also need the ARN of the role defined in the original account.\nvar stsAssumeRoleSessionCredentialsProvider = StsAssumeRoleCredentialsProvider\r\n   .builder()\r\n   .refreshRequest(\r\n      AssumeRoleRequest\r\n         .builder()\r\n         .roleArn(\"arn:aws:iam::111111:role/cross-account-role\")\r\n         .roleSessionName(\"Name for session role\")\r\n         .build())\r\n   .build();\r\n\nAnd then we need to ensure our client(s) use these credentials.\nvar amazonDynamoDBClient = DynamoDbClient\r\n   .builder()\r\n   .credentialsProvider(stsAssumeRoleSessionCredentialsProvider)\r\n   .build();\r\n\nAfter that, we will get a connection between the two accounts. We can still also define a separate client, which connects to a resource in its own account, by not providing the credentials.\nThat was quite easy! Now your target account has access to resources of the original account, as long as the role exists and the resources are available.\nSources:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html\nhttps://jayendrapatil.com/aws-iam-roles-vs-resource-based-policies/\nhttps://stackoverflow.com/questions/60310575/how-to-add-s3-bucketpolicy-with-aws-cdk\nhttps://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/\n", "tags": ["aws", "cdk", "cloud", "iam"], "categories": ["Cloud", "Development"]}
{"post_id": 31125, "title": "Production-Ready CDK \u2013 Prelude", "url": "https://www.luminis.eu/blog/cloud-en/production-ready-cdk-prelude/", "updated_at": "2023-01-27T12:53:40", "body": "I am starting a new blog series on Cloud Development Kits(CDK), first with AWS CDK. Then later, planning to jump to CDK for Terraform (CDKtf) and combine it with CDK for Kubernetes(CDK8s). Let\u2019s see how far we will come.\nWhat is a Cloud Development Kit, and how to start using it?\nI won\u2019t discuss what Cloud Development Kits are or why to use them compared to the other Infrastructure as Code tools. To learn more about CDKs and particularly AWS CDK, I recommend watching this video from AWS re:Invent 2018, especially if you don\u2019t have much experience with Infrastructure as Code tools. Although some things changed since 2018, the first part of the video will be relevant for many years because it explains the main concept behind CDKs.\n\nAfter watching the video, I highly recommend getting hands-on and following the CDK Workshop. Afterwards reading this blog series will make a lot more sense. Finally, the CDK Book can help if you want to dive in a bit deeper.\nWhat is my experience with CDKs?\nI work as a Cloud Architect/Consultant, and I have been using AWS CDK for more than two years. I used it mainly for serverless applications, CI/CD pipelines, and platform-level solutions at four different companies.\nHowever, I don\u2019t use it 8 hours a day, and there are people in this field who more actively develop the tools I use. So I will keep learning as I write, and meanwhile, I will share my knowledge in every detail to make your CDK journey as easy and smooth as possible. I hope you will enjoy it!\nWho is the audience?\nMainly anyone who is a Cloud Enthusiast. But providing some titles: Cloud Engineers, Software Engineers using Cloud, Cloud Architects, and DevOps Engineers.\nWhy \u201cproduction-ready CDK\u201d?\nI am doing the \u201cProduction-ready\u201d way because most of the resources and articles I find online focus on fundamental use cases. I often need to code my constructs very differently. The main reason is that those examples are not fit for the production level or enterprise level I work on.\nIn this series, I will share insights for production-grade CDK apps. So the focus is not explaining concepts shortly and quickly. Instead, we will do the opposite and try to get into the details that will make your applications secure, scalable, and available. Which doesn\u2019t mean the code I will be sharing will be ready to use in your production environment. You still need to make it your own, but it will give you ideas for real life cases.\nWhich tools are we going to use?\n\u2022 AWS\n\u2022 AWS CDK v2\n\u2022 Typescript\n\u2022 Projen\n\u2022 Github\n\u2022 CDK Pipelines\n\u2022 and others in the future\nIf you haven\u2019t decided to use AWS CDK as your Infrastructure as Code tooling for AWS:\nWell, for sure you need to compare and decide. But, if you are considering it for personal projects, I can promise you won\u2019t regret it. It will be fun, and you will learn valuable skills for both the present and the future.\nAWS CDK is a higher-level IaC tool compared to CloudFormation or Terraform\nIf you are considering using AWS CDK for your team: there are multiple factors you need to think about, like tools already in use, people\u2019s expertise, existing enterprise tooling, etc.\nBut to give you some ideas, you can read the following:\n\u2022 Cloudformation terraform or cloud development kit guide to iac on AWS\n\u2022 Terraform vs cdk vs cloudformation\n\u2022 Adopting aws cdk\nIf you are not sure which programming language you should use for AWS CDK:\nWe all have different tastes in programming languages, but I suggest using Typescript because of jsii. Jsii is a tool that enables some other programming languages to import and use Javascript classes. In other words, if we write our constructs in Typescript, we can package them for other languages, and for example, when using Python, we can import those constructs! But beware, it doesn\u2019t work the other way. If you write your constructs in Python, you cannot import them in Typescript. Also, most of the AWS CDK examples are for Typescript Apps.\nThe second language I can recommend is Python. It suits CDK programming very well; you can compactly write your constructs.\nImportant tip: we shouldn\u2019t pick our programming language thinking of our frontend or backend programming experience. Coding CDK apps is very straightforward, we use Object Oriented Programming much less, and we never use design patterns (well, you can, but why should you). Considering these, Typescript and Python-like languages suit better compared to Java or C#.\nFinally, I will be sharing the code on Github and keep updating and maintaining it.\nProjen \u2013 #TemplatesAreEvil\nOur first topic is Project Structure for AWS CDK Apps using Projen. See you in the next ones.\n", "tags": ["aws", "aws cdk", "cdk", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 31111, "title": "My experience becoming a FinOps Certified Practitioner (FOCP)", "url": "https://www.luminis.eu/blog/my-experience-becoming-a-finops-certified-practitioner-focp/", "updated_at": "2022-06-24T09:22:55", "body": "Recently, I sat down for the official FinOps Certified Practitioner exam. Now the experience is still fresh, I want to share my thoughts on getting this certification.\nFirst a bit of background, I\u2019m a software developer and architect working fulltime on cloud projects. I do not have much experience with the financial side of business but do have a healthy curiosity for things outside my direct line of work. In the Cloud, certifications are a well-established way to demonstrate proficiency. They are never proof of mastery, just evidence that you are familiar with the subject. For myself, I like certifications, they give you a tangible goal and something to show for your efforts. So, when I discovered the FinOps Certified Practitioner certification, a seed was planted.\nWhen you embark on such an endeavor, it\u2019s always good to start with the why. For me, one thing I always liked about the Cloud is that you have this extra dimension of cost, next to the usual suspects like compute, memory, storage, (and engineering effort of course). When you\u2019re working on a solution you try to balance all these dimensions and try to come up with the most optimal outcome. And what really makes it interesting is that the flexibility and speed of the cloud put all these dimensions in overdrive. This balancing act can (and maybe even should) be done continuously.\nWhat is FinOps?\nThe official website defines FinOps as follows:\n\nFinOps is shorthand for \u201cCloud Financial Operations\u201d or \u201cCloud Financial Management\u201d or \u201cCloud Cost Management\u201d. It is the practice of bringing financial accountability to the variable spend model of cloud, enabling distributed teams to make business trade-offs between speed, cost, and quality.\n\nIt\u2019s an area where two worlds collide, the technical world of IT and Cloud, and the business world of Finance and accounting. The Cloud has brought with it an enormous boost both in speed and unpredictability of IT cost, one that every serious user must come to grips with sooner or later. FinOps is an answer that will help you deal with this. Not by trying to mold the Cloud in an old form, controlling it, choking it to death, but by embracing all its benefits and capabilities. Another quote that I really like is:\n\nFinOps is not about saving money, it is about making money.\n\nHow? That\u2019s where the FinOps framework comes into play and what the certification is all about. It consists of a set of principles, phases, capabilities, and domains, all worked out in detail. They have a nice poster on the official website that gives a clear overview of this framework.\nImportant as all these components of the framework are, it\u2019s good to realize that at its core, FinOps is a cultural change. In that sense it is like the DevOps movement, which was probably a huge inspiration for the FinOps foundation. It\u2019s not just about tools and processes, but also about collaboration and establishing a shared language.\nFinOps Foundation\nThe FinOps movement is coordinated and organized by the FinOps Foundation, a program of the Linux Foundation. Their mission is to build and foster a community of people and companies practicing financial cloud management. Their members come with a wide variety of backgrounds, from technical, engineering type of backgrounds to finance or procurement professionals.\nOne way in which they share their knowledge is via training and certification. The best known and most achieved certification is the FinOps Certified Practitioner certification, and that is the one I had set my eyes on.\nFinOps Certified Practitioner\nThe certification:\n\nallows individuals in a large variety of cloud, finance and technology roles to validate their FinOps knowledge and enhance their professional credibility. The certification covers FinOps fundamentals and an overview of key concepts in each of the three sections of the FinOps lifecycle: Inform, Optimize and Operate.\n\nIt\u2019s a foundational level course, meant to ensure a common grounding between practitioners and a good introduction to the FinOps practices. There is also a Professional level certification, targeted at experience FinOps practitioners.\nPreparing for the exam\nThe most important source is the highly recommended Cloud FinOps book. This book covers all the material for the exam, and some more. It\u2019s a good read for anybody interested in this area, not just for wannabe certified FinOps practitioners.\nThere\u2019s also the official training from the FinOps foundation itself, either led by a virtual instructor, or in a self-paced format. Because I did not know what to expect from the exam, I decided to go for the self-paced training. It came with a hefty price of $499, which they bumped to $599 as of beginning this year. It includes the exam price (of $300), but even then, it\u2019s very expensive for an online training course that takes less than 6 hours.\nWas it worth the money? I\u2019m inclined to say no. It\u2019s basically a slimmed down version of the book, with an occasional quiz. If you\u2019ve read the book, made some notes, thoroughly went through the website, you can save yourself 300$. But it might also depend on your learning preference, if you\u2019re more a listener then a reader, it might make sense to splurge on the course. In that case I would recommend first investigating unofficial, third-party, online training courses which are usually a lot cheaper.\nWhat I also recommend is some practical experience. What I mean is, really going through the cloud bill and cost reporting of you cloud provider, to get some real-life understanding of the theory.\nThe exam\nThe exam is pretty easy, provided you went through all the material. It\u2019s only 60 minutes long, but you have plenty of time. It took me about 35 minutes and did not feel rushed in any way. It\u2019s a remote exam, unproctored, meaning you can complete it in your own browser, without having to install special surveillance software or somebody watching remotely. If you\u2019re familiar with the AWS certification space, it\u2019s more like an accreditation as opposed to a certification, although the difficulty is similar to the AWS Cloud Practitioner certification. The questions, 50 in total, are all multiple-choice and do not involve extensive reading.\nThe good part is also that you get 3 attempts available out of the box, so you can try it out as soon as you think you have the material covered and see if you\u2019re at the necessary level.\nNormally, when I study for an examination, I like to do one or more practice exams, to ensure I\u2019m at the required level. But with the 3 attempts available, I decided to first try one out before I\u2019d purchased some kind of practice exam. Luckily for me, the first try was successful (with 84% score, where 75% is required for passing), so I didn\u2019t have to go back to the drawing board and readjust my studies.\nFinal thoughts\nI thought it was a nice and interesting experience that really gave me good introduction into the subject.\nIn the end, it was easier than I expected. Maybe it was that hefty price tag that subconsciously made it appear bigger than it was. And when you expect things to be difficult, they often turn out easier than you thought they would be. Not to say it\u2019s a walk in the park, you do need to study and take it seriously.\nWould I recommend going for the FOCP certification? Well, it depends, of course. It depends on what your goals are.\n\nIf you want a job in this area, then the cost if probably small compared to what you can get out of it, and it is well worth your time and money.\nIf you\u2019re just interested in the field, then I would probably say no. The price/value ratio was not great. Reading the book is probably your best and most cost-effective approach.\nIf you don\u2019t have to pay fully for the exam yourself, and certification aligns with your learning preferences, then the required time and effort make sense\n\nMe personally, I fall into the last category and am happy with both the journey and the destination. It brought me practical tools for my day-to-day job and a stronger appreciation for the subject.\nAre you curious or do you want to know more? Get in touch.\nLearn from our experts:8 Feb 2021-Mark van KesselTraining: AWS Cost ControlOne of the big promises of the Cloud is that it makes IT cheaper. This is certainly a possibility, but not a given. Running the same workload as before in the exact same way usually only increases the cost. Cost...\n", "tags": ["FinOps"], "categories": ["Blog", "Cloud"]}
{"post_id": 31061, "title": "Getting more value out of your data\u00a0", "url": "https://www.luminis.eu/blog/data-en/getting-more-value-out-of-your-data/", "updated_at": "2023-09-06T13:53:56", "body": "\u201cData is eating the world\u201d, is a well-known variant on Mark Andreesen\u2019s famous quote \u201cSoftware is eating the world\u201d. In our current output driven society everything is measured by an interpretation of a set of data, even by the people and businesses that were not traditionally driven by data.\nThis has led to an increasing demand for, and thus increased value of data. But not just any data, it is data that provides us with insights that we value most. Whether data is insightful depends on many facets, most of which vary between different groups of stakeholders. But an overall rule of thumb is that for the data to be insightful, it must have a certain level of quality. To reach this level of quality there are four things that play a key role: collection, storage, availability and analysis. For the data to have decent quality these four facets need to be coordinated correctly.\nCollection\nThe first is collection: Not only do you have to decide which data is relevant for the insights you want to gain, but you also must decide the right method to collect this data. Depending on the data you want to collect, you might have to select reliable sources. Do your users provide the data, is it something collected by an (IoT) device, or is it an existing dataset from a third party? Different types of data come with different measures for determining reliability.\nYou also must define if there is a minimal amount of data required and if so, what that minimal amount is. In case of benchmarking for example, you want to compare a sample against a bigger population. To make reliable assumptions you need to have enough data for each of the benchmarked groups. But whether that is ten or ten million items per group depends on the use case. Finally, you need to know whether your data needs some form of preparation, transformation or optimization. You might want to train some AI models, transform some binary data into something more readable or distill only the useful pieces of data out of a larger set, so it can be retrieved faster and more easily. Depending on the use case data collection can be a repetitive process, so there is not one moment to collect data, but this is done throughout time.\nStorage\nThen there is storage: You must decide how to store the collected data in a secure manner. Think about which regulations apply (GDPR for example) and where you want to store it. Do you want to store your data in the cloud or on premise, and does the physical location matter, or is it OK that it is stored in a datacenter on the other side of the world?\nWhich type of storage is best for your data? Will a relational database suffice or is your data (partially) free format, in which case a document store will be the more obvious choice, while a graph database is more likely when it is important how parts of your data relate to each other. And is it possible to combine different types of storage if it is required by your data?\nDo you want the data to contain state, or does it need to be a set of state changes? While the former is less storage consuming, the latter makes it possible to keep a data trail and recreate the state of the data at any moment in time. In case a data trail is important, event sourcing might be something to look into.\nAnalysis\nAt this point you have collected (some) data and stored it somewhere. But does it already provide the insights you thought it would? In many cases raw data is not really that insightful. In that case Analysis of the data can be a solution. There are many ways of analyzing a set of data. It is possible to use a predefined algorithm to deduct the insightful information from a larger set, or you can use machine learning to make classifications or predictions for you. In this case analysis provides new views on the data. But you can also analyze the data to check the quality or find out if you collected the right data. With the feedback from the latter analysis, you can improve your data and tweak the way you collect data in the future. In this way you improve the data you have already collected while you make sure that the data you collect in the future is less flawed, win-win.\nAvailability / Accessibility\nAfter you have gotten to the point where your data contains the insights you wanted, it is time to share it with others. The concept of availability might seem contradictory because it is about making your data easily available, but only to the people that are eligible. For the \u201cmaking your data easily available\u201d part you need to know some things about your users. Who are they? Can you distinguish user groups with different needs? How are you going to visualize data? Do your users want dashboards, or do they want an API to communicate with? In case of dashboards, you need to think about which visualization is best to capture the insights for your users. In case of an API, what is your API strategy? How do you make sure that it is well documented for your users, and do you want to make it easily accessible for applications as well?\nFor the \u201conly to the people that are eligible\u201d part you must think about how to restrict the access to your data. Do you want people to login first, and do you want to have different access levels? In some cases you want to keep track of who accessed which part of your data, access logs might come in handy. In that case you want to think about how to store this metadata and the regulations that apply when storing it.\nAs you can see there is way more than meets the eye when it comes to creating more valuable data for your organization. This blog might have left you with more questions than answers it has given you. Partially because these answers vary for each use case, but also because it makes you rethink about choices you have made regarding your own data and the process of making it more valuable.\nAt Luminis we have developed a set of methods and frameworks that help you find the optimal data solution for your use case. From helping you with answering the questions in this blog on a more functional level, to technical solutions that help you implement your optimal data solution. Over the coming months we will publish more blogs that dive deeper into these methods and frameworks.\n", "tags": ["data", "structure"], "categories": ["Blog", "Data"]}
{"post_id": 31008, "title": "Luminis achieves the status of AWS Advanced Consulting Partner", "url": "https://www.luminis.eu/blog/luminis-achieves-the-status-of-aws-advanced-consulting-partner/", "updated_at": "2022-01-21T13:50:44", "body": "Amersfoort, 20 January 2022. Tech company Luminis has achieved the status of Advanced Consulting Partner in the Amazon Web Services (AWS) Partner Network. This achievement is another milestone in the collaboration between AWS and Luminis, and means that customers with issues related to AWS cloud services are in safe hands with Luminis.\nWith this new partnership level, Advanced Consulting Partner, it becomes even more clear to customers that Luminis is a recognized expert in the field of AWS. Examples of services that Luminis offers include: advice, integration, migration and the lifecycle management of applications in the AWS Cloud. With more than 60 different employees who have obtained one or more AWS certifications, Luminis joins the select group of AWS Advanced Consulting Partners in the Netherlands.\nLuminis as Advanced Consulting Partner\nTo become an Advanced Consulting Partner, an extensive set of requirements must be met in terms of knowledge, based on AWS certifications, and experience, based on projects and results. In addition, a long-term partner plan has also been drawn up that sets out how AWS and Luminis can strengthen and use each other. An important part of this plan also fits in well with Luminis\u2019 knowledge-sharing strategy. In 2021 Luminis organized various events for customers, colleagues and the AWS community in the Benelux. As a result, Luminis employees have been accepted into AWS\u2019 prestigious Community Builder program. The AWS Community Builders program provides a variety of materials and networking opportunities for AWS thought leaders with the goal of growing the AWS community in the Benelux.\nCloud Proficiency by Gamification\nLuminis has expanded its knowledge in the field of Cloud in a very special way: with Gamification. A number of enthusiastic Luminis colleagues have created a development program based on Gamification. that helps people get acquainted with the possibilities of Cloud technology. Bert Ertman, VP Technology explains: \u201cTo work on our Cloud Proficiency, a group of colleagues has started \u2018Cloud the Game\u2019. This is a platform with many gamification elements such as badges, pools and mini-challenges. Anyone who achieves something in the field of Cloud technology can share this performance with their colleagues. The platform and the challenges we organize challenge our employees to increase their Cloud Proficiency.\u201d\nLuminis as partner for Cloud technology\nCloud technology is both an opportunity as well as a challenge for many organizations. Luminis has been actively applying Cloud technology for both its customers and its own products since 2009. The gained knowledge and experience are the foundation for all our services and customers. These services focus on helping organizations with concepts such as \u2018Cloud-the-Game\u2019 and \u2018Accelerate\u2019, an intensive training for tomorrow\u2019s leaders in field of technology, as well as on the technological challenges with services in the field of migrations, software development and data strategy.\n", "tags": [], "categories": ["Blog", "Cloud", "News"]}
{"post_id": 30660, "title": "The Amplify Series, Part 3: Why should you use AWS Amplify?", "url": "https://www.luminis.eu/blog/the-amplify-series-part-3-why-should-you-use-aws-amplify/", "updated_at": "2023-05-08T13:08:08", "body": "Now that you know what AWS Amplify is and have a broad overview of how it works, we will look at several reasons you might consider using it. We will also look at scenarios where Amplify might not be the best choice. AWS Amplify is a powerful tool. However, you should always try to use the right tool for the job! Whether the right choice is Amplify or another tool.\nAmplify has evolved into a useful and mature toolset for creating cloud-native web and mobile applications. Having Amplify in your developer toolbelt will increase your productivity in different scenarios. Be it developing your next application, creating a quick prototype, or learning/teaching AWS. Let\u2019s look at why you might want to use AWS Amplify.\nIncrease speed, reduce risk: write less hand-crafted code\nThe primary reason to use AWS Amplify is that it increases your business speed and agility. This is achieved by greatly reducing the amount of code you need to write and manage to achieve cloud-native functionality. In part 1, we already listed all of the functionality available in AWS Amplify. However, that was just a brief overview. In this section, we want to zoom in on some of the functionality and advantages that impress us the most and end up using the most in our projects.\nAuthentication\nOne of the most valuable categories in the Amplify suite is the Auth category. It provides us with rich authentication functionality backed by AWS Cognito. After you use this for the first time, you will never want to write your own authentication mechanisms ever again. This category even comes with an authentication UI that you can optionally use to speed up development even more, and it is completely customizable:\n\nOnce the auth category is set up, it will create an AWS Cognito user pool to register your users. The Amplify SDK will provide several helper functions to register, sign in and sign out your users. There is also forgot-password and email activation functionality out-of-the-box.\u00a0\nBesides creating a user using an email and password combination, you can easily set up federated sign-in with Google, Facebook, Apple, or any provider that supports Open-ID connect and SAML.\nOne other nice feature to notice in this category is that it will handle authentication for the other categories such as API and Storage once you have it set up. This means that you do not need to handle any token management yourself.\nGraphQL\nWhile the REST version of the Amplify API category also provides a lot of functionality, including a REST API, Lambda function, and DynamoDB, we have decided to highlight the GraphQL version as it is a prime example how much AWS Amplify can help you with.\nIf you are familiar with GraphQL, you know that you can define your API in a GraphQL schema. What Amplify adds to this is several annotations called GraphQL directives that generate functionality. The following is an example of such a schema:\n\nJust by creating this schema, we will get the following generated for us:\n\n@model\n\nDynamoDB tables for Show and Review generated.\nGraphQL Query, Mutation, and Subscription client stub code for Show and Review\u00a0\n\n\n@auth\n\n403 http status codes when the auth rules are broken. In this example, only users that are logged in (\u201cprivate\u201d) are allowed to create, read, update and delete shows. There is also the possibility of making more complex authorization rules, such as allowing only users of a particular group to perform actions.\n\n\n@connection\n\nReferences from Show to Review and vice versa\n\n\n@function\n\nGraphQL queries for \u201creviewScore\u201d for \u201cshow\u201d routes to a separate Lambda function instead of going directly to the database. In this way, you can still customize the logic in the backend instead of just building CRUD.\n\n\n\nVisit the AWS Amplify docs to find out more about Amplify GraphQL directives.\nHosting and CI/CD\nThe final part we want to highlight is the entire hosting and CI/CD that you can set up for your project in just a few commands. By simply running \u201camplify add hosting\u201d and following a few instructions, you can set up an entire CI/CD pipeline for your repo that deploys the backend and hosts the frontend on a public URL.\n\nEvery part of the pipeline is customizable, and setting up redirects and HTTPS with a domain name is a piece of cake. You can even spin up different environments per branch to test them.\nEmpower front-end engineers: JavaScript front- and back-ends\nWe have noticed while using Amplify that it has allowed our front-end engineers to expand their reach in development by being able to contribute to the backend since it is also written in JavaScript.\n\nWhile Amplify supports Lambda functions in other languages such as Java, choosing for JavaScript while also building your frontend in JavaScript means that you develop your entire application with the one language all frontend engineers know. This will allow Frontend engineers to pick up whole vertical issues in your project to handle changes to the entire stack.\nNote that you can choose to ignore JavaScript completely and build an Android mobile app with Java Lambda\u2019s in the backend in Amplify. The advantage mentioned here does not hold in this case.\nInnovate at speed: prototype to reduce time-to-value\nPrototyping is one of the best ways to determine if your product/solution will solve the problems you are trying to solve. If you can test your ideas and get feedback as fast as possible, you will innovate quickly.\n\nWhile using paper prototypes or interactive mockups is a fast way to get user feedback, you can only reach the number of people you have time to test with. If you want to get an actual prototype up and running in no time so that it can be shared via social media and tested by several users, then Amplify can be used to skip all of the setup and let you focus on building the prototype.\nIf you combine this with analytics and surveys, you will get information from a more extensive user base should that be necessary.\nAccelerate cloud proficiency: leverage AWS best practices and patterns\nAWS has almost 200 services available at the moment. This can be very intimidating for someone trying to start their cloud journey. AWS Amplify only uses a subset of these services and guides you in creating and configuring the services.\nOne of the strengths of Amplify is that you can create an entire cloud-native application without having to know what is happening under the hood. However, learning what happens under the hood of AWS Amplify is an excellent way to start learning about cloud computing since the scope of services is much smaller, and you can take it step-by-step based on the categories.\nWhen you are done building your first cloud-native Amplify application and know what is happening in the background, you will have a sound base of knowledge and experience to build upon.\nKeep options open: extend Amplify with CDK\nOne of the most common questions is when we should use Amplify or CDK when starting up a cloud-native AWS project. By now, you have an idea of what AWS Amplify is. If you have never heard of CDK before, our colleague wrote an introduction blog post about it.\u00a0\n\nThe primary objective of CDK is to allow developers to write infrastructure-as-code in a programmer-friendly language instead of CloudFormation. Amplify wants to abstract as much as possible from the backend infrastructure and Cloudformation, while CDK has nothing to do with the frontend. Since they have different goals, they have different scenarios where you should pick one over the other.\nHowever, since the introduction of Amplify Extensibility, your options have become somewhat simpler. You can build your frontend using Amplify, extend it with CDK and even export your Amplify project as a CloudFormation templates to use in your existing CDK projects. \u200b\u200bPicking one over the other is no longer an binary exercise. Instead, we should explore when to leverage which option.\nWhen should you not use AWS Amplify?\nBefore Amplify Extensibility, which allows you to use AWS CDK to define AWS services you want to use in your Amplify application that is not present in the several Amplify categories, my advice was always:\n\u201cIf you can map your main success use cases to the Amplify categories, then you should use Amplify. Otherwise, you should avoid using it.\u201d\nHowever, this advice has become obsolete since we can now use any AWS service in our Amplify applications thanks to Extensibility. There are still some scenarios where you shouldn\u2019t use Amplify:\n\nNon-AWS or multi-cloud projects: AWS Amplify can only target the AWS cloud platform.\nNon-fullstack projects: if you creating pure front-end or back-end projects, Amplify might not be the most effective choice.\nMisfit with existing architecture: if fitting an AWS Amplify project into your existing landscape would require massive effort or does not align with existing architectural principles, it might not be the most logical choice. Maybe you should start with convincing your enterprise architect of Amplify\u2019s value first.\nNo clear advantage: if large parts of Amplify\u2019s functionality are already in place (e.g., slick build and deploy pipelines, or a library of similar building blocks), then the added value of Amplify might not be so great. But that\u2019s good news, right?\n\nUp next\u2026\nHopefully, this blog has given you even more reason to start working with AWS Amplify. Now that we have the high-overview and background information out of the way, it is time to get our hands dirty! In the following blog posts, we will be creating an AWS Amplify application, and we will take a closer look at several Amplify categories.\n\u00a0\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30467, "title": "The Amplify Series, Part 2: How does AWS Amplify work?", "url": "https://www.luminis.eu/blog/the-amplify-series-part-2-how-does-aws-amplify-work/", "updated_at": "2023-05-08T13:08:44", "body": "In the previous blog, we got a high-level overview of AWS Amplify. In this article I will examine how AWS Amplify works and explain the \u201cmagic\u201d behind the scenes. While one of the advantages of Amplify is that you don\u2019t need to know any of this to develop a cloud-native application, it is essential to understand its inner working if you want to do more than just quick scaffolding.\nI have worked on several Amplify projects without digging deep to get effective. So if you want to get to just building a cloud-native application, you can skip this one. However, for those of you who stay, we will be covering the foundation on which Amplify is built upon, the concept and technologies used, and describe what actions happen in the background when you run specific commands in the Amplify CLI. It is important to note that the examples we give in these articles are very web app-oriented to keep the articles shorter. However, Amplify also supports mobile development with Android, iOS, and hybrid frameworks, and for the most part, the information here is still relevant.\nInfrastructure as Code (IaC)\n\nTo explain how AWS Amplify works, we first need to discuss the concept of \u201cInfrastructure as Code\u201d.\nBefore IaC, we would probably log in on a cloud console or web portal of some sort and start configuring our infrastructure by clicking around. Additionally, the cloud provider might have a CLI or another tool to help us provision our infrastructure. After many clicking and cli commands, we finally have our first version up and running.\nAfter testing this setup, we are ready to go to production. Here is where we encounter one of the first problems that IaC solves: How do we replicate the test environment we provisioned and tested? One option is to follow all the steps we did to create a replica. However, as you can already tell, this is very error-prone, and missing even one step might make a big difference.\nWith IaC, we solve this issue by describing these steps we made into some form of code. Once we have this in code, we can easily create a copy of the infrastructure using a provider that understands the code and can provide us with resources. There are also other benefits to IaC. To learn more about it, check out this blog post or this short video.\nMany tools provide IaC functionality, such as Terraform, Puppet, Google Cloud Deployment Manager, and Azure resource manager, to name a few. When it comes to AWS, we use AWS Cloudformation.\nAWS CloudFormation\nAWS Cloudformation is the Infrastructure as Code solution provided by Amplify. We can define our cloud environment, meaning the resources and their dependencies, in so-called \u201cCloudFormation templates\u201d. These templates are written in JSON or YAML. CloudFormation can take such a template and create a CloudFormation Stack, a set of related resources. For CloudFormation to find the template, it must be uploaded to S3.\n\nHowever, the good news is that Amplify will do all of the steps above for you. You just need to answer a few questions about what kind of functionality you need for your cloud-native app. Amplify will generate CloudFormation templates based on these answers, deploy them to S3 and use CloudFormation to create, update or delete the stack. We can log in to AWS and go to CloudFormations to look at our stacks at any time. Here is an example of a CloudFormation template generated by Amplify:\n\nSome items in the example above are minimized, but we can see that we are declaring an API resource with a path \u201c/players\u201d and \u201c/achievements\u201d. The entire file is 2200 lines long, only for the Amplify API category. Learning to do this by hand is a lot of work and error-prone. That is why several tools generate CloudFormation, such as Amplify, AWS CDK, and AWS SAM. Which tool you should use will be covered in the next blog. For now, it is essential to understand the basics of CloudFormation.\nConfiguring AWS Amplify CLI\nBefore you start using the Amplify CLI, you need to configure it to know which account it can use to create AWS resources using S3 and CloudFormation. You can do this by running \u201camplify configure\u201d in the command-line terminal. This will open a browser window for you to log in to the AWS console and walk you through the steps of creating an IAM account that has enough access to do everything that the Amplify CLI needs to do. \u00a0At the end of all of the steps, you must give the profile a name. This name will be used in the CLI moving forward with Amplify, and at some points, the CLI will ask you which profile you want to use for which commands.\nOnce this is done, the CLI will use these credentials to communicate with AWS to generate cloud resources via CloudFormation. Here is a diagram showing the actions that happen when you first configure your CLI and how the credentials are used to access CloudFormation and S3:\n\nInitialising an Amplify project\nWhen your CLI is configured, you can start creating your Amplify project. This is done by navigating to the root of your project and running \u201camplify init\u201d. The CLI will ask several questions, one of which is related to the credentials we configured. Once we select the correct profile, Amplify will work and perform several actions to create the Amplify project.\nChanges in AWS\nAfter running the amplify init command, the CLI will create an S3 bucket to store the CloudFormation templates, and it will create a CloudFormation stack that will point to the S3 bucket. Here is an example of a stack built after running amplify init:\n\nAs we can see, the DeploymentBucket is present, which will hold our CloudFormation templates. There are also two IAM roles created for the project. One for authorized users and one for unauthorized users. Once we begin adding more Amplify functionality and setting up authorization rules, these IAM roles will be updated to match what we configure via Amplify.\nAnother action that is taken by the CLI when running Amplify init is the creation of an Amplify project in the Amplify console:\n\nThis will become relevant once we start looking into hosting and adding CI/CD for our project.\nChanges in the project repository\nThe CLI also generates and updates several files in our repository, reflecting our choices made in the Amplify CLI. An extensive list of files generated by Amplify can be found here; however,\u00a0here is a summary of the files changed if you init for an Angular application:\n\namplify/cli.json: Feature flags for the CLI\namplify/README.MD: A list of links to helpful resources\namplify/team-provider-info.json: Information needed so that other team members can use Amplify. This file can be added to git if running a private application. However, it is advised to remove it or add it to the gitignore if it is a public project. Note: Even if someone gets the information in this file, they would still need the correct credentials to use the information.\namplify/.config/..: Configuration options set during the \u201camplify configure\u201d command\namplify/backend/backend-config.json: Information about resources and how they connect.\namplify/hooks/\u2026: Command hooks that can be used during the Amplify lifecycle, such as \u201cpre-push\u201d, \u201cpost-add-function\u201d etc.\nsrc/aws-exports.js: This file only exists for JS projects and is used by the JS libraries to know which resources they should communicate with.\n.gitignore: This file is updated by the Amplify CLI to ignore certain generated files and should not be stored in Git.\n\nThese changes should all be stored in Git to have a good starting point for your project. In general, changes made by the Amplify CLI should be committed to Git because those are the changes in your Infrastructure as Code setup that Amplify is updating for you based on your CLI choices.\nAdding backend functionality to project\nOnce the project is initialized with Amplify, we can add backend functionality.\nBackend functionality\nWhen we talk about backend functionality, we mean AWS Cloud services that will be generated by the CLI and consumed by our frontend application. For example, adding the Amplify (REST) API category will make use of AWS API Gateway. Adding the Amplify Auth category will make use AWS Cognito. Here is a diagram that shows a deployment of a REST backend created with AWS Amplify:\n\nHere we can see that the CLI generates a set of AWS resources. Each resource is part of an Amplify category. On the right side we can see that our frontend application will make use of the Amplify SDK to use these resources.\nUsing the CLI\nWe can add backend functionality by running \u201camplify add <<name of category>>\u201d in the CLI. In this case, if I run \u201camplify add auth\u201d I will get the following questions:\n\nNote that this is one of the more straightforward functionalities to add. When adding API, for example, you will have to answer many more questions. Once we are done running this command, there will be changes in our project:\n\namplify/team-provider-info.json: It will now contain a \u201ccategories\u201d object containing the \u201cauth\u201d category with identifiers of the resource.\namplify/backend/backend-config.json: It will now contain an object called \u201cauth\u201d with information related to the AWS service and how it needs to be configured, in this case, AWS Cognito.\namplify/backend/auth/<<some identifier>>/cli-inputs.json:\u00a0This file contains the choices you made via the CLI for the auth category. This is the file that will be committed in Git with the state of the auth category for this project. This is much better than adding the CloudFormation templates themselves to Git, as that would make fixing merge conflicts harder.\namplify/backend/auth/<<some identifier>>/build/<<some identifier>>-cloudformation-template.json: This is the actual CloudFormation template which will be used. This file is already added to the .gitignore by Amplify and will be regenerated when there are relevant changes.\namplify/backend/auth/<<some identifier>>/build/parameters.json: Contains environment specific parameters for the CloudFormation template. It should also be ignored in Git.\n\nIt is important to note that we have not yet done anything in AWS. These changes all remain local until the next step.\nPushing backend changes to AWS\nAfter adding and updating functionality that we need in our application via the CLI, we can push the local changes to AWS running \u201camplify push\u201d. This is where we will be making changes in AWS and generating the resources that will be needed to support the functionality we want. With this command, we will be generating several resources into our CloudFormation stack by just answering questions in a CLI.\nWhen running the command, the CLI will compare the state of the infrastructure in the S3 deployment bucket and the local changes and give a preview of the changes that will happen if the push continues. In the case of the auth category we added in the last section; the preview would look as follows:\n\nWe can see that new resources will be created for the auth category. If we choose \u201cyes\u201d then the CLI will update the S3 bucket with the new CloudFormation templates and use CloudFormation to create/update the stack for our Amplify project. One other important thing to note is that once the CLI starts pushing the changes to AWS, it will also update the aws-exports.js file with the new references so that the frontend libraries can make use of this.\nAt any point,\u00a0we can run amplify status to see the state of your current changes. After pushing the changes and running amplify status, we can see that we no longer have any changes:\n\nWe can also check out our CloudFormation stack to see all the new resources created.\nThis is the primary development cycle for working with AWS Amplify using the CLI. In short, we:\n\nMake local changes by running \u201camplify add <<category>>\u201d or \u201camplify update <<category>>\u201d\nWe run \u201camplify push\u201d and check if the changes make sense\nWe click on \u201cYes\u201d to let the push continue\n(Optional) We check CloudFormation to see if the stack is in the \u201cCREATE_COMPLETE\u201d or \u201cUPDATE_COMPLETE\u201d status\n\nConnecting backend and frontend\nBesides the CLI, Amplify also provides libraries so that your frontend application can make use of the generated resources in an easy way. We need to inform the frontend libraries of the backend resources we have generated and how to find them.\nTo get this working, we first need to install the libraries. You can find tutorials for all of the integrations here. For Angular, we can run:\n\u201cnpm install \u2013save aws-amplify @aws-amplify/ui-angular@1.x.x \u201c\nFurthermore, we need to configure the Amplify library with the aws-exports.js to know which resources it can use for which category of Amplify functionality. This is usually done somewhere in your frontend code in the earliest part of the app lifecycle. For Angular, we add it in the src/main.ts:\n\nOnce this is done, we can run functionality such as \u201cAmplify.Auth.register(userInfo)\u201d to register a new user. We will look into more concrete examples in the following blogs.\nThe complete flow of adding Amplify functionality and using it can be illustrated in the following way:\n\nUp next\u2026\nIn this blog we looked at how AWS Amplify works behind the scenes. If you are considering using Amplify for real life projects, this information will help you have the confidence to make changes and fix issues. In the next blog, we will be looking at why you should consider using AWS Amplify, when to use it, when not to use it and when you should look at alternatives. See you in the next one!\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30110, "title": "Creating your serverless web-application using AWS CDK \u2013 part 2", "url": "https://www.luminis.eu/blog/creating-your-serverless-web-application-using-aws-cdk-part-2/", "updated_at": "2021-12-21T10:58:40", "body": "Welcome to part two of my blog about creating a serverless web-applications using AWS CDK. In the first part of this blog I explained why I prefer to use infrastructure as code and AWS CDK.\nA short recap: we use infrastructure as code because it gives us the opportunity to maintain our infrastructure with greater ease, as opposed to configuring our resources manually using the AWS console. My infrastructure tool of choice in this is AWS CDK. I chose AWS CDK because I can use it to define infrastructure in a programming language I already know, and because it offers me a great balance between flexibility when I need it and great defaults so I do not have to do too much work myself.\nIn the first part, we also created the basic setup for our serverless web-application with AWS CDK. However it is not quite ready for us to use in production.\nTo get the application production ready, we need to address the following:\n\nThe url to the application is something.s3-website-region.amazonaws.com. This does not help users easily reach the application.\nThe application does not offer a HTTPS connection for the frontend, so access to the application is not secure.\nThe current application has no user authentication. You can access the data of any user by simply entering the correct username.\n\nLeveraging the cloud\nIn addressing the topics I just mentioned, we can see some of the true potential of serverless applications built in the cloud. Setting up HTTPS with certificates, DNS or user authentication can be a lot of work. Even if you decide to use existing services to save you from doing all the work yourself, you will probably still wind up with more manual steps than you would care for. Is that not what we set out to prevent in the first place? Using Amazon\u2019s Web Services you really can use the hard work of others who have come before you. Join me as I show you how adding three new services with just a few lines of codes can bring all we need to get our application ready for production.\nOur production setup will look like this:\n\nWe will use Route53 to link a custom domain we own to the application. This step also allows us to create certificates that we need to enable HTTPS for the frontend. To get HTTPS going for our frontend we also need CloudFront. Finally we add Cognito for user management and authentication. The frontend will redirect users to Cognito to login and receive authentication tokens. These tokens will be validated by API Gateway using a new Lambda function we add.\nDomain\nTo allow users easy access to our application and to create HTTPS certificates, we need to have a custom domain. We also need to have access to the public hosted zone for that domain. A hosted zone is a concept used in Route53. It represents a collection of DNS records that can be managed together, belonging to a single parent domain name. The easiest way to get a custom domain that is ready to use with AWS is to register one through Route53 in the AWS portal. If you register a domain this way, AWS automatically creates a public hosted zone for that domain for you.\nAll we need to get access to it in the TodoApplicationStack is install and import @aws-cdk/aws-route53 and add this code:\n\nconst hostedZone = route53.HostedZone.fromLookup(this, 'TodoApplicationHostedZone', {\r\n  domainName:'tomhanekamp.com'\r\n});\nWhat we do here is a lookup of a hosted zone based on the domain name. For my to do list application I registered tomhanekamp.com as a domain. A condition for this lookup is that the hosted zone is registered in the same AWS account that you use to deploy the CDK stack. Also, the account and region need to be passed into the TodoApplicationStack, which is done by using environment variables. For my to do list application I chose to specify the environment by putting the following code into my bin/todo_application.ts:\n\n#!/usr/bin/env node\r\nimport 'source-map-support/register';\r\nimport * as cdk from '@aws-cdk/core';\r\nimport { TodoApplicationStack } from '../lib/todo_application-stack';\r\n\r\nconst app = new cdk.App();\r\nnew TodoApplicationStack(app, 'TodoApplicationStack', {\r\n  env: {\r\n    account: process.env.CDK_DEFAULT_ACCOUNT,\r\n    region: process.env.CDK_DEFAULT_REGION\r\n  }\r\n});\nBy doing this, I make CDK take the default account and region from my AWS CLI configuration.\n\n\nCertificates\nThe next thing we do for the production set-up is get some certificates for the HTTPS connections to the frontend and the API. You might wonder why we need a certificate for the API as well since it already allows HTTPS connections. While this is true, the API can currently only be accessed on a amazonaws.com domain. Since we are setting up a custom domain anyway I also want the API to be accessible on this domain. As the domain name is embedded in the certificate, this means we need to create a new one.\n\nTo create the certificates, we need @aws-cdk/aws-certificatemanager installed and imported in the TodoApplicationStack. We also add this code:\n\nconst frontendCertificate = new acm.DnsValidatedCertificate(this, 'TodoApplicationFrontendCertificate', {\r\n  domainName: 'todoapplication.tomhanekamp.com',\r\n  hostedZone: hostedZone,\r\n  region: 'us-east-1'\r\n});\r\n\r\nconst apiCertificate = new acm.DnsValidatedCertificate(this, 'TodoApplicationApiCertificate', {\r\n  domainName: 'todoapplication-api.tomhanekamp.com',\r\n  hostedZone: hostedZone,\r\n});\nWith the code above we create two certificates. These certificates will be automatically validated using DNS validation. To make the validation succeed, we specify the Route53 hosted zone we looked up earlier. Apart from the hosted zone we also specify the domain names for the certificates. I use the subdomain todoapplication.tomhanekamp.com for the frontend and todoapplication-api.tomhanekamp.com for the API. If you wonder why the frontend certificate has a third property where we set the region to \u201cus-east-1\u201d, well spotted. This is due to a requirement of CloudFront that certificates requested through ACM for usage with CloudFront should be requested for this region.\nCloudFront\nNow that we have the certificates we need. We can set up the CloudFront distribution. To do this, we install and import @aws-cdk/aws-cloudfront and the following to TodoApplicationStack:\n\n\n\nconst distribution = new cloudfront.CloudFrontWebDistribution(this, 'SiteDistribution', {\r\n  originConfigs: [\r\n    {\r\n      s3OriginSource: {\r\n        s3BucketSource: frontendBucket\r\n      },\r\n      behaviors : [ { isDefaultBehavior:true } ],\r\n    }\r\n  ],\r\n  viewerCertificate: {\r\n    aliases: [ 'todoapplication.tomhanekamp.com' ],\r\n    props: {\r\n      acmCertificateArn: frontendCertificate.certificateArn,\r\n      sslSupportMethod: \"sni-only\",\r\n      minimumProtocolVersion: \"TLSv1.2_2021\"\r\n    }\r\n  }\r\n});\nThe first setting we enter into the CloudFront web distribution, is a set of origin configs. These configurations specify the location of the source files for the distribution as well as some configuration for how to handle the origin. To set up our web application we need only a single origin, which is the S3 bucket hosting the frontend. In terms of behaviour we do not actually need to change anything from the defaults. However we must specify at least one default behaviour, so that is what we do.\nThe other thing we set onto the distribution is that we enable HTTPS. This is after all the main reason we included a CloudFront distribution in our deployment in the first place. We do this by specifying a viewer certificate. Our viewer certificate has a single alias, which in my case is todoapplication.tomhanekamp.com, the hostname for the frontend.\nWe also give it a couple of properties. First we specify the certificate ARN to be the one of the frontend certificate that we created earlier. The next setting is pretty important if you want to avoid a high bill from Amazon. The SSLSupportMethod setting offers two options, sni-only and vip. The first option makes the distribution accept HTTPS only from clients that support server name indication. SNI is supported by most browsers and clients and therefore should suit most use cases just fine. The second option, \u201cVIP\u201d, makes AWS reserve a dedicated IP address for your endpoint, and they will charge you for that. This option comes at a $600 a month prorated surcharge. Finally we also specify the minimum security protocol version. I choose the highest available and recommended option here.\nWith the CloudFront distribution in place, we now have HTTPS support for our application completely finished.\nDNS records\nEarlier in this blog we registered our custom domain and we looked up the public hosted zone that came with it. We now complete the process of adding a custom domain by creating alias records for the frontend and API. To learn more about alias records and when to use them, I recommend this page.\nThe first thing we do to start is install and import @aws-cdk/aws-route53-targets.\nFrontend\nLinking our custom domain to the frontend requires little work. We already configured the CloudFront distribution with the right settings. Now all we do is add the following code to the stack:\n\n\nconst websiteARecord = new route53.ARecord(this, \"TodoApplicationAPIRecord\", {\r\n  recordName: 'todoapplication.tomhanekamp.com',\r\n  zone: hostedZone,\r\n  target: route53.RecordTarget.fromAlias(new route53Targets.CloudFrontTarget(distribution))\r\n});\n\nThis adds a DNS \u201cA record\u201d to the stack. To set the domain name for the record we specify it as record name and we link it to the hosted zone we looked up earlier. Finally we set a target. The target we use is a so called AliasRecordTarget.\u00a0 To create it, we set the CloudFront distribution we created earlier as input. This points the record to the distribution. At the same time, because we used an AliasRecordTarget, it turns the record from a regular \u201cA record\u201d into an \u201calias record\u201d.\nAPI\nTo link our custom domain to the API, we first need to add some configuration to the API Gateway we added in the basic setup. Back then we added a RestApi construct and we only set the name of the API Gateway. We now change that code to the following:\n\nconst apiGateway = new apigateway.RestApi(this, 'TodoApplicationApiGateway', {\r\n  restApiName: 'TodoApplicationApi',\r\n  domainName: {\r\n    domainName: 'todoapplication-api.tomhanekamp.com',\r\n    certificate: apiCertificate,\r\n    securityPolicy: apigateway.SecurityPolicy.TLS_1_2\r\n  }\r\n})\nThe additions we make here configure a custom domain for the API Gateway. We set the domain name that will be used to reach the API Gateway, in my case todoapplication-api.tomhanekamp.com. We also configure the certificate for HTTPS on the API to be the apiCertificate we created earlier. Finally we set the minimum required TLS version for the HTTPS connection to the API Gateway. I set this to TLS 1.2 which is the safest available option at this time.\nAfter making this change to the API Gateway, we add another alias record to the stack using the following code:\n\n\nconst apiARecord = new route53.ARecord( this, \"TodoApplicationAPIRecord\", {\r\n  recordName: 'todoapplication-api.tomhanekamp.com',\r\n  zone: hostedZone,\r\n  target: route53.RecordTarget.fromAlias(newroute53Targets.ApiGateway(apiGateway))\r\n});\n\nYou will notice that this code is very similar to that for the frontend DNS record. The only changes are the record name and the target. We still create an AliasRecordTarget as the target, but this time we set the API Gateway as input.\nUser authentication\nThe last bit of functionality we add to make the application ready for production is user authentication. We do this by adding a Cognito user pool to our deployment where users can sign up and log in to retrieve tokens. We also need to add a functionality to our API that allows it to verify these tokens.\nUser pool\nTo add a user pool, we need to import @aws-cdk/aws-cognito. We can then add the following to TodoApplicationStack:\n\nconst userPool = new cognito.UserPool(this, \"TodoApplicationUserPool\", {\r\n  selfSignUpEnabled: true,\r\n  signInAliases: { email: true },\r\n  autoVerify: { email: true },\r\n  removalPolicy: cdk.RemovalPolicy.DESTROY\r\n});\r\nconst userPoolClient = userPool.addClient(\"TodoApplicationUserPoolClient\", { \r\n  oAuth: { \r\n    flows: { \r\n      authorizationCodeGrant:true, \r\n    }, \r\n    scopes: [ cognito.OAuthScope.OPENID ], \r\n    callbackUrls: [ `https://todoapplication.tomhanekamp.com/` ], \r\n    logoutUrls: [ `https://todoapplication.tomhanekamp.com/` ] \r\n  } \r\n});\r\nuserPool.addDomain(\"TodoApplicationCognitoDomain\", {\r\n  cognitoDomain: {\r\n    domainPrefix: \"todo-application\",\r\n  },\r\n});\r\n\n\nThe first piece of code adds the user pool. In the case of my application, I want anybody to be able to sign up for it using their email addresses without the need for manual verification. This reflects in the settings I pass into the user pool.\nFirst of these settings is selfSignUpEnabled. If you set this to false, users can only be created by an administrator of the user pool. The second setting I add is signInAliases. This setting defines the methods users can use to sign up and sign in to the user. As I mentioned earlier, I want user to use their email address. The third setting to the user pool is autoVerify. When users sign up to the user pool, they account will need to be verified. You can choose to allow this only as a manual action by an administrator. You can also opt to have Cognito automatically verify users by sending them a verification link via email, which is what I did here. Finally I set the removalPolicy for the user pool to destroy, as the default is \u201cretain\u201d.\nWith the user pool added, we register our application as app client to the user pool. This allows it to call the API\u2019s of the user pool to register and sign in users. When registering the application as client we specify the OAuth settings for this client. How to best set up OAuth for your application is topic that deserve a blog post of its own and has little to do with using CDK. You can find some explanation in the AWS documentation for configuring an app client and even more detail here.\nThe last bit of code configures the user pool domain. In this case I opted to use an Amazon Cognito hosted domain with todo-application as domain prefix. This means the urls for the sign-up and sign-in pages of my user pool are in a domain from Amazon Cognito, such as amazoncognito.com.\nVerifying tokens\nIn order to verify the tokens that are sent to the API, I use the Cognito user pool that we created as authorizer. I configure this authorizer for the methods in the API. Every time these methods are called, this authorizer is called upon to verify the token.\nWe add this authorizer to the TodoApplicationStack with the following code:\n\n\nconst authorizer = new apigateway.CognitoUserPoolsAuthorizer(this, 'TodoApplicationAuthorizer', {\r\n  cognitoUserPools: [userPool]\r\n});\n\nWith this code we first add the authorizer itself. We do not need to configure a lot of settings for it, all it needs is a reference to the user pool where the tokens will be verified.\nThe second thing we do is that we add this authorizer to the API. We do this by altering a piece of code we added in the first part of the blog:\n\n\nitemResource.addMethod('PUT', new apigateway.LambdaIntegration(addItemLambda), {\r\n  authorizer: authorizer\r\n})\r\n\r\nitemResource.addMethod('GET', new apigateway.LambdaIntegration(getItemsLambda), {\r\n  authorizer: authorizer\r\n})\n\nIn the first part of this blog we added the HTTP GET and PUT methods to the API and we specified the Lambda Function to call for these methods. Back then we did not specify any options for the method. Now however we specify one option, the authorizer.\nConfig JS\nWith all the resource in place for our production set up, we have one thing left to do before we can deploy it. In the previous blog we added code to upload a config.js file with the API url to S3. In this production setup, our frontend also needs to information about the user pool client we added. To get this information to the frontend we add it to the contents of the config.js file.\nReplace the code for the frontendConfig with the following:\n\nconst frontendConfig = {\r\n  serverUrl: `https://todoapplication.tomhanekamp.com/`,\r\n  region: 'eu-west-1',\r\n  cognitoClientId: userPoolClient.userPoolClientId,\r\n  cognitoDomain: 'todo-application',\r\n  itemsApi: 'https://todoapplication-api.tomhanekamp.com/',\r\n  lastChanged: newDate().toUTCString()\r\n};\n\nYou will see that we added four new properties to the configuration. These properties are all used by the CognitoService in the frontend to send requests to the user pool. You might notice that only one of the properties, the cognitoClientId, that are now in the frontend configuration comes from CDK instance in our stack. The other properties are static values. This means we could also just hard code them somewhere in the frontend. However, even though the values of these properties are static now, when we change the domain or region where the application is deployed they would still change. I therefore keep these properties in the frontendConfig. This also helps me if I decide to configure multiple environments for my application in the future as all the environment specific code is in TodoApplicationStack.\nBecause we added a property to the frontenConfig that relies on userPoolClient, we should also add a dependency for that:\n\ns3Upload.node.addDependency(userPoolClient);\n\nDeploying\nNow that all the code is complete, we can deploy our application. Once again let\u2019s make sure the compile the code we deploy to AWS. In my project I do this with:\nnpm install\r\nnpm run build\non the frontend and Lambda functions as they are NodeJS based. With everything compile we deploy the stack with:\ncdk deploy\nIf that passes, you can go to the custom you registered to open your production ready application.\nMy Todo list application now looks like this:\n\nIt doesn\u2019t look all that different from what I showed at the end of the previous blogs. However if you look closely you will see signs of all that we have accomplished in this blog.\nFirst of all, the hostname I used to reach it is now todoapplication.tomhanekamp.com\u00a0because of the custom domain we added. A big improvement from todoapplicationstack-todoapplicationfrontend8b34e-65bjgh3il4ru.s3-website-eu-west-1.amazonaws.com that it used to be when it comes to being easy to find for my users.\nIf you look to the left of the hostname in the address bar of my browser, you will also see a padded lock where it previously said \u201cNot Secure\u201d. This means I am now connected to the application using HTTPS and the certificate is considered valid by my browser.\nFinally you might notice that the application no longer has an input field for the username and that a logout button was added instead. When I first opened the application after deploying it, it actually looked like this:\n\nWhen you click on the Login button, the application directs you to Cognito. Here you can sign up for an account that you verify via email. After you sign in, the email address you used to sign up is also your username.\nWith these changes we have finished what we set out to do. Our application is now ready to be used in production.\nYou want to have a closer look at some of the code, you can find the complete solution on GitHub.\n", "tags": ["aws", "cdk", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 30557, "title": "Move faster with new feature flag capabilities in AWS", "url": "https://www.luminis.eu/blog/cloud-en/move-faster-with-new-feature-flag-capabilities-in-aws/", "updated_at": "2021-12-16T12:16:27", "body": "I always get excited about things that allow you to bring new ideas to production faster. And it seems we are in luck! During the yearly AWS re:Invent conference that took place in Las Vegas, AWS launched several services that help you do just that!\nBusiness around us is changing more and more rapidly. Year by year, the State of Devops research confirms that high performing companies excel in adapting to those changes by optimizing their ability to experiment, deliver quickly and gather feedback directly. You need to try many ideas and quickly find out which are successful and which are not. That ability to make many small changes reduces risk and increases the ability to innovate.\nThe new AWS services around feature flags enable organizations to achieve these goals. I will explain these briefly and show you how you can leverage them.\n\nMoving fast\nSo what ingredients do you need to go fast? First, you have to optimize your organization and teams so they can come up with diverse ideas. Make sure you enable them to try those out and get feedback quickly without having to go through committees to get approval.\nSecond, on the engineering side, you need to learn to do everything in small steps. Integrate all your changes with the rest of your team often, at least daily. This way of working is called trunk-based development. The key here is never letting go of high quality and keeping your software in a constant state of being able to deploy it to production. To achieve that, you need a (continuous) delivery pipeline that automates all the steps needed to get from commit to deploy. As excellently explained by Dave Farley, the key responsibility of that pipeline is not to ensure that the software is 100% correct \u2013 that is not realistic. Instead, the pipeline needs to do all that it can to figure out if something might be broken or misbehaving. That means you\u2019ll need at least a solid automated test suite that provides you with confidence that on the functional side everything works. Next to that, you can add validations to your pipeline for things like performance, code quality, and any other aspects that are important for your application.\nIf you do everything in small steps, does that mean you cannot have big innovations? No, of course it doesn\u2019t. Big innovations never happen in one go. They are the combination of smaller increments, even the failed ones that you learned from. Like Edison\u2019s light bulb, which was the result of thousands of failed experiments.\nFeedback, as early as possible\nA common trick is to separate the deployment of changes from the release of a feature. By using feature flags, you can include the new code you are writing without the feature showing up or being active. With all the code being deployed continuously, the act of \u201creleasing\u201d a feature simply becomes toggling the feature flag.\nThis way of working helps greatly in catching mistakes or wrong assumptions in your ideas early, while it is still easy to try another approach. It is much better to discover that your idea does not work after a few hours than finding out weeks later, when you\u2019ve already invested a lot of time. You also make it much easier to experiment with new features much earlier in their development, maybe just exposing them to a few interested beta users and receive feedback before you make the feature generally available.\n\nEnter feature flags\nI always like keeping things simple. The easiest way to implement feature flags is to just add some booleans and \u2018if statements\u2019 in your code and allow these to be toggled between the different environments you have. For example, showing the new feature you are working on in your staging environment, but hiding and deactivating it in production. To be honest, in many cases, a simple mechanism like this is more than sufficient.\nDo realize there is a tradeoff when using feature flags. Having these branches in your code means you add complexity and you make your application less predictable. In many cases the benefits greatly outweigh the downside, as long as you keep it under control. Don\u2019t let hundreds of feature flags linger around. Be responsible, be a good craftsman and keep your codebase clean. Once a feature is released, clean up all traces of the flag. That way you minimize complexity and keep your codebase lean and mean and easy to change. After all, you want to keep doing more experiments and keep delivering value in a sustainable way.\nDepending on how long your build pipeline takes, having to change booleans in code may mean that it takes a while to toggle a feature in production. This may be ok in some cases, but when it turns out that a feature doesn\u2019t play nice on production yet, you want to toggle it off again quickly. What if you could toggle it in real-time and let someone else handle part of the complexity around feature flags?\nLaunch 1: AWS AppConfig \u2013 Feature Flags\nAWS AppConfig has been around since 2019 and allows you to roll out configuration changes to your applications in real-time, without having to deploy code or take your application out of service. It can validate and then roll out configuration changes gradually while monitoring your application and rolling back changes in case of errors, minimizing impact to users.\nRecently AWS launched AWS AppConfig Feature Flags, which builds on AppConfig and makes it easier to roll out new configurations of your feature toggles. You can now setup and toggle feature flags directly from the AWS Console. Versions of your feature flags configuration are tracked and as with config changes, you can manage and monitor the gradual deployment to your environments.\n\nAs feature flags are just booleans, there is no validation around them. You can also add custom attributes to flags. An example of what I would use these for is creating flags that are only enabled for specific beta tenants. For these attributes you can set up some simple validation rules in the UI. That may not be as powerful as writing a validation lambda as you can do for normal AppConfig validation, but it sure is a lot simpler and probably sufficient. And don\u2019t forget, less handcrafted code means you reduce risk.\n\nApart from feature flags, you can also use this feature to toggle operational aspects of your application quickly. For example, in case of an incident you can temporarily switch off non-critical functionality or behavior to reduce load and keep the essentials of your service working, allowing your team to focus on resolving the incident.\nThat all sounds fantastic, but it does mean that releases of features are no longer tracked in your central git history, instead they are changed through the AWS Console. Luckily, the Console does track versions of your configurations and shows you the history of feature flags and configuration deployments that have been done to each of your environments. So, it may not be in one place anymore, but you can still audit when features went live or were rolled back. Just be aware that you are making this tradeoff.\n\nLaunch 2: AWS CloudWatch \u2013 Evidently\nWhere AppConfig Feature Flags is geared towards simplifying the technicality of managing feature flags and their deployment, Evidently is all about connecting the data you collect for experiments and A/B tests around feature roll outs that you do with smaller sets of users before making the feature generally available.\nEvidently allows you to set up multiple variants of a feature and test these variants on different subsets of users at the same time. Once you have set up and are running such an experiment, Evidently also helps in statistically analyzing which variant is performing better, based on data collected during the experiment, giving you confidence in making the right choices.\n\nIn today\u2019s fast paced world, you have to continuously reinvent your business to remain relevant. To do so you have to innovate and Evidently really helps you with tools to apply that mindset of running experiments and basing your decisions on the data that you gather.\n\nLaunch 3: Amazon CloudWatch RUM (Real User Monitoring)\nNow obviously, to make a service like Evidently work well, you need to collect the right data. Data that helps you prove the effectiveness and desired behavior of the features you build. To help with this, AWS launched RUM, another service under the CloudWatch umbrella.\nCloudWatch is all about monitoring, but so far that was mostly oriented towards raw infrastructure and backend metrics of apps running inside AWS. RUM enhances that with capabilities to monitor web application performance, aimed at user experience. You inject some JavaScript into your web application frontend which then allows you to anonymously collect page load and layout performance metrics, JavaScript errors and client-side experienced HTTP errors.\n\nWhere traditional CloudWatch metrics are somewhat raw, RUM feels more like Google Analytics, giving you out-of-the-box dashboards that provide visibility into your application\u2019s reliability, performance, and end user satisfaction. The service performs analysis on the collected data and breaks it down so you can gain insights about which areas of your application\u2019s user experience can be improved.\n\nOn the observability side, CloudWatch RUM integrates with AWS X-Ray, so you can view traces and segments for end user requests. You can even see your clients in CloudWatch ServiceLens service maps.\n\nPricing\nAs usual with AWS, pricing for these services are based on usage, which means you can try these out on a small scale and then evaluate if the benefits they bring you are worth the cost. Once you use these services in production, always put up some cost monitoring so you remain aware of what you are spending. Below you will find an overview of the pricing for these services. For details, look at their pricing page.\nAWS AppConfig Feature Flags are charged by the number of times you request and receive config changes:\n\n$0.0000002 per configuration request via API Calls\n$0.0008 per configuration received\n\nAWS CloudWatch Evidently has a first time free trial that includes 3 million Evidently events and 10 million Evidently analysis units per account. After that you pay:\n\n$5 per 1 million events (user actions & assignment events)\n$7.50 per 1 million analysis units\n\nAWS CloudWatch RUM allows you to control costs by configuring it to only analyze a limited percentage of user sessions. There is a first time free trial that includes 1 million RUM events per account. After that you pay:\n\n$1 for every 100,000 events collected (page view, JavaScript error, HTTP error)\n\nAlternatives\nCompared to products that specialize in this field like LaunchDarkly or Optimizely, these AWS services may feel somewhat bare bones. This is no coincidence, as Werner Vogels said: AWS services are designed to be building blocks, not frameworks.\n\nAWS may not be as feature rich as some of these others but having it all integrated into the rest of your service stack can be a big benefit. The low barrier to entry means you can start using it today and get value out of it quickly, or switch to something else later if you discover that your needs have grown.\nAnd don\u2019t forget, if you don\u2019t need all the experimentation statistics or real-time flag controls, just keep it simple and add a few booleans in your code.\nConclusion\nIt is great to see that AWS is serious about giving their users the keys they need for innovation. What I personally find most exciting is that with this combination of services, AWS makes it easier for all of us to adopt that mindset of doing experiment-based and data-driven roll outs.\nIf you need help to introduce that mindset of experimentation, don\u2019t hesitate to reach out to us!\nTo learn more about these AWS services, check out the official blogs:\n\nIntroducing AWS AppConfig Feature Flags In Preview\nNew \u2013 Amazon CloudWatch Evidently \u2013 Experiments and Feature Management\nNew \u2013 Real-User Monitoring for Amazon CloudWatch\n\n", "tags": ["aws", "AWS re:Invent 2021", "cloud"], "categories": ["Blog", "Cloud"]}
{"post_id": 30355, "title": "The Amplify Series, Part 1: What is AWS Amplify?", "url": "https://www.luminis.eu/blog/cloud-en/the-amplify-series-part-1-what-is-aws-amplify/", "updated_at": "2023-05-08T13:09:15", "body": "The way we develop software is rapidly and increasingly changing with the world around us. One of the more exciting developments is the advent of serverless technology, which, when applied correctly, opens doors to organizational agility and speed \u2014 and thus innovation and happy users.\nOne of the quickest ways to get familiar with this way of working is using AWS Amplify, a set of powerful tools and features that can help you quickly and easily build cloud-powered, extensible, and scalable full-stack applications.\nAmplify has selected a subset of all AWS services and default settings to help you quickly get started with building your Cloud Native application and thus provide value at a higher pace. This might seem like you are losing control over your application and that you will be limited somehow; however, with AWS Amplify, you can still override any defaults and access any AWS service if you need to.\nBlog series\nIn this blog series I will introduce AWS Amplify, explain why you would want to use it, how it works, and give detailed examples of the several categories of functionality available out-of-the-box with AWS Amplify.\nThe entire series will cover the following topics:\n\nWhat is AWS Amplify?\nHow does AWS Amplify work?\nWhy should you use AWS Amplify?\nDeveloping and deploying a Cloud Native application with AWS Amplify\nUploading and retrieving images with Amplify Storage\nUsing the power of AI and Machine Learning with Amplify Predictions\nTrack app usage with Amplify Analytics\nLocation functionality with Amplify Geo\nChatbots with Amplify Interactions\nDo everything with AWS Amplify! (Extensibility)\nLow Code with Amplify Studio\n\nWe hope you will enjoy this series and extend your AWS Amplify knowledge.\n\u00a0\nWhat is AWS Amplify?\nIn this blog, we will give a high-level overview of the functionalities and tools provided by Amplify.\nFunctionality categories\nWith Amplify, we are more concerned with what we want to build instead of how it will work with AWS. Amplify offers the possibility to generate AWS services based on a set of functionality categories. At the moment of writing, these categories are:\n\nAuthentication: Fast and easy authentication and authorization out-of-the-box\nAPI: Create and use a REST or GraphQL API as a gateway to other services in AWS, such as Lambda functions or databases\nStorage: Used for storing and retrieving user-generated content on AWS such as photos, videos, or other files\nGeo: Location-based functionality such as maps and location search\nHosting:\u00a0Hosting your app on AWS and setting up a CI/CD pipeline\nInteractions: Create chatbots for your application\nPubSub: Connectivity with cloud-based message-oriented middleware\nDataStore: Use on-device persistent storage to be able to use your application when offline\nFunctions: Create Lambda functions that are linked to an API or create CRON jobs\nAnalytics: Add tracking of user behavior of your apps\nAI/ML Predictions: Use AI/ML to add functionality such as text translation, speech-to-text, and entity recognition in images\nPush Notifications: Send messages to your customers to improve engagement\nExtensibility: Leverage any other AWS service that is not covered by the categories above by using AWS CDK\n\nAs you can see, the functionality categories already cover many everyday use cases for web and mobile apps while still offering flexibility by leveraging CDK.\nTools\n\nAWS Amplify consists of 4 tools that help you develop cloud-native applications. These are:\n\nCLI: The Command-line interface that you can use to initialize your Amplify project and generate AWS resources\nLibraries/SDK: The set of libraries used by your frontend of any kind to more easily be able to communicate with the AWS resources created with Amplify\nConsole: The AWS service where you can keep track of your Amplify projects and configure many settings related to the deployment and hosting of your AWS Amplify app\nAmplify Studio: Amplify Studio allows you to configure your AWS resources using the console and visual modeling. Everything you change here is still compatible with the CLI, and you can even use the CLI to download the resources you designed in the studio.\n\nCLI\nWith the command-line interface (CLI), you can run a few commands, you will be asked specific questions about the functionality you want to achieve, and it will generate the AWS resources based on your answers. Here is an example of such an exchange with the Amplify CLI:\n\nIn this example, we add a REST API to my project using the Amplify CLI. Depending on what functionality you want to add, you will need to answer different questions.\nIn the following blogs in this series, we will be using the CLI a lot and show you everything from starting a project to adding several functionalities. For now, it is important that you have a broad idea of how the CLI works.\nLibraries (SDK)\nWe can use the Amplify libraries to connect our app to AWS resources we created using the CLI. There are libraries for all of the most popular frameworks to develop web and mobile apps:\n\nThe combination of the CLI and libraries makes it possible for your app to use the AWS resources instantly, without knowing the IP or ARN for every resource you created. Here is an example of making a REST call to an endpoint created with the AWS Amplify in JavaScript:\n\nThere are helper functions for most functionality categories covered by Amplify that help you use the AWS resources you have created via the CLI.\nConsole\nJust like services such as API Gateway or AWS Lambda, AWS Amplify is can now also be considered a first-class citizen and can be found as a service in the AWS console. Here is an example of what it looks like when you have active projects:\n\nThe following is a list of things you can do on the Amplify console:\nCI/CD Pipeline\nYou can set up a CI/CD pipeline via the Amplify CLI and track the progress via the Amplify Console. Here is a screenshot of what it looks like:\n\nThe default CI/CD pipeline setup has these four steps. However, it is fully configurable either in the Amplify console or by adding a .yml file in the root of your Amplify project. If the project builds and is deployed successfully, you will even see a preview of your app, and you will be provided with a link to the hosted app.\nConnect Git Branch\nYou can connect git branches from your projects to an AWS Amplify environment you have created with the Amplify CLI. Once connected, your app can be built every time you push to that branch, and the frontend and backend environment it is related to will be deployed.Domain management\nBy default, you will get a cloudfront.net URL on which your application is hosted when you first start. However, in the Amplify Console, linking a domain you have registered to a specific git branch of your project is possible. In the case shown above, every time something is merged to develop, a build is run, and once it is deployed, it is available at https://cloudthegame.com.\nConfigure build settings\n\nYou can change the build steps in the console as previously mentioned. You can configure custom webhooks to trigger builds for specific branches, and you can configure the settings for the Amazon Linux container that will run the build. Setting up environment variables is also possible.\nView metrics\n\nWhile services created with the Amplify CLI, such as API Gateway and Lambda, have their metrics, the Amplify console shows you metrics related to your hosted application. You can see metrics such as viewer requests, bytes downloaded, and errors.\nAmplify Studio\nThe Amplify Studio allows you to achieve the same functionality you could using the CLI. The difference is that instead of answering questions in a CLI, you are clicking in the console. You will see that all functionality categories have tabs that you can configure. Once you are done configuring your backend, you can \u201cpull\u201d the backend to your local Amplify project and start using it in your application in the same way you would if you generated the backend using the CLI.\nHere is a screenshot of the studio where we designed a data model for our pizza restaurant:\n\nThere is also a set of functionalities that is exclusive to the Amplify Studio, such as:\n\nUI Library: You can make component UI designs in Figma and generate ReactJS component code. You can even link the components to backend logic in the studio\nData and files:\u00a0You can manage your database (CMS), files, and user/groups.\nAuthentication:\u00a0You can give users access to your Amplify studio environment without creating an AWS account. This can be useful if you have clients who want to manage the data for their application.\n\nIn short, Amplify Studio aims to be a visual development environment. While it is low-code, it is still backed by human-readable code and AWS CloudFormation, the infrastructure-as-code solution on AWS. We will talk more about CloudFormation in the next blog.\nUp next\u2026\nWe hope this blog post has given you a clear overview of Amplify and has gotten you excited and interested in learning more. We can achieve a lot with Amplify out-of-the-box to create a cloud-native application using the most popular Javascript and hybrid mobile frameworks and native Android and iOS. In the next blog, we will look at how AWS Amplify works to achieve all of this functionality. In the subsequent blogs, we will be using Amplify to develop and deploy a cloud-native application. See you there!\nLearn from our experts:30 Jan 2021-Evertson CroesTraining: AWS AmplifyHave you always wanted to create a Cloud native app but you don\u2019t know where to begin? The Cloud can be an intimidating concept. In this training you will get a good introduction to the Cloud and you are going...\n", "tags": ["amplify", "aws", "cloud", "infrastructure as code"], "categories": ["Blog", "Cloud"]}
{"post_id": 30493, "title": "Top Takeaways from AWS re:Invent: 2021 Keynotes and Announcements", "url": "https://www.luminis.eu/blog/cloud-en/top-takeaways-from-aws-reinvent-2021-keynotes-and-announcements/", "updated_at": "2021-12-08T09:57:23", "body": "\n\nAs a cloud enthusiast, I like this time of the year. Early December, just before the Christmas holiday, the anticipation of \u2018getting presents\u2019 from my favorite cloud provider with announcements and extraordinary sessions.\nThis year\u2019s re:Invent took place in Vegas, in-person. Of course, fewer people were on-site because of the Covid measures. I participated virtually and followed all the keynotes, announcements, and leadership & tech sessions. I digested the event, collected my notes and thoughts, then came up with my key takeaways from the keynotes and the announcements. I would like to share these with you today:\n\n\n\n\n\nNo game-changer announcements but improvements\nI have to admit I missed Andy Jassy. This year\u2019s event was the 10th, and the first time we didn\u2019t have his usual speech. Instead, the new CEO of AWS, Adam Selipsky, did the main keynote given in high spirits and with top customer stories. But the speech lacked any significant announcements.\nA reason for this is that we are getting to the point that technology is not as new anymore. Instead, it is mature, as this year was the 15th anniversary of AWS. Or, as Dr. Werner Vogels said in his keynote, much of the innovation this year was going on behind the scenes, as the company worked to simplify operations for AWS customers.\nLet\u2019s go through the key announcements quickly.\nSelipsky announced many new services and features: the new Graviton3 chips, C7g instances powered by the Graviton3 chip, EC2 Trn1 instances, Mainframe Modernization service, Private 5g, Lake Formation updates, Serverless analytics for Redshift, EMR, MSK and Kinesis, SageMaker Canvas, IoT Twinmaker, and IoT Fleetwise.\nThis year\u2019s Infrastructure Keynote, Peter DeSantis, focused on AWS Nitro SSD and Graviton 3, AWS Tranium, and Inferentia Processors. He wrapped up his keynote with sustainability and\u00a0the climate pledge of Amazon.\n\n\n\n\nAWS re:Invent 2021 Infrastructure Keynote by Peter DeSantis\nNext up was Dr. Swami Sivasubramanian with his Machine Learning Keynote. He announced Amazon DevOps Guru for RDS, AWS Database Migration Service (DMS) Fleet Advisor, Amazon Kendra Experience builder, and new SageMaker features.\nLastly, my favorite one this year\u00a0wasDr. Werner Vogels\u2019 keynote on the closing day. After an amazing intro video, he started with an Amazon M1 Mac instance and 30 new Local Zones announcements. Then he introduced maybe the most critical networking-related update of the event, AWS Cloud WAN. Finally, some Cloud Development Kit (CDK) love with Matt Coulter. Highly recommend watching this keynote.\nFan-favorite keynote\n\n\n\n\nYou can check Top Announcements of AWS re:Invent 2021\u00a0here. But here are my top takeaways from the announcements:\nEasier, Better, Faster, Stronger\nAlthough we don\u2019t have a new and shiny service this year that will change our daily work-life, many improvements have been implemented in the background. First, AWS worked on core infrastructure improvements with chip & instance additions to make our computing resources run faster and cheaper with Graviton3 & Trainium chips and M1 Mac Instances.\nWe also see AWS is really focusing on networking improvements and even started competing with network and security companies and ISPs. For me, the two most exciting announcements of the event were on the networking topic.\u00a0The first exciting news was the announcement of\u00a0AWS Private 5G. AWS primarily focused on software development services in the past, such as S3, SQS, EC2, ECS, Lambda, but is currently offering a private 5G networking service. This managed, pay-as-you-go service makes it easy to deploy, operate, and scale your own private cellular network, with the required hardware and software provided by AWS. Of course, this one is not for everyone, but it is a fantastic option for some AWS customers.\nThe\u00a0second most exciting news\u00a0is\u00a0AWS Cloud WAN. With the help of its partners, AWS is offering a Wide Area Network service.\u00a0AWS Cloud WAN provides an easy way to connect your data centers, branch offices, and cloud resources into a centrally managed network and dramatically reduces operational cost and complexity. Doing this on AWS without a third-party WAN product simplifies networking for many companies that need a WAN solution.\nAWS Cloud WAN \u2014 \u201cEasily build, manage, and monitor global wide area networks.\u201d\n\n\n\n\nAs we all know, AWS has already accomplished providing cloud services with its infrastructure, and services and in the past couple of years, they moved to bring their services to customers\u2019 on-premise using\u00a0AWS Outposts. Today they continue with an \u2018uncharted\u2019 area: the mainframe, and mainframe modernization.\nReducing the Complexity\nThere already are over 200 AWS services, and the number keeps growing every year. As a result, the cloud is getting more complex, and it is becoming more and more impossible to know and use all of the services. Because of this, it is crucial to reduce complexity when introducing new technologies.\nAWS reduces the complexity by introducing more serverless offerings. This year the focus was the analytics services: Amazon Redshift, Amazon Elastic MapReduce(EMR), Amazon Managed Streaming for Apache Kafka(MSK), and Amazon Kinesis. Some would argue about this, but to me, the increasing number of serverless services is another sign that Serverless is the next \u201cbig thing\u201d, especially compared to Kubernetes.\nSecurity: Secrets Management can be challenging at the beginning. However, the code review tool Amazon CodeGuru now has Secrets Detector\u00a0to help us. Not a significant feature, but nice to have.\nFor frontend development with minimal complexity:\u00a0AWS Amplify Studio, \u201ca visual development environment that offers frontend developers new features to accelerate UI development with minimal coding,\u201d is available. It can be handy for small applications.\nA final service to mention in terms of reducing complexity, a no-code Machine Learning capability: SageMaker Canvas. This new capability enables people with less technical knowledge to use Machine Learning services.\nNew Pillar: Sustainability\nAWS announced extending AWS Well-Architected Framework by adding a new Sustainability Pillar. Sustainability is becoming one of the hottest topics of the cloud infrastructure industry and we get more and more questions about this topic. I am pleased to see AWS is continuously putting effort into environmental issues and thinking about energy efficiency and sustainability.\nAs we know, the Well-Architected Framework is a list of questions aiming at different pillars to improve the design, architecture, and implementation of our systems and resources. The new pillar helps customers evaluate their workloads in order to reduce energy consumption and improve efficiency.\nAWS\u2019s shared responsibility model applies to sustainability\n\n\nCheck out the AWS blog post about the Sustainability pillar\u00a0here. To learn more or to have AWS Well-Architected Reviews for your workloads, you can always reach out to me.\nEven Better Developer Experience\nAWS has always been developers\u2019 choice when it comes to cloud providers because it offers so much flexibility and ease of use when it comes to its services. AWS, by far, has the best cloud services portfolio to enable DevOps teams in any task. Again this year, many announcements were about improving the developer experience.\nThe major announcement on the Infrastructure as Code tooling was the introduction of AWS Cloud Development Kit(CDK) Version 2. In addition, we can now use\u00a0Construct Hub: it is the home of all CDKs; AWS CDK, CDK8s, CDKtf. Both AWS CDK v2 and Construct Hub are\u00a0currently generally available(GA). Of course, we already knew about these two for a year, but it is good to see AWS actively promote its new developer tooling. Soon AWS CDK will eclipse CloudFormation and become the new standard. If you haven\u2019t tried a CDK yet, I strongly recommend trying it.\nSharable & repeatable cloud constructs thanks to Construct Hub\n\n\n\n\nMore positive news is that AWS offers new SDKs for the Swift, Kotlin, and Rust programming languages. Currently, these SDKs are on the public preview.\nInteresting note, AWS converted an internal tool to external:\u00a0AWS re:Post. It is a community tool to find answers, answer questions, and join groups.\nAfter all, this years re:Invent was another excellent learning experience. I am looking forward to the future ones. See you all at the following AWS events!\n\n\n", "tags": ["Architecture", "aws", "AWS re:Invent 2021", "cloud", "Well-Architected Framework", "Well-Architected Reviews"], "categories": ["Cloud", "News"]}
{"post_id": 30362, "title": "Silver Linings, No Silver Bullets: 3 Perspectives on Innovating with Cloud", "url": "https://www.luminis.eu/blog/silver-linings-no-silver-bullets-3-perspectives-on-innovating-with-cloud/", "updated_at": "2023-04-20T13:41:38", "body": "We\u2019ve heard the phrase, seen the cynical stickers: \u201cThere is no cloud; it\u2019s just someone else\u2019s computer.\u201d That might have been true 15 years ago, but the thing we call the cloud has evolved significantly since. We live in times of accelerating change; organizations that can leverage the cloud effectively will survive them, even thrive in them. How well prepared are you?\nTaking great advantage of the cloud requires a shift in thinking: technology no longer belongs to the IT department (if it ever did). To become cloud natives, organizations need to learn to streamline their effort around everything the cloud offers.\nLet\u2019s look at this paradigm shift from several perspectives: operations, development, and leadership. Each of these perspectives starts with describing the ineffective way of working. We use the term cloud naive (as in: inexperienced or lacking knowledge). Or as the State of DevOps report classifies it: the way of the low performers. Then, we cross the chasm and look at it from the cloud-native perspective, as used by companies that use cloud technology to accelerate their business continuously.\n\nOperations: embrace speed to reduce risk\nLow performance: cloud naive operations\nIn simpler times, developers delivered complete software packages to operations people, who in turn installed and ran them on the appropriate infrastructure. Preferably not too often and with as little change needed as possible. A similar rule determined a snail\u2019s pace of change for platforms: standardization before innovation. Why? Because every change introduced risk and operations were held responsible for it, often having to clean up the mess when things went wrong.\nThis model led operations departments to push back hard on changes and reduce the speed of change\u2014developers adjusted by increasing release sizes and time between releases.\nThe result: slow change, little cooperation, suffering users.\nElite performance: cloud-native operations\nThe way out of this standoff is well-known by now but not always well understood or effectively implemented: DevOps. Making agile teams responsible for business outcomes forces down the classical barriers between the business, developers, and operations, leading to better customer outcomes. Cloud platforms have evolved by adding layer upon layer of abstraction onto the classic data center, leading to hundreds of managed services that teams can continuously leverage to decrease their time-to-value. Perhaps counterintuitively, this increased rate of change enables teams to manage risk better than ever.\nThink about it: if time-to-value is the most critical metric for success, change size logically also gets smaller. Minor changes are easier to manage in case of failure, and deploying multiple times a day yields teams totally in control of their technological solutions.\nThis cloud-native way of doing operations yields precisely what these teams aimed for in the first place: less risk, more control.\n\nDevelopment: focus on outcomes, not technology\nLow performance: cloud naive development\nBefore the emergence of the cloud, servers had to be installed by hand, as did the software running on them. Installing and testing releases was a time-consuming process, mostly done by other teams. So, developers made sure their sparsely released work contained as many features as possible. Any infrastructural requirements and other dependencies had to be carefully communicated and planned with IT operations, dictating long lead times between innovative ideas and customer value.\nAs a result, developers preferred working with stable, minimally variable environments simulated on their development machines. Their products were massive monoliths often written using a single programming language, running on single application container instances. Development feedback might have been fast; customer feedback certainly wasn\u2019t. Developers were often kept totally in the dark about the impact of their changes in production until nightly calls from frustrated IT operations, who spoke in an entirely different IT dialect.\nAgain: slow change, little cooperation, suffering users.\nElite performance: cloud-native development\nThe step-by-step virtualization and containerization of servers, platforms, and runtime environments have enabled development teams to master what was previously the sole job of operations. And more importantly, the marriage of Dev and Ops has given birth to beautiful things like infrastructure as code, CI/CD, and SRE. Managing complex systems this way has opened up opportunities for creating massively scalable but well-observable systems. The old monoliths have been split into small, modular services or even single functions that typically communicate asynchronously using a mind-boggling number of events.\nNow the customer wins. Changing, replacing, or radically rewriting a single, small module and testing and deploying it in the blink of an eye has become a reality. Experimenting \u2014 seeing if an idea for a new job to be done holds up in production \u2014 has become cheap as chips using the pay-as-you-go managed services as provided by cloud platforms like AWS, Azure, and GCP.\n\nLeadership: get out of the way of innovation\nLow performance: cloud naive leadership\nDuring less volatile times, markets were predictable, transformations were carefully planned and budgeted, and users were okay with incremental changes every so often. Software, and the hardware it ran on, were complex and expensive beasts, mastered only by tech masochists stuffed away in cost centers called IT departments. Nothing got built until it was very meticulously specified in requirements documents. Even then, the delivered result was often wildly different from what the market specialists and innovators defined months or years before, if not already outdated.\nOnce more: slow change, little cooperation, suffering users.\nElite performance: cloud-native leadership\nSoftware has been eating the world for a while now. Organizations need to adapt or accept a slow and painful decline into irrelevance. Users now expect to get what they need before realizing they do: a new Netflix series, a recommended product, or even an improved driving experience. Understanding users is key to market success and requires continuous experimentation and software development mastery. The cloud is a massive enabler: it enables organizations to focus on user needs by handing the management of undifferentiated technology over to the cloud provider.\nBut while technical skill development is essential, it is just one part of the innovation puzzle. Another is culture: leaders need to focus everyone\u2019s attention on the importance of time-to-value and agility as the metrics for success. To create empowered, self-sufficient, and effective teams, people need to come together and shatter the walls previously separating them. Lastly, leaders need to take the long view and realize that short-term risk aversion can be a major innovation blocker.\n\nTo boldly go, together\nTransforming an organization from cloud naiveness into cloud-native isn\u2019t for the faint of heart, but very necessary nonetheless. Luckily, Luminis has traveled this path many times before and is a very effective guide on the path to cloud-native. For example, we helped OHRA, one of the largest Dutch digital insurers, migrate its complete data center to the cloud. Another example is the digital transformation of The Learning Network (TLN) we helped shape and implement.\nGet in touch and let us help you kickstart your cloud-native transformation.\nRelated post:cloudwhitepaperWhite paper Cloud migrationCloud migration: 5 effective steps towards a successful result. Download our free white paper. Do you want to migrate your organisation\u2019s systems to the cloud? A cloud migration provides speed and efficiency, among many other advantages. This white paper is...\n", "tags": ["cloud", "devops", "innovation"], "categories": ["Blog", "Cloud"]}
{"post_id": 29809, "title": "Creating your serverless web-application using AWS CDK \u2013 part 1", "url": "https://www.luminis.eu/blog/creating-your-serverless-web-application-using-aws-cdk-part-1/", "updated_at": "2021-11-02T09:44:22", "body": "So you are looking to build a web-application. You want it to be serverless and hosted on AWS, what do you do? In this blog post I will tell you why I like to use CDK for this. I will also explain how to use CDK to set up your own web-application with ease.\nInfrastructure as code\nCreating a serverless application in AWS means that you will be configuring certain AWS resources to behave the way you want them to. By doing this, you set up the infrastructure for your application. A quick and easy way to set up this infrastructure would be to navigate to the AWS portal using your browser and manually configure everything there. But what will you do if you need to deploy your application a second time? You would have to painstakingly document every step you take and every field you fill so that you can repeat it at a later time. Also if you make any changes you need to make sure to update your documentation. Sounds exhausting right?\nThis is where infrastructure as code solutions come in to save the day. By defining your infrastructure as code, just as you would with other code in your application, you can maintain it in a version control system. Any change you make in the infrastructure is being tracked and at any time you can checkout the latest version of the infrastructure and deploy it immediately. You can even automate such deployments to start automatically whenever you check in your changes.\nWhy I use CDK\nNowadays there are plenty of options for you to choose from when it comes to infrastructure as code tooling. Each of these tools comes with its own pro\u2019s and con\u2019s. It is hard to pick one tool that is best for every single situation. CDK however has two strong points which for me, makes it the tool I choose.\nThe first is that as a developer, CDK feels very familiar. CDK allows you to define your infrastructure setup using Typescript, Javascript, Python, Java, C#/.Net and Go. This means that if you are a developer with experience with one of these languages, you do not have to spend time learning a syntax first. Right away you can put all your focus on learning about AWS resources and how to set them up.\nThe second is that CDK offers a very nice balance between helping you deploy resources with little effort and giving you the flexibility where you need it. It has a bunch of higher level constructs with reasonable defaults that you can easily use to quickly get started. However you can override the defaults on these constructs, or use lower level constructs in the same framework to create whatever you need. This means you are always in control.\nWhat do you need?\nThe first thing you need is to decide on the language you are going to use with CDK. In this case I chose to use Typescript.\nApart from picking a language, in order to get started with AWS CDK you need:\n\nAn AWS account\nTo install and bootstrap the AWS CDK\nA code editor, I tend to use Visual Studio Code for this\n\nThe basic setup\nWe will start building our web-application with a simple basic setup which contains the minimum you would need to see something work. After that is set up, we will expand upon this basic setup until we have something that we could put into production.\nOur basic setup will look like this:\n\nThe static web resources of our application frontend such as the HTML, CSS and Javascript will be hosted in an S3 bucket. We will use DynamoDB to store the persistent data of our application and Lambda and API Gateway to offer an API over HTTPs that allows access to this data.\nThe first thing we will need to do is create a new directory for our application. In my case I am creating a simple Todo list application, so I create a directory called TodoApplication. Move into this directory and execute the command:\ncdk init app --language typescript\nThis will give us the files and folders we need to get started with our application.\nIn order to keep the length of this blog somewhat limited I am only showing the CDK code in this blog, with a few exceptions here and there. If you want to see the full solution however it is available on GitHub. For the basic setup, you have to checkout the basic_setup\u00a0tag.\nThe frontend\nThe next thing we will do is set up the frontend. We create a directory called application and another directory called frontend in that. In this directory we will put the code for the frontend application. I have used Angular to create my frontend, but feel free to use another frontend framework if you like.\nOne of the files that we created when we initialised our CDK application is libs/todo_application-stack.ts.\u00a0This is where we define all AWS resources our application uses. We do this by defining them in the constructor of the TodoApplicationStack class that was created for us.\u00a0 For our frontend application to be hosted in S3, we add the following:\nconst frontendBucket = new s3.Bucket(this, 'TodoApplicationFrontend', {\r\n  websiteIndexDocument: 'index.html',\r\n  publicReadAccess: true,\r\n  removalPolicy: cdk.RemovalPolicy.DESTROY,\r\n  autoDeleteObjects: true\r\n});\r\n\r\nconst bucketDeployment = new s3deploy.BucketDeployment(this, 'DeployTodoApplicationFrontend', {\r\n  sources: [s3deploy.Source.asset(`application/frontend/dist/todo-application`)],\r\n  destinationBucket: frontendBucket\r\n});\r\nbucketDeployment.node.addDependency(frontendBucket);\nThis code adds two CDK constructs to the CDK stack. The first construct is an S3 bucket that we create. By setting the websiteIndexDocument property we also enable static website hosting for this bucket and provide the name for the index document of the website. The second setting we set on the S3 bucket is that we enable public read access on the bucket. The last two settings dictate what happens to this bucket when the CDK stack is destroyed.\nThe removal policy tells CDK what to do with the bucket itself. The default setting \u201cretain\u201d orphans the S3 bucket, leaving it disconnected from the CDK stack but does not remove the bucket or the files in it. I want the bucket and its contents to be deleted along with the rest of the CDK stack which is why I set it to destroy. In the case of an S3 bucket, this action will fail if there are still any files left in the bucket. To allow these files to be removed along with the bucket, I set autoDeleteObjects to true.\nThe other construct is a deployment of resources to S3. In this case, we deploy everything in\u00a0application/frontend/dist/todo-application\u00a0to the S3 bucket we just created. This directory is where Angular stores the frontend distribution after running npm run build. If you use a different framework to create your frontend you may have to change this path.\nYou might notice that if you paste the above code in the CDK stack, it will not compile. This is because we are missing the @aws-cdk/aws-s3 and @aws-cdk/aws-s3-deployment node modules. We can install those using:\nnpm install @aws-cdk/aws-s3 @aws-cdk/aws-s3-deployment\nWe also need to import them into the stack:\n\nimport * as s3 from '@aws-cdk/aws-s3';\r\nimport * as s3deploy from '@aws-cdk/aws-s3-deployment';\nNow we can deploy this stack to AWS using:\ncdk deploy\nWhen this is done, you will have a publicly accessible S3 bucket that hosts your frontend.\nDatabase\nNow that we have our frontend, we shall set up the DynamoDB database we use to store our data.\nFirst we need to add the\u00a0@aws-cdk/aws-dynamodb\u00a0node module to our project and import it just like we did earlier. Then secondly we add the following to the TodoApplicationStack:\n\n\nconst todoItemsTable = new dynamodb.Table(this, 'TodoApplicationTodoItemsTable', {\r\n  partitionKey: {\r\n    name: 'who',\r\n    type: dynamodb.AttributeType.STRING,\r\n  },\r\n  sortKey: {\r\n    name: 'creationDate',\r\n    type: dynamodb.AttributeType.STRING\r\n  },\r\n  removalPolicy: cdk.RemovalPolicy.DESTROY\r\n});\n\nThis code adds a new construct to our CDK stack that creates a DynamoDB table. The table will have a partition key of type String named \u201cwho\u201d. It\u2019s value will contain the username of the user the Todo item was created by. The table will also have an additional sort key, also of type String, that contains the creation date of the Todo item. This allows us to retrieve the Todo items from the table ordered by their creation date. The last property that I added to my table construct is again an override of the removal policy. By default the removal policy for a DynamoDB table is \u201cretain\u201d. Like with the S3 bucket I want the table to be removed when I delete the CDK stack, so I set it to destroy.\nAPI\nIt is a good thing to have our database table set up, however at the moment the frontend has no way to access it. To solve this, we need to set up our API. Setting up the API that allows the frontend to reach the database is the largest part of our basic setup. To keep things clear and orderly, we will do this in a couple of steps.\nShared code layer\nWe start with a Lambda layer that houses shared code that can be used by multiple Lambda functions. In my Todo list application I have a TodoItem class that I want to reuse. Another reason why I want to have this shared code layer is that I use NodeJS for my Lambda functions. This means my functions use node modules, but I do not want to include those in every Lambda function when I deploy them to AWS. Instead I put all the node modules in this shared code layer. This way I keep my functions nice and clean with only function specific code.\nTo create this shared code layer, we start by creating a directory called\u00a0functions in the\u00a0application directory we created earlier. Here we keep the code for our Lambda functions. Within that directory we then create another directory called shared-code and finally a directory called nodejs in that. In this directory we place a package.json for the node modules as well as the TodoItem.ts and the tscompile.json and tsconfig.json files to compile it.\nTo use Lambda for the shared code layer in the TodoApplicationStack, we need to import it just as we did earlier on with S3 and DynamoDB:\nimport * as lambda from '@aws-cdk/aws-lambda';\n\nNext, to deploy our shared code layer we add the following code to the stack:\n\nconst sharedCodeLayer = new lambda.LayerVersion(this, 'TodoApplicationSharedCode', {\r\n\u00a0 code: lambda.Code.fromAsset('application/functions/shared-code'),\r\n\u00a0 compatibleRuntimes: [lambda.Runtime.NODEJS_14_X]\r\n});\n\nThis adds a LayerVersion construct for us. The settings we give it are not all the special. We simply point to the location of the code to include and specify that that we want to use the NodeJS 14.X runtime.\nLambda functions\nTime to make a Lambda function that uses the shared code layer we created just now. In this case I have two Lambda functions, one to retrieve Todo items from the DynamoDB table and one to create items in the table. Within the existing directory application/functions\u00a0we create the directories\u00a0get-items and\u00a0add-item. The code for our Lambda functions goes into these directories.\nMost of the code in our Lambda function is like in any other Lambda function. I will therefore not be going into the code of the Lambda functions here, if you want you can check them out here. However there is one thing I do want to point out. Because we use a Lambda layer to house some shared code, any class that is in this layer must be imported as follows:\n\nimport { TodoItem } from '/opt/nodejs/todoItem';\n\n\nThis in turn will make the compiler complain when you compile the Typescript on your local machine, because this path cannot be found in the local repository. To counteract this, add the following to the tsconfig.json of the Lambda functions:\n\n\"paths\": {\r\n  \"/opt/nodejs/*\": [\"../shared-code/nodejs/*\"]\r\n}\n\nNow we can add the Lambda function to the TodoApplicationStack with the following code:\n\n\n\nconst getItemsLambda = new lambda.Function(this, 'TodoApplicationGetItemsFunction', {\r\n  runtime: lambda.Runtime.NODEJS_14_X,\r\n  handler: 'index.handler',\r\n  code: lambda.Code.fromAsset('application/functions/get-items', {exclude: [\"node_modules\", \"*.json\"]}),\r\n  environment: {\r\n    TODO_ITEMS_TABLE_NAME:todoItemsTable.tableName,\r\n    ALLOWED_ORIGINS:'*'\r\n  },\r\n  layers: [\r\n    sharedCodeLayer\r\n  ]\r\n})\r\ntodoItemsTable.grantReadData(getItemsLambda)\n\n\n\nWith this code we add another CDK construct, this time for a Lambda function. In this case the function shown here is get-items. Apart from some names and paths the add-item function is exactly the same.\nIn settings we set the runtime for the function to NodeJS 14.X and set the handler for the function to the exported handler function of Index.ts. We also direct CDK to the location of the code for this function. From this path, we exclude everything under node_modules and everything that has the extension json. The node modules we exclude because they are in our shared code layer. The JSON files in our function we exclude because we only use them at compile time before pushing the code to AWS. For these two functions I also included two environment variables that can be accessed within the function. This keeps me from having to hard code certain values in the function code itself. Finally I pointed this Lambda function to the shared code layer so that it can access the code there.\nThe last line of this code instructs the DynamoDB table construct to allow read access to the Lambda function. This creates the correct IAM access rights for the Lambda function.\nAPI Gateway\nThe last component needed for the basic setup is an API Gateway that exposes our lambda Functions to the frontend. To set up this API Gateway we add the @aws-cdk/aws-apigateway\u00a0node module to our project and import it into the TodoApplicationStack. Next we add the following code to the stack:\n\nconst apiGateway = new apigateway.RestApi(this, 'TodoApplicationApiGateway', {\r\n  restApiName: 'TodoApplicationApi'\r\n})\r\n\r\nconst itemResource = apiGateway.root.addResource('item')\r\nitemResource.addCorsPreflight({\r\n  allowOrigins: [ '*' ],\r\n  allowMethods: [ 'GET', 'PUT' ]\r\n});\r\nitemResource.addMethod('PUT', new apigateway.LambdaIntegration(addItemLambda), {})\r\nitemResource.addMethod('GET', new apigateway.LambdaIntegration(getItemsLambda), {})\nThe first thing this code does is add the CDK RestApi construct for the API Gateway. Other than setting the name, we currently do not need to change any settings on this construct from their default setting.\nWith the RestApi construct in place, we add a resource to the root endpoint of this API called item. To this resource we then add the CORS preflight OPTIONS method. We set this resource to allow all origins since we want to create a public API. We also set it to allow the GET and PUT methods as those are the ones our Lambda functions will listen to.\nFinally we add the GET and PUT methods to the item resource by creating two new LambdaIntegration instances. Into their constructor we pass the two Lambda functions that were created earlier in the stack as the handler functions for these methods.\nConfig JS\nWith the API Gateway in place we have all the components that we require for the basic setup. We have one problem left to solve however before the basic setup is fully functional. Because we do not yet use a custom domain for the API we created, the url to invoke this API is one in the amazonaws.com domain and generated when we deploy the API Gateway. This means we cannot simply hard code this url into the code we deploy for the frontend.\nI solve this by creating a config.js file and uploading this to the frontend S3 bucket at the end of the deployment. At this stage of the deployment the API Gateway construct is in place. This means I can use the property url on the construct to get access to the url and write that into config.js. As my frontend is created with Angular, it can read out this config.js file to access the url to the API Gateway.\nTo create this custom resource and upload it, I need to add the following import to the TodoApplicationStack:\n\nimport * as customResources from '@aws-cdk/custom-resources';\nThen I add the following code to the stack:\n\nconst frontendConfig = {\r\n  itemsApi: apiGateway.url,\r\n  lastChanged: newDate().toUTCString()\r\n};\r\n\r\nconstdataString = `window.AWSConfig = ${JSON.stringify(frontendConfig, null, 4)};`;\r\n\r\nconst putUpdate = {\r\n  service: 'S3',\r\n  action: 'putObject',\r\n  parameters: {\r\n    Body: dataString,\r\n    Bucket: `${frontendBucket.bucketName}`,\r\n    Key: 'config.js',\r\n  },\r\n  physicalResourceId: customResources.PhysicalResourceId.of(`${frontendBucket.bucketName}`)\r\n};\r\nconst s3Upload = new customResources.AwsCustomResource(this, 'TodoApplicationSetConfigJS', {\r\n  policy: customResources.AwsCustomResourcePolicy.fromSdkCalls({ resources: customResources.AwsCustomResourcePolicy.ANY_RESOURCE }),\r\n  onUpdate: putUpdate,\r\n  onCreate: putUpdate,\r\n});\r\ns3Upload.node.addDependency(bucketDeployment);\r\ns3Upload.node.addDependency(apiGateway);\nThis code creates an object that contains the API url and a last changed date. It then turns that object into a data string. Lastly it makes an AwsCustomResource instance. We instruct this instance to perform the action putObject on S3 with the specified parameters whenever it is either created or updated. We make CDK generate the correct IAM policy statements based on the SDK calls made by the actions performed. Because we give the AwsCustomResource instance a dependency on the bucketDeployment and apiGateway, we ensure that it is deployed after those were created.\nDeploying\nBefore we deploy everything, make sure to compile the frontend and all Lambda functions. In my project this means I use:\nnpm install\r\nnpm run build\non all of them as they are all NodeJS based. With the code compiled, we use:\ncdk deploy\nto deploy the stack. In the AWS portal you can find the S3 bucket. In its properties under static web hosting you will find the url where your web application is now running.\nMy Todo list application now looks like this:\n\nWe have now completed the basic setup for our serverless web-application with AWS CDK. We have a running frontend to serve to our users. To store the data of our users we have created a DynamoDB database. Finally we created a HTTP API that connects to frontend to the database.\nThis basic setup however is not quite ready for us to use in production. To get the application production ready, we need to address the following:\n\nThe url to the application is somethingsomething.s3-website-region.amazonaws.com. This does not help users easily reach the application.\nThe application does not offer a HTTPS connection for the frontend, so access to the application is not secure.\nThe current application has no user authentication. You can access the data of any user by simply entering the correct username.\n\nWe solve these issues in the second part of this blog.\n\n\n\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 29954, "title": "Moving your iOS app development and release process to the cloud using Azure DevOps", "url": "https://www.luminis.eu/blog/cloud-en/moving-your-ios-app-development-and-release-process-to-the-cloud-using-azure-devops/", "updated_at": "2021-12-16T11:43:41", "body": "Developing your applications in the cloud has many advantages, including cost reduction, scalability, and ease of use for developers. In this blog I will explain the basics of what is needed to move the build and release process of an iOS app to the cloud using Azure DevOps.\nWe can roughly divide this process in the following steps:\n\nMoving your code repository to Azure DevOps;\nSetting up a build pipeline;\nUsing a Service Connection to automatically push app builds to App Store Connect.\n\nI will describe each step in more detail below.\n\u00a0\nNote: this guide assumes you have an Azure DevOps Project for your repository and build pipelines. Microsoft\u2019s own documentation on how to create such a project can be found here. For more information on the advantages of moving to the cloud, read Luminis\u2019\u00a0 whitepaper on the subject via this link.\n\u00a0\nStep 1: Moving your code repository to Azure DevOps\n\u00a0\nOur first step will be to move our app\u2019s source code from any existing repository to an Azure DevOps repository. It is of course also possible to have our Azure DevOps pipeline pull code from an external repository, but that is beyond the scope of this blog.\nIf you are creating a new repository in your Azure DevOps Project, you can simply use the import repo feature during the creation of your repository as described here. If you want to use an existing (empty) Azure DevOps repository for your code, you can manually import your source code from your current repository into the existing Azure DevOps repository using this guide.\nAssuming your source code was successfully pushed to your Azure DevOps repo, we can continue and create a pipeline to actually build our app.\n\u00a0\nStep 2: Setting up a build pipeline\n\u00a0\nInitial pipeline creation\n\u00a0\nA pipeline is created by navigating to Pipelines > Pipelines from the left bar and selecting Create Pipeline. On the Connect tab, we will select \u2018Azure Repos Git\u2019 as our source code location:\n\n\u00a0\nOn the Select tab, we will select the repository that we moved our app\u2019s source code to in Step 1. Then, on the Configure tab we can select \u2018Starter pipeline\u2019. On the \u2018Review\u2019 tab we can see that an azure-pipelines.yml file will be created in our repository. This file contains the instructions that Azure DevOps needs to successfully build and publish our app. On this page we can edit its contents before the file is actually created:\n\nEven though we can change the contents of this file later on, we might as well make some changes for the initial commit.\nRemove the comment header and the script parts of the file and change the vmImage to \u2018macOS-latest\u2019. Your file should now look like this:\ntrigger:\r\n- develop\r\n\r\npool:\r\n\u00a0 vmImage: 'macOS-latest'\r\n\r\nvariables:\r\n\r\n- group: \r\n\r\nsteps:\r\n\nThe trigger key here describes that every commit to the \u2018develop\u2019 branch will trigger an automatic build. Change this to whatever branch you want, or remove it entirely if you do not want to trigger automatic builds. Builds can also be triggered manually from the Pipelines screen. The \u2018macOS-latest\u2019 image is needed here because we are building an iOS app which requires Xcode on our build agent.\nWe will now add tasks under our steps key to further define our build process. Add the following tasks under the steps key:\nsteps:\r\n\u00a0 - task: InstallAppleCertificate@2\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 certSecureFile: ''\r\n\u00a0 \u00a0 \u00a0 certPwd: ''\r\n\u00a0 \u00a0 \u00a0 keychain: 'temp'\r\n\u00a0 \u00a0 \u00a0 deleteCert: true\r\n\r\n\u00a0 - task: InstallAppleProvisioningProfile@1\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 provisioningProfileLocation: 'secureFiles'\r\n\u00a0 \u00a0 \u00a0 provProfileSecureFile: ''\r\n\u00a0 \u00a0 \u00a0 removeProfile: true\r\n\r\n\u00a0 - task: Xcode@5\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 actions: 'build'\r\n\u00a0 \u00a0 \u00a0 scheme: ''\r\n\u00a0 \u00a0 \u00a0 sdk: 'iphoneos'\r\n\u00a0 \u00a0 \u00a0 configuration: 'Release'\r\n\u00a0 \u00a0 \u00a0 xcWorkspacePath: '**/YourProject.xcworkspace'\r\n\u00a0 \u00a0 \u00a0 xcodeVersion: 'default'\r\n\u00a0 \u00a0 \u00a0 packageApp: true\r\n\u00a0 \u00a0 \u00a0 signingOption: 'manual'\r\n\u00a0 \u00a0 \u00a0 signingIdentity: '$(APPLE_CERTIFICATE_SIGNING_IDENTITY)'\r\n\u00a0 \u00a0 \u00a0 provisioningProfileUuid: '$(APPLE_PROV_PROFILE_UUID)'\r\n\r\n\u00a0 - task: CopyFiles@2\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 contents: '**/*.ipa'\r\n\u00a0 \u00a0 \u00a0 targetFolder: '$(build.artifactStagingDirectory)'\r\n      flattenFolders: true\r\n\u00a0 \u00a0 \u00a0 overWrite: true\r\n\r\n\u00a0 - task: PublishBuildArtifacts@1\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 pathtoPublish: '$(build.artifactStagingDirectory)' \r\n\u00a0 \u00a0 \u00a0 artifactName: 'drop' \r\n\u00a0 \u00a0 \u00a0 publishLocation: 'Container'\r\n\nIn order, these tasks do the following: installing an App Store certificate, installing a provisioning profile, running xcodebuild and copying and publishing the created artifacts. As you can see, some values are currently empty. For now, make sure the scheme key under the \u2018Xcode@5\u2019 task contains the scheme name of your app and the xcWorkspacePath key contains the correct path to the .xcworkspace file in your repository. After this, select the \u2018Save\u2019 sub option:\n\nEnter the commit message and select the branch you want to commit the initial pipeline file to. As the pipeline file is not final in its current form, I recommend committing it to a temporary branch and merging it into your main branch later on.\n\u00a0\nConfiguring the certificate and provisioning profile\n\u00a0\nAs you may have noticed, the \u2018InstallAppleCertificate@2\u2019 and \u2018InstallAppleProvisioningProfile@1\u2019 tasks still have some empty keys. These tasks require us to upload a .p12 certificate file and a .mobileprovision file to our secure files. Please refer to this guide on how to obtain these files. Note: the linked guide refers to a Cordova project (phonegap), but the part about certificate file generation applies here as well.\nIf you have added a password to your .p12 file (which is recommended), we will need to store it in our DevOps pipeline. For security reasons, the p12 password will be added to a variable group, which our pipeline file can refer to. To create a variable group, from the sidebar go to Pipelines > Library and select \u2018+ Variable group\u2019. The following screen will look like this:\n\n\u00a0\nAdd the p12 password as a variable and select the lock icon to hide it. This will prevent us from having to store our p12 password in our repository. Name your group and save it.\nThe .p12 and .mobileprovision files can be stored as secure files under the Secure files tab on the same page (Library):\n\nNow, we can go back to our azure-pipelines.yml file and fill in the missing keys. Go to Pipelines > Pipelines using the sidebar, select your pipeline and then press the \u2018Edit\u2019 button to modify the pipeline file.\nFirst of all, we can change the variable group to refer to our newly created group:\nvariables:\r\n- group: luminis-blog-group\r\n\nSecondly, we can now fill in the blank keys in the \u2018InstallAppleCertificate@2\u2019 and \u2018InstallAppleProvisioningProfile@1\u2019 tasks:\nsteps:\r\n\u00a0 - task: InstallAppleCertificate@2\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 certSecureFile: 'certificate.p12'\r\n\u00a0 \u00a0 \u00a0 certPwd: '$(p12Password)'\r\n\u00a0 \u00a0 \u00a0 keychain: 'temp'\r\n\u00a0 \u00a0 \u00a0 deleteCert: true\r\n\r\n\u00a0 - task: InstallAppleProvisioningProfile@1\r\n\u00a0 \u00a0 inputs:\r\n\u00a0 \u00a0 \u00a0 provisioningProfileLocation: 'secureFiles'\r\n\u00a0 \u00a0 \u00a0 provProfileSecureFile: 'myApp.mobileprovision'\r\n\u00a0 \u00a0 \u00a0 removeProfile: true\r\n\n\u00a0\nWith these final blanks filled, our template can be tested! Save the file and select \u2018Run\u2019. If all goes well, you can download your artifact on the result summary page after your build completes:\n\n\u00a0\nNow that we have a working pipeline, there is one remaining step: pushing the artifact automatically to App Store Connect, so that it can be released to testers or pushed to the App Store.\n\u00a0\nStep 3: Using a Service Connection to automatically push app builds to App Store Connect\n\u00a0\nEven though we have now fully automated building our artifact, we still have to manually upload our artifact to App Store Connect. We can automate this process as well by expanding our pipeline configuration to push our artifact to Apple using a so-called Service Connection. We will however first need to create this Service Connection before we can use it in our pipeline.\n\u00a0\nCreating a Service Connection\n\u00a0\nTo create a service connection, we require an API token to authenticate ourselves with the App Store Connect API each time we want to push a build automatically. Details on how to generate such a token can be found here. Make sure the token is generated with the developer role.\nWe also need to install the \u2018Apple App Store\u2019 pipeline extension, located here.\nWith the extension installed, we can proceed to actually create the Service Connection. In your Azure DevOps Project, click on the \u2018Project settings\u2019 button located on the bottom of the sidebar. On this page, from the second sidebar, select Service Connections > Create service connection. From the list, select \u2018Apple App Store\u2019 and then fill in the details required. Save the connection.\n\n\u00a0\nUsing the Service Connection in our pipeline file\n\u00a0\nNow that we have configured a Service Connection, we can use it in our azure-pipelines.yml file. This can be done by adding the \u2018AppStoreRelease@1\u2019 task to the bottom of the file, below the \u2018PublishBuildArtifacts@1\u2019 task:\n\u00a0- task: PublishBuildArtifacts@1\r\n \u00a0 inputs:\r\n \u00a0 \u00a0 pathtoPublish: '$(build.artifactStagingDirectory)/output/iphoneos/Release' \r\n \u00a0 \u00a0 artifactName: 'drop' \r\n \u00a0 \u00a0 publishLocation: 'Container'\r\n\r\n - task: AppStoreRelease@1\r\n \u00a0 inputs:\r\n \u00a0 \u00a0 serviceEndpoint: 'Luminis Blog Service Connection'\r\n \u00a0 \u00a0 appIdentifier: eu.luminis.blog\r\n \u00a0 \u00a0 ipaPath: '$(build.artifactstagingdirectory)/**/Luminis Blog.ipa'\r\n \u00a0 \u00a0 shouldSkipWaitingForProcessing: true\r\n \u00a0 \u00a0 shouldSkipSubmission: true\r\n\nSave the file and run the pipeline. Provided our Service Connection is working, our pipeline will now push the IPA artifact to App Store Connect at the end of the build, where it can be rolled out to testers or released to the App Store.\nCongratulations! You have successfully moved your iOS app build process to the cloud.\n\u00a0\nFinal words\nAs developing software in the cloud is the new way of working, we need to be able to move our development processes to cloud environments. In this post I have explained the basics of getting an iOS app built on an Azure DevOps environment. If you have any questions or comments regarding this guide, don\u2019t hesitate to contact me at ejnar.kaekebeke@luminis.eu.\n", "tags": [], "categories": ["Blog", "Cloud", "Development", "No category"]}
{"post_id": 29843, "title": "Easily configure Azure API Management with bicep and az-cli", "url": "https://www.luminis.eu/blog/cloud-en/easily-configure-azure-api-management-with-bicep-and-az-cli/", "updated_at": "2021-12-16T12:15:45", "body": "A while ago Tom Meulenberg wrote a blogpost about API Management and the challenges he faced. With this blogpost I will help him make things easier next time by splitting managing the configuration of API Management from defining the api\u2019s.\nOne of his observations is the verbosity of the arm-template. That is mainly caused by all the operations he added in the template. Using a new (but still under development) az-cli subcommand (apim) these operations can be removed from the template, making the template a lot cleaner. This requires a clean separation between your api definition (all endpoints) and your api configuration allowing you to update your api-spec (using open-api yaml) without adjusting the configuration (ie. policies).\nAPI Management concepts\nIn the first version of this blog I took knowledge about API Management for granted and left my reviewers puzzled. Let\u2019s take one step back and explain some concepts before diving into details.\nOperations, APIs, Products and subscriptions\n\nOperations define actions on a resource (like HTTP POST /user to create a user)\nAPIs group operations that belong together\nProduct can be linked to one or more API\u2019s\nSubscription (the right to consume an API) are applied to a product\n\nAPI Management concepts\nPolicies\nAs Microsoft states here, policies are a collection of statement that change the API behavior through configuration. Popular statements include:\n\nFormat conversion from XML to JSON\nRate limiting to restrict the number of incoming calls\nAdd an http-header (like an API key) on the request to the backend\nForward authentication data (like bearer tokens) to the backend\nRemove sensitive information from the response\n\nPolicies can be configured globally (on all APIs) or at the scope of a product, api or operation. The superset of policies are applied to sequentially to a request or response. API Management allows for deterministic ordering of the combined policy statements via the base\u00a0element.\n<policies>\r\n  <inbound>\r\n    <rate-limit-by-key \r\n             calls=\"150\" \r\n             renewal-period=\"60\" \r\n             counter-key=\"@(context.Subscription?.Key ?? \"anonymous\")\" />\r\n    <base />\r\n    <set-header name=\"X-Forwarded-Uri\" exists-action=\"override\">\r\n      <value>@(context.Request.OriginalUrl.ToString())</value>\r\n    </set-header>\r\n  </inbound>\r\n  <backend>\r\n    <base />\r\n  </backend>\r\n  <outbound>\r\n    <base />\r\n  </outbound>\r\n  <on-error>\r\n    <base />\r\n  </on-error>\r\n</policies>\r\n\nWhen you do not require any policy at operation-level, you are ready to configure your API by importing OpenApi-specs.\nLet\u2019s take a look at the code\nFor readability and of course to learn something new, I switched to bicep for defining my deployment. I\u2019ll skip the obvious parts like creating the Api Management-instance and configuring logging, but you can find those in the complete bicep-file linked at the end of this post. I will configure API Management with the commonly used Petshop sample that can be found at github.\nDefine the API\nIn the code below, I define the Petshop-api and it\u2019s policies. These policies are defined as xml and will generate a simple default inheriting policy with some comments on top.\n\nThis will result in the following api definition in API Management.\nAzure API Management Petshop API\nDefine the products\nNext I define the products. In the sample I define a basic product with rate-limiting and an advanced product without limitations. Both products are linked to the petshop api.\n\nThis will create the following policies on the basic product.\nAzure API Management Basic Product Policies\nDeploy the template\nThat\u2019s all that\u2019s required for configuring api management. To deploy this template, run the following command:\naz deployment group create --name demoapim --resource-group <rgname> --template apimanagement.deploy.bicep\nImport the api definition\nWith this basic configuration, it\u2019s now time to import the operations. As mentioned, I am using the `openapi.yaml` downloaded from github.\naz apim api import --resource-group <rgname> --service-name thijdemoapis --api-id petshop --path petshop --display-name 'Petshop API' --specification-format OpenApi --specification-path openapi.yml\nThe parameter api-id should be the same as the name defined here in the bicep file: Microsoft.ApiManagement/service/apis@2020-12-01. Otherwise you will add a new api to API Management and miss the configuration you made.\nThe path and display-name parameters overwrite the settings configured in the api. So make sure they are the same as in the bicep file.\nWhen running the command, you\u2019ll see the disclaimer \u2018Command group \u2018apim\u2019 is experimental and under development.\u2019 After a few seconds, the operations and definitions are available in API Management.\nAzure API Management List of operations\nAutomating deployment in your release pipeline\nOne final consideration: when you decorate the functions in your function-app with OpenApi-attributes, you can generate the OpenApi-spec after the release of your function app and import it into API Management using the az apim command.\nConclusion\nAlthough the az apim command is still under development, I would certainly choose it to configure api\u2019s in API Management. It forces you to think about your policies by removing the possibility to create them at operation level, making it easier to maintain it. And for bicep? Well, I am loving it! The 0.4-release I used is production ready and intellisense support in VSCode helps creating correct templates quickly.\nLinks\nWhat is bicep\naz apim api import\nThe complete bicep-file\nPetshop OpenApi spec\n", "tags": ["Api Management", "az cli", "Azure", "bicep"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 29680, "title": "Bluetooth Low Energy logging by placing a Mac-in-the-middle", "url": "https://www.luminis.eu/blog/bluetooth-low-energy-logging-by-placing-a-mac-in-the-middle/", "updated_at": "2021-09-09T15:53:52", "body": "In the past two years or so I have been delving into Bluetooth Low Energy (BLE) for a project I did for one of our customers at Luminis Arnhem. In this project we have been tasked with implementing mobile applications for Android and iOS that used BLE to communicate with various products made by our customer. Because of this, a lot of my focus has gone to the BLE stacks that Apple and Google have created on their platforms for application developers to use.\nThe need for logging\nThese stacks take care of the lower levels of the BLE protocol for developers and in very many ways this is a good thing. I don\u2019t know if it is even possible, without first having to hack your own device, to completely create your own BLE stack for these platforms and use that on iOS and Android devices instead but I certainly would not recommend you to try it. One issue with using these stacks however, is that they make it hard as a developer to know what is actually being communicated at the protocol level. The API\u2019s that are offered are pretty high level so that they are easy to use, which is good. However the documentation provided with the API\u2019s is not very descriptive or transparant about how calling those API\u2019s translates to actual communication in the BLE protocol. So when your connection does not act as you expected it would and chances are that it will, what you really want to do is take a look at the lower level packets that are actually being transferred.\nCool, how do I do this?\nDepending on the platform that you are developing, for there may already be options available that you can use to get access to such logging without having to put in a lot of effort. For iOS devices with an OS version higher than 13 for example, Apple already offers a solution using PacketLogger that is fairly easy to set up. I have not actually tried this solution myself yet, but as the solution I am about to propose builds on some of the same concepts I can say with some confidence that this will work. In fact if iOS is the only target you are focussed on I would probably advise you to just use this instead and forget about my solution (Keep reading though! It never hurts to learn).\nIf you are also targeting Android, the challenge becomes a lot bigger. If you google logging ble on android\u00a0you will find plenty of blogs that explain how to enable\u00a0Bluetooth HCI snoop logging to achieve this, however I have a few problems with this. The main issue that I have with this solution is that it simply logs everything your device does over Bluetooth to a file and leaves you with the task of figuring out where that one bit of communication you were actually interested in went. There is no option to view the logs during the connection and any filtering that you may want to do on the traffic completely relies on you to find a tool or way to do that. Secondary, getting this logging to work on your device can be quite difficult as the steps required also tend to differ based on your device and Android version.\nAnother option you could consider is to get what is called a packet sniffer, one that supports BLE. Packet sniffers are devices that allow you to listen in on traffic. Usually you can plug them into your computer and they come with software that allow you to view the traffic and filter it. The problem with this option is that there are actually two types of such devices for BLE, singleband and multiband sniffers. You can buy a singleband packet sniffer for around 50 euros which is affordable enough, but these do not really work well for our purposes. A connection between two devices over Bluetooth Low Energy tends to jump between bands multiple times during the connection. As singleband sniffers by their design are only able to listen in on one band at a time you will find it difficult if not impossible to capture a complete connection. Multiband packet sniffers on the other are able to listen in on multiple bands at the same time and should therefore not have these difficulties. Multiband packet sniffers however are a lot harder to come by and when you do find them they can easily cost ten times as much as a singleband sniffer.\nWhat\u2019s Mac got to do with it?\nSo the Mac then? Earlier when talking about logging BLE traffic for iOS I already mentioned PacketLogger. PacketLogger is a tool that comes with the\u00a0Additional Tools for XCode. It allows you to view a live log of all the Bluetooth traffic going to and from your Macbook. Various BLE protocol layers and message types are coloured differently to easily detect certain messages and you can also filter the traffic in a few ways and save log files to view them later on. It is a great tool and I have used it a lot to analyse problems in BLE connections. But PacketLogger can only log traffic to and from your Mac, how will this help us log traffic between our application and another device? Well XCode allows you to create applications that run on your Mac and by using CoreBluetooth, the BLE stack that Apple has created for iOS and MacOS applications, it is not that hard to create a small application that acts as a proxy between the peripheral and central in your BLE connection. Which is exactly what I did and you can use it too!\nHow do I use it?\nThe first thing you need is some basic information about the peripheral you want to use this on. The information you need to know is:\n\nThe name the peripheral advertises with.\nThe password for the peripheral, if it uses a secure BLE connection.\nThe Services offered by the peripheral, or at least the ones that you wish to use in your app.\nThe Characteristics on the Services offered by the peripheral. Once again you would at least need to know about the ones you wish to use in your app.\n\nIf you plan on developing an application that communicates with your peripheral then you will probably have all of this information already anyway.\nThe second step to take it to check out the\u00a0BLEProxy Git repo. Within the repository you will find a project that you can open using XCode. When you do so, open up the BLEConstants.swift file. In this file are a few constants that you need to fill:\n\nDEVICE_NAME: The name the peripheral advertises with.\nSERVICES_AND_CHARACTERISTICS: UUID\u2019s of the services and characteristics on the peripheral.\n\nAfter filling these values with the correct values, you can start the project using XCode to run it on your local Mac.\nWant to know how to build your own version of this application? Check out the README.\n", "tags": [], "categories": ["Blog", "Internet of Things"]}
{"post_id": 29342, "title": "Deploying Spring Boot applications to AWS App Runner with AWS CodePipeline", "url": "https://www.luminis.eu/blog/cloud-en/deploying-a-spring-native-application-to-aws-app-runner/", "updated_at": "2022-06-24T09:23:49", "body": "In a previous post, we looked at AWS App Runner. AWS App Runner is a container service that lets you focus on your application and allows you to deploy your application in minutes without setting up any infrastructure.\nAWS App Runner supports two source options for your App Runner service:\n\nBy pointing to a Git(Hub) repository that contains your application source code\nFrom an existing container image stored in ECR (public or private is both possible)\n\nThe code-based source option only supports two languages at the moment: Node.js and Python. Your source also has to be stored on Github, but I expect AWS to add CodeCommit as a valid option pretty soon though. Now in my case, I would like to deploy a Java / Spring Boot based application to AWS App Runner, so my best bet, for now, is to use a container image based deployment. Now we could go for a pre-build image, but what\u2019s the fun in that right? So in this post, we will take a look at a setup that you can leverage to deploy a Spring Boot based application to AWS App Runner.\nThe overall setup\nOur source code will be hosted in Github and we are leveraging CodePipeline + CodeBuild to build and test our Java application. If the build and test are successful we push the resulting container image to a private container registry in ECR. App Runner can then pick up the container image for deployment.\n\nNow that we have a clear overall idea of what we need, let\u2019s create the build and deployment stack with AWS CDK.\nSetting up the basics\nThe first thing we will need is a private container image repository in ECR. This will allow us to store our container image and can be used later by App Runner to get our application as a container.\n\nWith the image repository in place, we can continue with the next step, creating a CI pipeline for our project.\n\nThe build pipeline makes sure that any code change will result in an update of our service. As you might have seen in the code snippet above we only build from the master branch, so any PR request, merge or commit to master will trigger a new build and will result in a new container image. In the App Runner service, which we will define later on, we can choose if we want App Runner to automatically deploy a new version of our application once it\u2019s available. To instruct CodeBuild to build our maven based project and create a docker image out of that we can do so with a custom buildspec.yml file.\n\nAs you can see we\u2019ve split the build into 3 separate phases:\n\nWe log into ECR and we create a hash for the image tag that we will create later on.\nWe build the maven project and leverage the Spring Native maven plugin to create an a native image. For tagging with the correct container registry, we inject the repository location as an environment variable into our build process. As a final step, we tag the created docker image.\nWe push the docker image to our ECR repository.\n\nTo make sure the App Runner service can fetch the docker image from our ECR repository, we will need to create and assign a role that has the permissions to do so.\n\nCreating the App Runner service\nNow for the last and final part of our setup, we will need to create the AWS App Runner service via CDK.\n\nIf we look at the above code snippet we\u2019ve setup five configuration options for App Runner:\n\nThe health check endpoint.\nThe role required to access the image in ECR\nThe image source and the Port that our container will listen on\nThe service name\nThe type of resources (memory/cpu) required to run our service\n\nNow we have all the code we need for CDK to create the entire stack and create the service for us. It\u2019s just a matter of running cdk deploy and you will have the entire stack up and running.\n$ cdk deploy\nIf we want to know the URL of our new service, we can leverage a CDK `CnfOutput` construct in which you can request the URL of the service and it will be printed out once the stack is finished deploying.\n\nWhen CDK is done with the deployment you will be able to find the URL to your service in the output of the CDK deploy.\n\r\nDo you wish to deploy these changes (y/n)? y\r\napprunner-runtime-stack: deploying...\r\napprunner-runtime-stack: creating CloudFormation changeset...\r\n\r\n\u2705 apprunner-runtime-stack\r\n\r\nOutputs:\r\napprunner-runtime-stack.serviceUrl = https://someid.eu-west-1.awsapprunner.com\r\n\n\u00a0\nSummary\nEven if you\u2019re not using a language supported by AWS App Runner, it\u2019s still pretty straightforward to deploy your service to AWS App Runner. You can simply use your existing pipeline or create a new build pipeline in AWS CodePipeline that will result in an image in ECR, from which App Runner can do the rest. For this service, I\u2019ve chosen to use Spring Native. Spring Native will create a native image for your Spring Boot application, which results in a much faster application startup. In my case, for a simple application, the time it takes for the application to start is about 500ms instead of 3 seconds (non-native image). When you expect your application to retrieve traffic spikes that might trigger app runner to scale out , this improvement can help for sure.\nLearn from our experts:30 Jan 2021-Bj\u00f6rn LammersTraining: Spring CloudTo be able to fully take advantage of the benefits of the Cloud, the Cloud environment needs to be taken into consideration while applications are developed. Applications should be designed to be cloud Native. Description This workshop will explain the...\n", "tags": ["aws", "cdk", "cloud", "containers", "java"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 28220, "title": "A first impression of AWS App Runner", "url": "https://www.luminis.eu/blog/cloud-en/a-first-impression-of-aws-app-runner/", "updated_at": "2022-06-24T09:25:14", "body": "About three months ago AWS released a new service named\u00a0AWS App Runner. After reading the introduction blog post, I got pretty excited to check it out. AWS App Runner is a new service that provides organizations with a fast, simple, and secure way to deploy containerized applications on the AWS platform without managing any infrastructure. AWS already offers a wide range of container based services like AWS Fargate, ECS, Elastic BeanStalk, and AWS EKS, so why did they come up with App Runner?\nWhat makes App Runner different from the other services?\nLet\u2019s see how AWS describes App Runner.\nAWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner automatically builds and deploys the web application and load balances traffic with encryption. App Runner also scales up or down automatically to meet your traffic needs. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications.\nTo me the key selling point for App Runner is the fact that it\u2019s very easy to use and a secure and\u00a0fully managed service. It\u2019s an opinionated architecture that makes it really easy to run containerized applications in AWS. App Runner offers another level of abstraction for the complex ecosystem of container runtime and orchestration options. Compared to the existing AWS container services there is almost no learning curve as you only need to point to a GitHub repo or a container image in ECR and provide\u00a0some configuration settings for security, a default number of instances and required resources for a single instance, and it will create a secure, fully load-balanced and autoscaling service.\nScreenshot of the AWS Console showing the different source and deployment options for an App Runner service.\nScreenshot of the AWS Console showing the different service settings for an App Runner service.\nWhat\u2019s hiding under the hood of AWS App Runner?\nApp Runner is built upon a wide range of different AWS services:\n\nAWS CodeBuild \u2013 For building, testing, and packaging the application. Only used if you choose to build from source. Supported languages are Node.js and Python.\nAWS Fargate & AWS ECS \u2013 Used for the underlying managed container orchestration platform.\nAWS Auto Scaling \u2013 Makes sure that the application scales based on the number of concurrent requests.\nAWS Elastic Load Balancing \u2013 Makes sure that the load is evenly distributed amongst the different instances of the service.\nAmazon CloudWatch \u2013 Used for storing App Runner Logs (events, deployments, and application) and metrics.\nAWS Certificate Manager \u2013 Used to provide out-of-the-box SSL/TLS certificates for the service endpoint.\nAWS KMS \u2013 Used to encrypt copies of the source repository and service logs.\n\nNext to that, you can also configure/register your own (sub-) domain for the service, which presumably is using Route53 (could not find that anywhere).\nAs you can see that\u2019s a whole bunch of services, configuration, provisioning, and management you don\u2019t have to think about. AWS App Runner hides/abstracts this from you and lets you focus on your application and business problems. Pretty neat, huh?\nSo what\u2019s missing?\nIt\u2019s a new service so some features are not (yet) available which you might find in similar services. While experimenting with App Runner I noticed a couple of them, so let\u2019s take a look.\n\nNo native support for Parameter Store or Secrets Manager when configuring an App Runner service. You can still use those services via the SDK within your application, but you can\u2019t use them for instance to configure some environment variables for your container.\nAs far as I could see App Runner does not (yet) allow you to work with resources inside a private VPC (for instance an RDS instance). UPDATE: App runner now supports resources inside your private VPC (https://aws.amazon.com/blogs/containers/deep-dive-on-aws-app-runner-vpc-networking/)\nIf you\u2019re a fan of AWS CDK, you will have to keep in mind that CDK only offers L1 constructs for creating an App Runner service for now. No L2 support yet, but it seems that\u2019s on the roadmap. Update: As of CDK version 1.126.0, CDK also has L2 support for App Runner.\nApp Runner only supports the blue/green deployment model, so other options like rolling, canary or traffic split are not an option right now.\nNo other container repositories besides ECR are supported.\nNo other git repositories besides GitHub are currently supported. I expect them to add CodeCommit pretty soon.\nAs with most new services App Runner is currently only available in a few regions: Europe (Ireland),\u00a0Asia Pacific (Tokyo),\u00a0US East (N. Virginia),\u00a0US East (Ohio),\u00a0US West (Oregon).\nThere is no support yet for languages like Java, Go, Rust, Ruby, etc. So if one of those is your favorite programming language you will have to create a container image before you can launch the service in App Runner.\nApp Runner does not allow you to scale to zero instances. You will always have a single instance running and will be charged for the allocated resources.\n\nPricing\nWhen you look at the pricing mentioned on the AWS App Runner page you might think it\u2019s pretty cheap with $ 5 dollar a month, but there is more to it. They mention that it costs about $ 5 a month, but while reading the small letters it says that that\u2019s the case for an app running a single instance that is paused for about 22 hours a day. With App Runner, you are charged for the compute and memory resources used by your application. From what I noticed while running a service is that from a CPU perspective you seem to be only charged for the actual CPU resources spent. If your application is not being used, you are not consuming CPU and therefore are not billed for CPU usage. Memory on the other hand will stay reserved for your application and you will be billed accordingly. You will also be charged for additional App Runner features like building from source code or automating your deployments. Pricing therefore might not be straightforward at first, but also not too complicated to figure out. Before you start migrating your apps to App Runner be sure to try and estimate your bill as it might not be worthwhile to migrate from a micro EC2 instance to App Runner.\n\nFinal thoughts\nThe first time I read the announcement about App Runner it made me think about Google Cloud Run. I see App Runner as the AWS response to Google Cloud Run. It has a strong opinionated architecture and built on top of other great AWS services. The ease of use is really great and without having a lot of experience with containers you can get started really quickly. I\u2019ve tested App Runner with a bunch of Spring Boot applications and it was really easy to get an application up and running within a couple of minutes. I think App Runner can be very useful for creating small applications with use cases such as REST APIs or web applications. I think it\u2019s great for rapid prototyping and deploying PoCs and MVPs.\nThere is a public roadmap that starts to take shape with some great features. Popular tools used by the AWS community are actively adding support for App Runner.\u00a0I\u2019m looking forward to seeing where App Runner will be in a year from now.\nAdditional resources\n\nAWS App Runner Workshop \u2013 Great if you want to get started with some hands-on exercises.\nAWS App Runner Deep Dive (video) \u2013 Really nice overview of the services and gives some good insights into how features like auto-scaling works.\n\nFeel free to leave a comment or tweet at @jreijn or @luminis_eu!\nLearn from our experts:8 Feb 2021-Mark van KesselTraining: AWS SecurityCloud computing offers unrivaled possibilities with its vast array of services and features allowing an unprecedented speed of development. But using it in the wrong way can lead to serious risks. Hackers, data-leaks, DDoS attacks, bad actors, the internet is...\n8 Feb 2021-Mark van KesselTraining: AWS MonitoringWith the rise of Devops and the \u201cyou build it, you run it\u201d mentality, it is more important than ever for teams to have a thorough understanding of how their applications are running and performing. With the ever-increasing complexity of...\n", "tags": ["aws", "cloud", "containers"], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 24847, "title": "Flash around your AWS badge of honor.", "url": "https://www.luminis.eu/blog/flash-around-your-aws-badge-of-honor/", "updated_at": "2021-11-11T16:58:46", "body": "\n\u00a0\nThis blog will go to detail into:\n\nWhy one should or should not certify.\nMy experience with AWS.\nPractical tips to study for the exam.\nThe result I achieved.\n\n\u00a0\nWhy certify?\nThe title might sound like I am immediately going to advocate getting certified on AWS but the first question to ask is always why?\nWhy would someone need a certification/badge? How does a standardized test make you as a developer more knowledgeable?\n\nThe short answer is that you probably don\u2019t need to get certified in order to get the knowledge that you need to do your job right.\nTo me, the main benefit is that someone else (e.g. employer) can quickly validate that you have a certain level of knowledge required to handle their systems in AWS. Other than that, it\u2019s a nice validation to yourself that you have the skills required to work with AWS.\nThis does not mean that it\u2019s entirely useless to pursue a certification, it is the exact opposite. Regardless of the current amount of knowledge that you currently possess, the journey of getting AWS certified will give you a lot more information and some experience. By having to answer most of the questions correctly you will learn about topics you might not have touched before your journey of getting certified. I experienced it as a moment of \u201cUltra Learning\u201d where I learn so much that I can apply it in my daily work immediately after.\n\u00a0\nMy journey into AWS\nIn October 2019 I started working at Luminis as my first full-time job. The project I started with required the use of AWS IoT. This is a specialized service for building internet of things services.\nAs a front-end engineer, I was tasked to build an Angular application directly on top of the HTTP API that Amazon Web Services provides. In order to access the AWS services, we needed temporary credentials, which let met to using Cognito through AWS Amplify. To host the website we used CloudFront as a CDN and S3 as the storage.\nThis project got me interested in AWS and to learn more about it in a theoretical manner and it was also a huge help in studying for the first exam. I chose to take the developer exam before doing the cloud practitioner exam because I was already experienced.\n\u00a0\nHow do you get more knowledge and experience for the AWS exam?\nFirst of all, it is great to review the official AWS page: https://aws.amazon.com/certification/certification-prep/\nIf you like to read more about AWS to get a broad overview first, I suggest you look at some of these cheat sheets to get some more information about all the services and concepts related to AWS:\nhttps://tutorialsdojo.com/aws-cheat-sheets/\nTo get more knowledge for the AWS developer associate exam that I took, I followed the Udemy course AWS developer associate. Although this was a great way to get a guided tour through several AWS services, this would not have been enough for me to pass the exam.\nFor that I have a few more tips listed below.\n1. Look at some of the product (services) FAQs\nStudying for the exam, you mainly have to learn about AWS services. A good way to learn about them is by reading their FAQs. This will list all the common questions others have asked about the services.\nThis is not something I did for ALL of the services that AWS is offering. However, I found it really useful to get an idea of what these services are for. Examples I looked at are:\n\nIAM\nKMS\nAWS X-Ray\n\nMore advanced products:\n\nKinesis data firehose\n\n2. Take practice exams.\nPractice exams will get you familiar with the way AWS asks you questions and what kind of knowledge you need to possess in order to pass the exam. They can also tell you whether or not you are ready to schedule a real exam.\nThe practice exam service that I used is called\u00a0Whizlabs for which you can buy exams through a one-time purchase. They also give one free exam for you to try them out first. That one has less questions, so it is not representative of the other practice exams. You could also purchase an official AWS practice exam, but that one is more expensive.\nWhen you already completed an exam successfully you will get a free official practice exam for your next certification and you will also get a 50% discount towards your next exam. These can be found on https://www.certmetrics.com/amazon/candidate/benefit_summary.aspx\nThere are five practice exams that you can make:\n\nThis is what the results look like:\n\nThe added benefit of learning through the practice exams is that you can check out extra information and even buy labs to get hands-on experience based on a question you got wrong or right:\n\n3. Saving links as resources that you didn\u2019t know about when making practice exams.\nFor me one of the links I saved was:\n\nParameter store\n\nThis is something I wanted to do more before taking the exam, but I didn\u2019t get around to it because I had already planned my exam in advance. Although it is very helpful to do in the beginning while studying.\nI also made notes from the Udemy course that I took, so I only saved one link from my Whizlabs journey. For my next exam, I will save more links to resources based on Whizlabs and I will not be taking a Udemy course. This because a video-based course makes me learn slower than being confronted with exam questions and further references after making a mistake.\n4. Doing the exam preparation training created by AWS themselves.\nJust a few days before taking your exam you have to follow the following course: https://explore.skillbuilder.aws/learn/course/internal/view/elearning/42/exam-readiness-aws-certified-developer-associate-digital\nThis course will go into detail on how AWS will ask questions and it will give an interactive way of stressing the important details that exist in the question.\nFor example, you will notice that usually the first sentence of a question is not informative for answering the question.\nThe course is free to do and takes a few hours to complete. For me this brought my practice exam score from 67% up to 84%.\n5. Schedule your exam before you think you are completely finished studying.\nIn order to actually go through with your plans of getting certified, I recommend planning the exam before you think you are fully ready for it. This will make sure that you put some more effort into getting your practice exam score above the required 80% to be ready for the real exam. My experience was that I put in 3x more studying efforts in the last two weeks leading up to the exam.\n\u00a0\nWhat you will get in return for the effort you put in\nIn the end, this is what you gave all that effort for:\n\nOn the 1000 point scale, I got an 880 grade. The minimum to pass the exam is 720. Click here to get more information on the scoring system.\nFor the cost of $150 (~\u20ac135) I wouldn\u2019t personally spend this amount of money on the exam, especially since there is a tax portion included which would be more beneficial to pay through your own company or an employer for write-off benefits.\n\u00a0\nEnding notes\nAt the company I am working for, we are continuously working to develop our cloud knowledge. In the year 2021 we started participating in \u201ccloud the game\u201d, a game to learn cloud technologies made by colleagues to be used by other colleagues.\nIf this seems like something fun for you to partake in, you can always have a look at the\u00a0careers page\u00a0and be sure to mention my name once you decide to apply.\nMy professional goals for 2021 are to learn more about security in AWS. I will be doing this by getting the\u00a0AWS Certified Security \u2013 Specialty\u00a0certification.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 27259, "title": "Untangle your micro-service driven application with Azure API Management", "url": "https://www.luminis.eu/blog/cloud-en/untangle-your-micro-service-driven-application-with-azure-api-management/", "updated_at": "2024-02-07T15:49:32", "body": "In this blog I will introduce you with the basics of Azure API Management. Firstly, I will start by introducing an use case for API management: a merger of the webservices of two online web shops. Additionally, I will show you how to create the APIM instance, a product, operations and policies. This is preferably all done using ARM templates.\nAZ-204 & Azure API Management\nIn my last blog I wrote about the renewed Azure AZ-204 exam. I had the intention to write some more blogs. I used the blogs as a way to learn for the exam, while sharing knowledge at the same time. Well, that did not go as planned.\nI found that writing and studying did cost way more time than only studying. Who would have thought so.. Nonetheless, I have completed the AZ-204 exam, and I am going to continue sharing information about the topics.\nIn this blog I will show you Azure API Management, as it is part of the exam, but also because of the added value it offers to your micro-service driven application.\nMotorcycleParts.com\nWe are building an online shop for a motorcycle parts dealer, called MotorcycleParts. It consists of multiple Azure Functions, including:\n\nProduct-service\n(Shopping) Basket-service\nProduct-stock-service\n\nThe company recently bought a car part dealer called CarPartz. The CarPartz site is going to be incorporated into the MotorcyleParts site. CarPartz recently updated their whole backend for their shop, so the requirement is to reuse their backend in the current state.\nAPI Management is going to help us to organize the calls to our functions, as well as incorporating the CarPartz backend into the MotorcycleParts site.\n\u00a0\n\nAPI management\nAPI management, in short APIM, is placed in front of our backends. It gives us the possibility to present a single endpoint to our website, instead of multiple URLs for different Azure Functions. This way the website does not have to know about all the different backend service that may exist. All the routing is done by APIM.\nIt also allows us to switch one of the backends, for a totally different implementation. Due to APIM there is a low coupling between the front-end and the backends. APIM can also help us with IP restriction, rate-limiting, and more.\nFirst off, we are going to create an APIM instance. This is possible via multiple ways: ARM templates, Azure CLI, Portal and Powershell. I am going to create the instance using an ARM template. I always choose ARM in favor of the other options since it give us the ability to use version control and it is less manual work, in other words, less error prone. The same can be said about Powershell and the Azure CLI when used in a script.\n{\r\n    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\r\n    \"contentVersion\": \"1.0.0.0\",\r\n    \"parameters\": {\r\n        \"publisherEmail\": {\r\n            \"type\": \"string\",\r\n            \"minLength\": 1,\r\n            \"metadata\": {\r\n                \"description\": \"Email owner service\"\r\n            }\r\n        },\r\n        \"publisherName\": {\r\n            \"type\": \"string\",\r\n            \"minLength\": 1,\r\n            \"metadata\": {\r\n                \"description\": \"Name owner service\"\r\n            }\r\n        },\r\n        \"location\": {\r\n            \"type\": \"string\",\r\n            \"defaultValue\": \"[resourceGroup().location]\",\r\n            \"metadata\": {\r\n                \"description\": \"Location to deploy to\"\r\n            }\r\n        }\r\n    },\r\n    \"variables\": {\r\n        \"apimInstanceName\": \"apim-poc-prod-01\",\r\n        \"apimProductName\": \"motorcyle-parts\",\r\n        \"apimApiProductServiceName\": \"product-service\",\r\n        \"getProductsOperation\": \"[concat(variables('apimInstanceName'), '/', variables('apimApiProductServiceName'), '/get')]\",\r\n        \"getProductByIdOperation\": \"[concat(variables('apimInstanceName'), '/', variables('apimApiProductServiceName'), '/get-by-id')]\"\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"type\": \"Microsoft.ApiManagement/service\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"name\": \"[variables('apimInstanceName')]\",\r\n            \"location\": \"[parameters('location')]\",\r\n            \"sku\": {\r\n                \"name\": \"Consumption\",\r\n                \"capacity\": 0\r\n            },\r\n            \"properties\": {\r\n                \"publisherEmail\": \"[parameters('publisherEmail')]\",\r\n                \"publisherName\": \"[parameters('publisherName')]\"\r\n            },\r\n            \"resources\": []\r\n        }\r\n    ]\r\n}\r\n\n\nAs you can see above, I have created an ARM template that uses the bare minimum to create the APIM instance. The first parameter, publisherEmail, is required to receive notifications about your APIM instance. This email address is for example used to notify you when the creation of the instance is finished. The creation of an instance can sometimes take up to one hour.\nFor this blog I chose to create APIM in a consumption plan, since all the Azure Functions are also on a pay-per-use plan. The consumption plan has, in contrary to a normal subscription, some usage limits and, custom domain names are not supported. The documentation shows all the differences and the limitations that apply.\nYou can deploy this template, and after a while the APIM instance will be available. But without API definitions, APIM is not going to do anything. We can add API definitions through the portal. When you navigate to the APIM Resource -> APIs, it should look like this. But again, I like to automate as much as possible, so I will use ARM templates.\n\n\nAdding APIs to APIM\nAs described in the introduction, the APIM instance should make four endpoints of the backend services available. Three new Azure Functions and an external system which is, for the sake of the difference, running on an Azure Kubernetes cluster. We are going to add the endpoints using the ARM template.\nI like to start by creating a product in APIM. Using a product you can group one or more APIs. It is possible configure some stuff on product-level. In the example below you can see the product definition. For the product I only defined a name and a reference to the API that is going to be part of the product. The API on the contrary, contains more properties. The path defines the route we can call from the APIM perspective. The service-url defines the base url of the Azure Function.\n\n\n{\r\n    \"type\": \"Microsoft.ApiManagement/service\",\r\n    \"apiVersion\": \"2019-12-01\",\r\n    \"name\": \"[variables('apimInstanceName')]\",\r\n    \"location\": \"[parameters('location')]\",\r\n    \"sku\": {\r\n        \"name\": \"Consumption\",\r\n        \"capacity\": 0\r\n    },\r\n    \"properties\": {\r\n        \"publisherEmail\": \"[parameters('publisherEmail')]\",\r\n        \"publisherName\": \"[parameters('publisherName')]\"\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"name\": \"[variables('apimProductName')]\",\r\n            \"type\": \"products\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"properties\": {\r\n                \"displayName\": \"[variables('apimProductName')]\"\r\n            },\r\n            \"dependsOn\": [\r\n                \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\"\r\n            ],\r\n            \"resources\": [\r\n                {\r\n                    \"name\": \"[variables('apimApiProductServiceName')]\",\r\n                    \"type\": \"apis\",\r\n                    \"apiVersion\": \"2019-12-01\",\r\n                    \"dependsOn\": [\r\n                        \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\",\r\n                        \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/products/', variables('apimProductName'))]\",\r\n                        \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n                    ]\r\n                }\r\n            ]\r\n        },\r\n        {\r\n            \"name\": \"[variables('apimApiProductServiceName')]\",\r\n            \"type\": \"apis\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\"\r\n            ],\r\n            \"properties\": {\r\n                \"displayName\": \"[variables('apimApiProductServiceName')]\",\r\n                \"path\": \"product-service\",\r\n                \"serviceUrl\": \"https://apim-poc-test.azurewebsites.net/api\",\r\n                \"protocols\": [\r\n                    \"http\",\r\n                    \"https\"\r\n                ]\r\n            },\r\n            \"resources\": []\r\n        }\r\n    ]\r\n}\r\n\n\n\nOperations\nOne key ingredient is still missing, operations. Operations are the actual API calls that will be available from APIM. I have created two operations. One for retrieving all products, and a second to filter by id. The defined url template is appended to the API service-url we defined earlier.\nMost properties of the operation are self-explanatory. The templateParameters, are the parameters that we expect to receive at the APIM endpoint. We can also define required or optional query parameters and headers. For POST requests we define an expected request body.\n\n\n\n{\r\n    \"name\": \"[variables('apimApiProductServiceName')]\",\r\n    \"type\": \"apis\",\r\n    \"apiVersion\": \"2019-12-01\",\r\n    \"dependsOn\": [\r\n        \"[resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName'))]\"\r\n    ],\r\n    \"properties\": {\r\n        \"displayName\": \"[variables('apimApiProductServiceName')]\",\r\n        \"path\": \"product-service\",\r\n        \"serviceUrl\": \"https://apim-poc-test.azurewebsites.net/api\",\r\n        \"protocols\": [\r\n            \"http\",\r\n            \"https\"\r\n        ]\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"name\": \"get-all\",\r\n            \"type\": \"operations\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n            ],\r\n            \"properties\": {\r\n                \"displayName\": \"get_products\",\r\n                \"method\": \"GET\",\r\n                \"urlTemplate\": \"/products\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"get-by-id\",\r\n            \"type\": \"operations\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n            ],\r\n            \"properties\": {\r\n                \"displayName\": \"get_product_by_id\",\r\n                \"method\": \"GET\",\r\n                \"urlTemplate\": \"/products/{id}\",\r\n                \"templateParameters\": [\r\n                    {\r\n                        \"name\": \"id\",\r\n                        \"type\": \"number\",\r\n                        \"required\": true\r\n                    }\r\n                ]\r\n            },\r\n            \"resources\": []\r\n        }\r\n    ]\r\n}\r\n\n\n\n\nNow we can call APIM, which will redirect our calls to the Azure Function. You can test your APIM implementation using any REST client, but also via the Azure Portal. Once you have deployed your ARM template, the defined operations are shown in the portal. Using the test tab, you can test your operation. For APIM development I prefer this over any other rest client, since it shows you an extensive trace of the retrieved request and how it is passed on to the backend service. In my case the call to retrieve all products is working just fine. Below you can see how the APIM url is translated to the correct Azure Function url.\n\n\nPolicies\nAs I tried to retrieve a single product, I came to the conclusion that the endpoint is not working yet. I forgot that the Azure Function expects the product id via a query parameter. The APIM get-by-id operation defines the product id as a route parameter. Since the path of the APIM request is just appended to the service url of the API, I am getting a 404 response. The Azure Function only has an endpoint on /products, \u00a0there is no endpoint /products/{id}.\nIn order to redirect the get-by-id request to the Azure Function, we need to introduce something to rewrite the calls to APIM. This is where policies come into play. Policies allow you to change the behavior of APIM. APIM policies are defined in XML. There are multiple types of policies:\n\n\n\nPolicy Type\nUsage\n\n\nInbound\nStatements to apply to the request\n\n\nBackend\nStatements to apply before the request is forwarded to the backend service\n\n\nOutbound\nStatements to apply to the response of the backend\n\n\nOn-error\nStatements to apply in case of an error\n\n\n\n\u00a0\nThere are lots of policies you can create. For this example I am going to create a rewrite inbound policy. This means we are going to alter the requested path to another path. Below you can see how I have defined the policy. It is really basic. You can implement more advanced stuff, like IP-filtering and rate-limiting, as well. Keep in mind that it is possible to create policies in operation scope, but also in global, product and API scope.\n\n\n\n\n\n\n<policies>\r\n    <inbound>\r\n        <base />\r\n        <rewrite-uri template=\"/products?id={id}\"/>\r\n    </inbound>\r\n    <backend>\r\n        <base />\r\n    </backend>\r\n    <outbound>\r\n        <base />\r\n    </outbound>\r\n    <on-error>\r\n        <base />\r\n    </on-error>\r\n</policies>\r\n\n\n\n\n\n\n\n\nThe last thing to do, is to add this policy to the ARM template. In order to add the policy, you need to join the lines so it becomes a single line, and escape the quotes. Now we can add another resource to the get-by-id operation, a policy type resource. In the value property we can provide the xml of the policy.\n\n\n\n{\r\n    \"name\": \"get-by-id\",\r\n    \"type\": \"operations\",\r\n    \"apiVersion\": \"2019-12-01\",\r\n    \"dependsOn\": [\r\n        \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'))]\"\r\n    ],\r\n    \"properties\": {\r\n        \"displayName\": \"get_product_by_id\",\r\n        \"method\": \"GET\",\r\n        \"urlTemplate\": \"/products/{id}\",\r\n        \"templateParameters\": [\r\n            {\r\n                \"name\": \"id\",\r\n                \"type\": \"number\",\r\n                \"required\": true\r\n            }\r\n        ]\r\n    },\r\n    \"resources\": [\r\n        {\r\n            \"name\": \"policy\",\r\n            \"type\": \"policies\",\r\n            \"apiVersion\": \"2019-12-01\",\r\n            \"dependsOn\": [\r\n                \"[concat(resourceId('Microsoft.ApiManagement/service', variables('apimInstanceName')), '/apis/', variables('apimApiProductServiceName'), '/operations/get-by-id')]\"\r\n            ],\r\n            \"properties\": {\r\n                \"value\": \"<policies> <inbound> <base /> <rewrite-uri template=\\\"/products?id={id}\\\"/> </inbound> <backend> <base /> </backend> <outbound> <base /> </outbound> <on-error> <base /> </on-error> </policies>\",\r\n                \"format\": \"xml\"\r\n            }\r\n        }\r\n    ]\r\n}\r\n\n\n\n\n\nConclusion\nWe have just created an APIM instance and added the definition of two endpoints of one service. In the same way you can add APIs for the remaining Azure Functions and the Kubernetes cluster.\nI have had some troubles fixing errors in my ARM template. I managed to resolve them using the specification for the ARM template which you can find here. Other great examples for the ARM templates are available in the Microsoft Azure Quickstart Github repo.\n\nIn retrospect, I did not like how verbose the ARM template became. I found the definition of the policy in the ARM template to verbose and error prone. I will look for a cleaner way to do so.\n\n", "tags": ["Api Management", "Azure", "Azure Function", "Micro-services"], "categories": ["Blog", "Cloud"]}
{"post_id": 29324, "title": "Luminis Reaches New Milestone in AWS Partnership", "url": "https://www.luminis.eu/blog/news-en/luminis-reaches-new-milestone-in-aws-partnership/", "updated_at": "2021-08-10T16:07:51", "body": "Software and technology company Luminis has achieved another milestone in its AWS partnership this month. With more than 50 AWS certifications, Luminis joins a select group of organisations in the Netherlands.\n\nA strategic choice\nIn recent years, Luminis has invested heavily in cloud knowledge and expertise, with a major focus on Amazon Web Services (AWS). Because of this Luminis is now increasingly being asked for challenging cloud assignments.\n\nWorking with the latest cloud technology is not something that has only happened in recent years. Luminis has followed the rise of the Cloud since the start of AWS in 2006, and had already done several projects with AWS solutions before 2010.\nBert Ertman, VP Technology at Luminis, says: \u201cOver the past 5 years, the Cloud has become mainstream. Many companies now use cloud solutions, from hosting to cloud applications. Yet we encounter major differences among companies. Particularly in fields like software development and data analytics, solutions from before the cloud era are still widely used. As a result, organisations often lag behind in terms of speed and innovation. Understanding cloud technology in its entirety and applying it at all levels gives you a head start. That is why access to the best cloud knowledge and partnerships is of strategic importance to Luminis.\u201d\nInvesting in Cloud Proficiency\nA lot has happened in the field of cloud at Luminis in recent years. Bert Ertman cites some examples: \u201cTo work on our cloud proficiency, a group of enthusiastic colleagues started \u2018Cloud the Game\u2018. A competition with many gamification elements such as badges, groups and mini-challenges. Anyone who achieves something in the field of cloud can log their achievements via our \u2018Cloud the Game\u2019 application. Achievements include obtaining a certification, giving a presentation, writing a blog or reading one. It is wonderful to see our colleagues started doing this en masse and that considerable competition arose. And not only the engineers took part, our sales people, marketers and directors also gained a lot of knowledge about the Cloud. What you achieve with something like Cloud the Game is that everyone becomes self-sufficient in the cloud area at their own pace and finds their way. At Luminis, we\u2019re working with the Cloud every single day.\u201d\nLuminis also organised \u2018AWS Game Day\u2018 with AWS. During this day, teams from different companies competed against each other. The goal was to keep an AWS infrastructure up and running as best as possible, while things were going wrong. This requires not only broad knowledge of all kinds of AWS services, but also problem-solving ability and collaboration. In the end, the Planon team took home the first prize. AWS Game Day is a great example of how learning goes hand in hand with fun and a healthy dose of competition.\nBeneficial for customers\nLearning new things is fun, but the ultimate goal is for our customers to benefit from the best AWS knowledge available in the market. Bert Ertman notices that not only more and more customers are finding us in this area: \u201cIn recent years we have invested a lot in our relationship and cooperation with AWS. Because we focus on AWS across the board, we see that we have easier access to specialists from AWS as well. We are even being asked by AWS to help expand their own professional services department, AWS Professional Services. That to me is the ultimate proof that our knowledge and experience is at the right level. In addition, more cloud engineers and architects are interested in working at Luminis, because they can work with the latest technology, which is very nice in the current market!\u201d\nAbout Luminis\nLuminis is a software and technology company with offices in Amsterdam, Apeldoorn, Arnhem, Eindhoven, Rotterdam and the UK. From these offices, 200 professionals work on high-tech solutions for customers, including major brands such as KLM, Nike and Bol.com. Luminis distinguishes itself by demonstrably leading the way in the field of (cloud) technology and innovation.\nWant to know more about AWS? Check out our AWS consultancy page, or contact us.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 28683, "title": "Enable B2B users on an Azure AD B2C tenant", "url": "https://www.luminis.eu/blog/cloud-en/enable-b2b-users-on-an-azure-ad-b2c-tenant/", "updated_at": "2021-08-10T16:05:44", "body": "For one of our customers, we created a set of B2B applications using Azure Active Directory (B2B) and Azure App Service Authentication (EasyAuth). Now they want to enable customers on one of the applications while still enabling single sign-on for (external) business users on all applications. To accomplish this, we used Azure AD B2C and linked it with the existing Azure AD tenant that already contains all business users. In this blog I will describe the individual steps and missteps we made. I will focus solely on the Azure AD-configuration and leave out any other changes.\nLet\u2019s assume the Azure AD tenant and the AD B2C tenant including the app registration and user flows are created and focus on linking the active directory tenants.\nCreate an identity provider\nConfigure the custom identity provider\nFirst create an app registration in the Azure AD tenant. This app registration will function as the Identity Provider for the Azure AD B2C tenant. The redirect uri is the authentication response endpoint of the B2C tenant. This uri has the format https://<b2c-tenant-id>.b2clogin.com/<b2c-tenant-id>.onmicrosoft.com/oauth2/authresp.\nNext configure a client secret. Remember the client id and the secret value since they are needed when configuring the identity provider in the B2C tenant.\nSwitch to the B2C-tenant to create a new identity provider. Make sure it has a nice and recognizable name, since that name will be displayed on the sign-in and sign-up pages.\nProvide here the client id and client secret of the app we just created and build up the metadata url based on the following template: https://login.microsoftonline.com/<b2b-tenant-id>/v2.0/.well-known/openid-configuration.\nSign-in flow with corporate account\nThe last step is enabling the identity provider in the user flows. This is nothing more than selecting the OpenID Connect Identity provider in the required user flows. Do not forget to enable the identity provider in the sign up flow. See the \u2018issues I ran into\u2019 for more details. When testing the sign-in flow the \u2018corporate\u2019 sign-in button will be displayed.\nIssues I ran into:\nDeleting an identity provider\nWhile preparing the blogpost, I removed an identity provider that was in use by a user flow. This caused the user flow to be hidden and it was almost impossible to get it back again.\nAdmin consent for B2B user\nWhen signing in with a B2B account, the user is asked to consent. When the user has no admin privileges it will not be possible consent. Either an admin of his organization has to sign in first or you should give admin consent in the B2B App Registration on the Api Permissions-tab.\nSigning in with B2B user\nWhen signing in with a B2B user, I end-up with a 401-error on the \u2018callback\u2019-request to my application. When looking closer at the request in the network tab in the developer tools I discovered the following description:\nUser does not exist\nAlthough it\u2019s a business user registered in the B2B tenant, the user needs to sign up first. This will create the link between the B2C tenant and the B2B tenant. When using the combined sign-in and sign-up flow, the user will be registered automatically in the B2C-tenant.\n", "tags": ["Azure AD", "Azure AD B2C"], "categories": ["Blog", "Cloud", "Security"]}
{"post_id": 28443, "title": "How to use Java CDK to define a DynamoDB-backed REST API with only AWS API Gateway \u2013 part 2", "url": "https://www.luminis.eu/blog/how-to-use-java-cdk-to-define-a-dynamodb-backed-rest-api-with-only-aws-api-gateway-part-2/", "updated_at": "2021-08-10T16:05:35", "body": "In the previous blog we discussed how you can define a no-code or low-code REST-API on top of a DynamoDB table. We used a high level construct from the AWS Solutions Constructs library, that defines all in one go. The limitation we ran into was that we couldn\u2019t customise the output of the GET method in order to return only the (stored) JSON document. In this blog, we\u2019ll replace the solution based on AWS Solutions Constructs library by a solution build with the standard CDK elements, that gives all the freedom we need.\nAs a starting point, we take the source code developed in part 1. Our first step is to refactor the code by replacing the \u201cAWS Solutions Constructs\u201d by constructs from the AWS Construct Library, and create exact the same deployment as in part 1. From there, we\u2019ll improve the response of the GET method by adding a response template. Note that the complete sample code can be found on github.\nDefining the major elements\nFor this refactoring, we only need to replace the content of the DynamoRestStack class. Just remove all code from the constructor, except for the call to super. We start with defining the major elements: the REST API and the DynamoDB table. Defining the REST API is pretty simple:\nRestApi dynamoRestApi = new RestApi(this, \"DynamoRest\", RestApiProps.builder()\r\n    .deployOptions(StageOptions.builder()\r\n    .loggingLevel(MethodLoggingLevel.INFO)\r\n    .build())\r\n    .build());\nActually, it\u2019s not much more then setting a meaningful name (\u201cDynamoRest\u201d) and enabling logging (in the previous version, the AWS Solution Construct enabled logging by default).\nDefining the table is almost identical as before:\nTableProps tableProps = TableProps.builder()\r\n    .partitionKey(Attribute.builder()\r\n         .name(\"id\")\r\n         .type(AttributeType.STRING)\r\n         .build())\r\n    .tableName(\"exerciseStats\")\r\n    .build();\r\nTable dynamoDbTable = new Table(this, \"exerciseStats\", tableProps);\nwe only have to instantiate the Table explicitly (last line in the sample).\nAllow access to db\nNext, we need an IAM Role, in order to allow the API Gateway to access the DynamoDB table. This is typically something a higher level construct can arrange for us, which is why we didn\u2019t have to bother while using the AWS Solution Constructs library in part 1. This is how the Role is defined in Java code:\nrole = new Role(this, \"dynamorest\", RoleProps.builder()\r\n    .assumedBy(new ServicePrincipal(\"apigateway.amazonaws.com\"))\r\n    .build());\r\n\r\nrole.addToPolicy(new PolicyStatement(PolicyStatementProps.builder()\r\n    .actions(List.of(\"dynamodb:Query\", \r\n                     \"dynamodb:PutItem\", \r\n                     \"dynamodb:UpdateItem\"))\r\n    .effect(Effect.ALLOW)\r\n    .resources(List.of(dynamoDbTable.getTableArn()))\r\n    .build()));\nNote how the second part adds ALLOW permissions on the DynamoDB table (identified by its ARN) for the Query, PutItem and UpdateItem actions.\nDefining the REST resource\nThis brings us to the interesting parts: setting up the resources and methods that make up the REST API. As before, we want the resource to be created by sending a POST request to root (\u201c/\u201d). Translated to CDK code: get the root resource and add a POST method:\nIResource rootResource = dynamoRestApi.getRoot();\r\nrootResource.addMethod(\"POST\", createIntegration, \r\n                       MethodOptions.builder()\r\n    .methodResponses(List.of(MethodResponse.builder()\r\n        .statusCode(\"200\")\r\n        .build()))\r\n    .build());\nTo make this compile, we first need to define how this method integrates with the back-end service (i.e. we need to define createIntegration). This is a bit more complicated, lets review each line of the following fragment:\nIntegration createIntegration = AwsIntegration.Builder.create()\r\n    .action(\"PutItem\")\r\n    .service(\"dynamodb\")\r\n    .integrationHttpMethod(\"POST\")\r\n    .options(IntegrationOptions.builder()\r\n        .credentialsRole(role)\r\n        .requestTemplates(Map.of(\"application/json\", createRequestTemplate))\r\n        .integrationResponses(List.of(IntegrationResponse.builder()\r\n            .statusCode(\"200\")\r\n            .build()))\r\n        .build())\r\n    .build();\n\naction (PutItem): the command that is sent to the back-end service\nservice: definition of the back-end service. Use lowercase; even though the AWS console shows \u201cDynamoDB\u201d, that doesn\u2019t work in CDK!\nintegrationHttpMethod: the HTTP method that is used to communicate with the back-end service; always POST for DynamoDB (do not confuse with the HTTP method used for in the REST call itself)\ncredentialsRole: the IAM role that will give the integration the necessary permissions\nrequestTemplate: the template that defines the command being send to DynamoDB\nintegrationResponses: the response(s) the REST call might return.\n\nThe (createRequestTemplate) is exactly the same template we used in part 1.\nWe\u2019re nearly there. For the PUT method, the recipe is almost the same (with action = \u201cUpdateItem\u201d), except for the resource. To update the resource, we don\u2019t want to PUT to root, but to the resource to be updated of course (e.g. /2021-05-31). So, instead of retrieving and using the root, we create the document resource first:\nResource doc = dynamoRestApi.getRoot().addResource(\"{id}\");\nThe rest (pun intended) is pretty similar. The same holds for the GET method: in this case the action is \u201cQuery\u201d. Refer to this commit for the complete solution at this stage.\nBack where we started\nThe code is now functionally equivalent to the version created in part 1: if you run cdk deploy you can check the result in AWS Console or test it by \u201ccurling\u201d a POST request.\nThe final part\nFinally, we get at the part that made us start. Our aim is to write a template that will transform the response from DynamoDB into a proper REST API response. Let\u2019s recall what DynamoDB serves us when issuing the Query command:\n{\"Count\":1,\r\n\"Items\":[ {\r\n    \"content\": {\r\n        \"S\": \"{ date: \\\"2021-05-25\\\", exercise: 42}\"\r\n    },\r\n    \"id\": {\r\n        \"S\": \"2021-05-25\"\r\n    }\r\n} ],\r\n\"ScannedCount\":1}\nWhat we want to return is the content, but with the \u201cS\u201d (type indicator) removed. This transformation is achieved by this piece of code:\n#set($inputRoot = $input.path('$'))\r\n#if(!$inputRoot.Items.isEmpty())$inputRoot.Items[0].content.S\r\n#end\nThe syntax is from the velocity templating language, which has been used in multiple Java web frameworks in the past. See API Gateway Developer guide for more info. It\u2019s a bit cryptic, but you probably can guess what it does: if the \u201cItems\u201d element of the response is non-empty, it navigates to the first item, takes the content part and and then the \u201cS\u201d part. If the \u201cItems\u201d element is empty, it returns nothing. The template rendering is quite literal: you can add spaces, but they\u2019ll end up in the result at the same spot; that\u2019s why we skipped all spaces, even though spaces would improve readability ;-).\nDone\nJust expand the Integration element with this template:\n.integrationResponses(List.of(IntegrationResponse.builder()\r\n    .statusCode(\"200\")\r\n    .responseTemplates(Map.of(\"application/json\", \r\n        \"#set($inputRoot = $input.path('$'))\\n\" +\r\n        \"#if(!$inputRoot.Items.isEmpty())\" +\r\n        \"$inputRoot.Items[0].content.S\\n\" +\r\n        \"#end\\n\"))\r\n    .build()))\r\n\nand cdk deploy the solution. To see the result, copy-paste the production URL from the CDK output (or look it up in the AWS Console) and query an item with\ncurl https://xxx-api.eu-west-1.amazonaws.com/prod/2021-05-25\nWrap up\nNow we have transformed the code to use only standard CDK constructs (from the AWS Construct Library, it becomes even more clear that higher level constructs like the one we used in part 1, make life easier. However, as with all abstractions, it also hides details that would actually give us more insights how things work under the hood. I think the takeaway is that knowing both solutions improves your understanding and ensures you can always pick the right tool for the job. In this case, we had to use the lower level API to complete our solution in a proper way and avoid that a GET request would return implementations details about the underlying persistence mechanism.\nDo not forget the cdk destroy the sample to avoid unwanted costs.\n", "tags": [], "categories": ["Blog", "Cloud", "Development"]}
{"post_id": 28369, "title": "Kotlin Multiplatform \u2013 A remedy for all mobile app development?", "url": "https://www.luminis.eu/blog/development-en/kotlin-multiplatform-a-remedy-for-all-mobile-app-development/", "updated_at": "2021-08-10T16:05:27", "body": "Ever since mobile apps were built there was a desire to have a single codebase that can run upon multiple platforms. Way more solutions and technologies saw the light of day than I can think and can keep track of and with Kotlin Multiplatform (Mobile) there is a (fairly) new player in town. Is this new player in town worth to invest your valuable time in? In this article, we\u2019ll explore Kotlin Multiplatform, what it is, how it is different from what already is available, how it works, and finally arrive at a conclusion as to why you should at least try it in one of your next mobile projects.\n\u00a0\nSo, what is Kotlin Multiplatform Mobile?\n\nSome say Kotlin Multiplatform Mobile is an SDK, a mechanism, an extension or even a way of living. In my opinion it doesn\u2019t really matter what it is, although Jetbrains, the company behind Kotlin, says it\u2019s an SDK, as long as it does what it says it does. Kotlin Multiplatform Mobile is a subset of Kotlin Multiplatform and allows you to write business logic code for both iOS and Android from within a single codebase. While the superset, Kotlin Multiplatform, enables you to include other platforms like Linux, Windows and the web (JavaScript). Kotlin Multiplatform Mobile projects are still considered alpha, which means features and tooling may change in future Kotlin releases, but nonetheless it is getting more mature fast, with almost all its components already considered beta.\n\u00a0\nOkay, but how is it different than what is already known?\nIt really is not, at least if you compare the goals of most \u2013 if not all \u2013 other solutions with each other. Most \u2013 if not all \u2013 solutions aim to make developing apps easier and more maintainable. They differ by how they are achieving that goal and, in my opinion, most compelling difference between Kotlin Multiplatform Mobile and most other solutions is that Kotlin Multiplatform Mobile allows you to run compiled code straight onto native platforms, without the code being run using a virtual machine or in a webview-like solution. The second big difference is that Kotlin Multiplatform Mobile guarantees a full native user experience, because (for now) it doesn\u2019t support sharing UI logic across platforms, which means iOS developers have no other option but to use Apple\u2019s frameworks. With Xamarin a similar approach can be used. Solutions like React Native (and other webview-like solutions) need to bridge code between the native OS platforms and JavaScript. This causes performance issues and on top of that the animations are not as satisfactory as within native apps. Flutter solves many of these issues common to React Native, and thus it also doesn\u2019t guarantee a full native user experience. The table below displays a few of the main differences between a couple of widely adopted other solutions.\n\n\u00a0\nNow, how does it work?\n\nKotlin Multiplatform (Mobile) enables you to write common (Kotlin) code for your business logic, which is then usable on every platform supported, because all code gets compiled to bytecode for Android and native for iOS. When you need to write code that needs to access platform-specific APIs you can use the Kotlin mechanism of expected and actual declarations. With this mechanism, a common source set defines an expected declaration, and platform source sets must provide the actual declaration that corresponds to the expected declaration as seen below in the small code example. One of the key benefits is that you can use code from platform-dependent libraries like Foundation and UIKit for iOS.\n// Common\r\nexpect fun randomUUID(): String\r\n\n// Android\r\nimport java.util.*\r\nactual fun randomUUID() = UUID.randomUUID().toString()\r\n\n// iOS\r\nimport platform.Foundation.NSUUID\r\nactual fun randomUUID(): String = NSUUID().UUIDString()\r\n\nWith this knowledge it is possible to leverage the advantage of this mechanism and move as much code to the shared library as possible and thus make the actual native app as \u2018dumb\u2019 as possible. Ideally the native app is only used to link the UI with the actual shared library. Using Kotlin Multiplatform I think that it should be possible to share 70-80% of code across mobile platforms and even up to maybe 90% on \u201csimple\u201d apps. Adding a new platform will solely be a matter of implementing UI code.\n\u00a0\nWhy should I choose Kotlin Multiplatform Mobile for my next project?\n\nIt has no overhead as it compiles to bytecode for Android and native for iOS.\nMany Android developers are already familiar with Kotlin and the Gradle build system, it\u2019s a little to no change for them.\nNot sharing the UI can be a curse or a blessing. While reducing code by writing UI once, this can come at the expense of reduced user experience as every platform has its own way / method of interaction, guidelines and so on, this can either be a curse or a blessing: Everyone bakes his cake as he likes to eat it.\nDue to the interop with the other platform specific languages Kotlin Multiplatform allows you to write Java, Swift or Objective-C code and use Kotlin code within those languages and vice-versa.\nIt is also possible to use platform-specific libraries and even choose a totally different library for let\u2019s say logging where each platform defines a complete other interface to log (you need to define what functions should be called using the expect/actual-mechanism, but this opens up for tons of possibilities).\n\n\u00a0\nAre there any downsides?\nOf course. If there are advantages there are probably also disadvantages, which is also the case for Kotlin Multiplatform (Mobile).\n\nKotlin Multiplatform (Mobile) projects are considered alpha, so in future Kotlin releases features and or tooling may change.\nIt has a relatively smaller community as compared to React Native or Flutter, which means it is possible there will not be an answer for all your questions or problems. I personally haven\u2019t experienced it yet, but it is possible.\nFor iOS-only developers, with no Kotlin or Gradle experience, it means a new build system and/or language should be learned.\nObjective-C interop (still) has its limitations. Inline classes and custom classes implementing standard Kotlin collection interfaces aren\u2019t fully supported.\n\n\u00a0\nHow do I get started?\nI won\u2019t get into too much detail about how you can get started. This isn\u2019t supposed to be a how-to guide, but merely an article informing you about what Kotlin Multiplatform Mobile is. Nevertheless, I\u2019ve found some great resources that I think you should most definitely check if you want to try it out.\n\nFirst, I would suggest visiting its website.\nAfter that I would take a look at the docs.\nRay Wanderlich always has a lot of good hands-on tutorials.\nLast but not least; I would suggest joining the Kotlin slack or get otherwise involved with the community.\n\n\u00a0\nFinal Thoughts\nNow that we\u2019ve explored Kotlin Multiplatform Mobile, we\u2019ve learned that it is an SDK available to share as much code as possible between platforms, that it guarantees a full native user experience, while compiled code is being run straight onto the native platforms. We\u2019ve also explored how Kotlin Multiplatform Mobile works and that we can access platform-dependent APIs with the help of the expected and actual declarations. All in all, when weighing the advantages with the disadvantages, I think it is safe to say Kotlin Multiplatform Mobile is worth your valuable time (especially if you\u2019re an app developer like me, even if you\u2019re a iOS-only developer) and thus you should most definitely try it out on one of your next projects to become just as enthusiastic as I am.\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 28160, "title": "How to use Java CDK to define a DynamoDB-backed REST API with only AWS API Gateway", "url": "https://www.luminis.eu/blog/cloud-en/rest-api-on-dynamodb-with-aws-api-gateway-in-java-cdk/", "updated_at": "2021-08-10T16:05:11", "body": "A step by step tutorial to make AWS API Gateway implement a REST API on top of a DynamoDB table, deployed using a CDK script written in Java.\nIn \u201ctraditional\u201d Java development, a REST API for objects stored in a database would mostly be served by some Java code handling the HTTP requests and transforming the database model to the appropriate models used in the API. You probably wouldn\u2019t even consider to implement it in another way, for example in the HTTP proxy that connects your application to the internet. When architecting a Java application, it doesn\u2019t feel right to put business logic in an infrastructure component. And it would not be very convenient in the development stage either, because the HTTP proxy is often not even present in the development environment.\nEnter cloud\nWith cloud development, this is all a bit different. With infrastructure-as-code, the infrastructure has become part of the development and probably, its definition is even in the same source code repository as the application itself. Moreover, one of the major advantages of the cloud is that it provides a plentitude of services, or building blocks, that should make our (developer) live easier. Well, if it makes it easier is something we\u2019ll cover later on, but at least it should support the notion of writing less code and concentrate on business code in stead of code that moves data (\u201cwrite code that transforms, not transports\u201d).\nCDK\nIn this blog, we\u2019ll take a look at using AWS API Gateway to provide a REST interface on top of a DynamoDB table, as a low-code alternative to writing a Lambda function to implement a REST interface. Of course we\u2019ll be using CDK to define and deploy the cloud components (infrastructure-as-code) and we\u2019ll the Java variant of the CDK. Of course you can use CDK with different languages, and you could as well use TypeScript, but if support for different languages has the same maturity level, you should pick the language you are most familiair with, so that no doubts about how to express yourself in the language get in the way of what our focus is: get it deployed in the cloud.\n\nThe sample we\u2019ll use is storing a small and simple JSON document that records the distance you walked / biked / run everyday (i know there are great Apps for this, it is just a simple example ;-)). The tiny JSON document will have a format like this:\n{ date: \"2021-05-23\", distance: 38 }\nPlease note that the complete sample code developed in this blog, as well as the different stages of development presented in this blog, can be found on github.\nGetting started\nAssuming you\u2019ve already installed and bootstrapped the CDK (if not, check out this), we can generate a project template with cdk init app --language=java. This will even initialise a git repository and create a first commit with the generated sources; the name of the project and generated classes is based on the directory name. The first thing to do is to change the package name of the generated classes from com.myorg to something sensible.\nThe next thing to do is the edit the DynamoRestApp class and remove the call to the DynamoRestStack.Builder and replace it by a simple instantiation of the DynamoRestStack. Even though usage of a builder is very common in CDK, we remove it here because the referenced DynamoRestStack.Builder class is not generated at all. It will compile (thanks to inheriting a no-op implementation), but it won\u2019t do anything useful, which is of course very confusing and not something you would expect from generated code that is there to give you a quick start. So replace the contains of the generated main method by\npublic static void main(final String[] args) {\r\n    App app = new App();\r\n    new DynamoRestStack(app, \"DynamoRestStack\");\r\n    app.synth();\r\n}\nWe\u2019ll also remove (or truncate) the generated test case; we won\u2019t need it for this blog.\nNow it\u2019s time to start coding. The AWS Solutions Constructs library provides a solution construct that seems ideal for our use case: the aws-apigateway-dynamodb module.\nTo use it, we need to declare it as a maven dependency (in the pom.xml):\n<dependency>\r\n    <groupId>software.amazon.awsconstructs</groupId>\r\n    <artifactId>apigatewaydynamodb</artifactId>\r\n    <version>${cdk.version}</version>\r\n</dependency>\nTo construct our solution, we could do with this minimal piece of code\nApiGatewayToDynamoDBProps apiGatewayToDynamoDBProps = \r\n    ApiGatewayToDynamoDBProps.builder()\r\n\u00a0 \u00a0     .allowCreateOperation(true)\r\n\u00a0 \u00a0     .allowUpdateOperation(true)\r\n\u00a0 \u00a0     .allowReadOperation(true)\r\n\u00a0 \u00a0     .build();\r\nApiGatewayToDynamoDB apiGateway = \r\n    new ApiGatewayToDynamoDB(this, \"dynamogateway\", \r\n                             apiGatewayToDynamoDBProps);\nHowever, that would create an API with the rather meaningless name \u201cRestApi\u201d, so to avoid our AWS account gets messy with non-descriptive names, we\u2019ll fix that right away by explicitly defining the name:\nRestApiProps apiGatewayProps = RestApiProps.builder()\r\n    .restApiName(\"DynamoRest\")\r\n    .build();\nAlso, the default will use IAM authentication, which is very inconvenient for testing with for example a curl client, so we\u2019ll replace that by authentication type none. If you code along with this blog, make sure you remove the deployment when done. (Alternatively, you could secure the API with an API_KEY, which is much easier to use.)\nRestApiProps apiGatewayProps = RestApiProps.builder()\r\n    .restApiName(\"DynamoRest\")\r\n    .defaultMethodOptions(MethodOptions.builder()\r\n    .authorizationType(AuthorizationType.NONE)\r\n    .build())\r\n.build();\nWhen you deploy the solution with the cdk deploy command, it will ask for confirmation when IAM roles are affected. This can be very annoying, especially when deployment takes several minutes, and you spend the waiting time doing something else only to discover upon return that all the time, the deployment command has been waiting for your approval. To get rid of this confirmation, add the following line to the cdk.json file:\n\"requireApproval\": \"never\"\nNearly done!?\nIf you deploy what we have got so far and take a look at the API in the AWS console, you\u2019ll be disappointed to see the API defining only one HTTP method. The solution only has a GET method that can be used to read from DynamoDB, there are no create and update operations, even though we defined them in our code (e.g. with allowCreateOperation(true)).\n\nThis is because the create and update methods need a template that tells API Gateway how to convert the request body into a proper DynamoDB command. Let\u2019s start with the creation. The template we need is this:\n{\r\n    \"TableName\": \"exerciseStats\",\r\n    \"Item\": {\r\n        \"id\": {\r\n            \"S\": \"$input.params('id')\"\r\n        },\r\n        \"content\":\r\n            \"S\": \"$util.escapeJavaScript($input.body)\"\r\n        }\r\n    }\r\n}\nIf you read this template, it\u2019s probably obvious that the DynamoDB item will consist of a partition key (by default named \u201cid\u201d) and an attribute named content, that we will use to store our JSON data. The command syntax of DynamoDB requires the values to be indicated by type, the \u201cS\u201d stands for String (see Data Type Descriptors in DynamoDB developer guide)\nBecause the content is JSON, the quotes need to be escaped, which is why we need the $util.escapeJavaScript() function.\nThe update template is similar, but uses a different syntax as you must explicitly define what to update:\n{\r\n    \"TableName\": \"exerciseStats\",\r\n    \"Key\": {\r\n        \"id\": {\r\n            \"S\": \"$input.params('id')\"\r\n        }\r\n    },\r\n    \"UpdateExpression\": \"set content = :v1\",\r\n    \"ExpressionAttributeValues\": {\r\n        \":v1\": {\r\n            \"S\": \"$util.escapeJavaScript($input.body)\"\r\n        }\r\n    },\r\n    \"ReturnValues\": \"NONE\"\r\n}\nAfter deploying this version, you can POST, PUT or GET items with curl. To obtain the URL, open the AWS Console in your browser and go to API Gateway, select the \u201cDynamoRest\u201d API, select stages and click the \u201cprod\u201d stage; the invoke URL is displayed in the stage editor at the right.  To create or update an item, use curl commands like these:\ncurl -X POST -H 'Content-Type: application/json' -d '{ date: \"2021-05-25\", exercise: 42 }' https://xxx-api.eu-west-1.amazonaws.com/prod?id=2021-05-25\r\ncurl -X PUT -H 'Content-Type: application/json' -d '{ date: \"2021-05-23\", time: 38 }' https://xxx-api.eu-west-1.amazonaws.com/prod/2021-05-23\r\n\nNote that the POST has the id as query parameters, while the PUT has the id embedded in the URL.\nGET\nOf course, the GET is just curl https://xxx-api.eu-west-1.amazonaws.com/prod/2021-05-25. This returns\n{\"Count\":1,\"Items\":[{\"content\":{\"S\":\"{ date: \\\"2021-05-25\\\", exercise: 42}\"},\"id\":{\"S\":\"2021-05-25\"}}],\"ScannedCount\":1}\nwhich is not exactly what we want. What we get here is the raw DynamoDB result, not the response we would expect from a proper REST API, which is the JSON content returned in the same format as we used for inserting ({ date: \"2021-05-23\", time: 38 }).\nIf we would deploy this solution, it would leak implementation details: anyone could tell from the result that under the hood, DynamoDB is being used. From engineering point of view, this is a bad thing \u2013 and not because the fact that DynamoDB is being used should be kept secret. It is a signal that implementation and interface are not clearly separated. This is bad because once clients of there API exist, they will always expect this syntax and thereby depend on (the implementation detail that) DynamoDB is being used. If you would ever want to change implementation by using a different persistence mechanism, your API will change and clients will break. If the interface is cleanly separated from the implementation, this will never happen.\nTo make the GET request return the JSON document only, we would need to use custom response template: a script that defines how the output from DynamoDB is mapped to the response of the GET request. AWS API Gateway supports this out of the box, but unfortunately, the AWS solution construct we used here does not support this (yet). In order to finish this REST API properly, we need to return to using the constructs from the basic CDK library. This requires more coding, but it will enable us to customise every single piece of it, including the response template. How this is coded exactly will be covered in part 2.\n", "tags": [], "categories": ["Blog", "Cloud"]}
{"post_id": 27901, "title": "PIT Mutation Testing", "url": "https://www.luminis.eu/blog/resilience-en/pit-mutation-testing/", "updated_at": "2023-01-31T11:54:44", "body": "Real world mutation testing\nIt\u2019s been hard keeping up with everything else going around the world, even after eliminating the long travelling hours. But still, I do have something interesting to share from work. So far, the projects I have been assigned to, had an average testing coverage, ranging from 70-90ish%. But here at our client, we have\n\nJunit tests\nRegression tests\nPitests\n\nEverything should be at 100% before it is merged into develop branch.\nFor us at Luminis, Java, Junit test sit in the core, so I won\u2019t be using my word limit for that.\nRegression tests is a synonym for Cucumber tests at our client. Most of us are familiar with Cucumber tests too. It allows automation of functional validation in easily readable and understandable format (like plain English) to Business Analysts, Developers, Testers, etc.\nCucumber (created in 2008) executes tests specified written in language called Gherkin. It is a plaint-text natural language (for example, English or one of other 60+ languages supported by Cucumber) with a given structure.\nIn a Gherkin file, non-blank lines can start with a keyword, followed by text in natural language. The main keywords are the following:\n\nFeature: High-level description of the software feature to be tested. It can be seen as a use case description.\nScenario: Concrete example that illustrates a business rule. Scenarios follow the same pattern:\n\nExample of a feature file:\nScenario Outline: Successful login to Searchlight as a legit Searchlight user\u00a0\n\u00a0\u00a0\u00a0 Given I am on the \"Login\" page\u00a0\n\u00a0\u00a0\u00a0 When I fill in \"username\" field with \"foo@mydomain.com\"\u00a0\n\u00a0\u00a0\u00a0 And I fill in \"password\" field with \"CucumberIsMagic\"\u00a0\n\u00a0\u00a0\u00a0 And I click \"submit\" button\u00a0\n\u00a0\u00a0\u00a0 Then I should be logged in successfully as \"foo@mydomain.com\"\nPitests \u2013 Now this is the interesting part. At least for me. I never came across mutation testing, i.e., a software\u00a0testing\u00a0in which certain statements of the source code are changed/mutated to check if the\u00a0test\u00a0cases can find errors in source code. The goal of\u00a0Mutation Testing\u00a0is ensuring the quality of\u00a0test\u00a0cases in terms of robustness that it should fail the\u00a0mutated\u00a0source code.\nTraditional test coverage (i.e line, statement, branch, etc.) measures only which code is\u00a0executed by your tests. It does\u00a0not\u00a0check that your tests are actually able to\u00a0detect faults\u00a0in the executed code. It is therefore only able to identify code that is definitely\u00a0not tested. The most extreme examples of the problem are tests with no assertions. Fortunately these are uncommon in most code bases. Much more common is code that is only\u00a0partially tested\u00a0by its suite.\nPitest at work\nOur code:\npublic boolean isPositive(int number) {\n\u00a0 \u00a0 boolean result = false;\n\u00a0 \u00a0 if (number >= 0) {\n\u00a0 \u00a0 \u00a0 \u00a0 result = true;\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return result;\n}\nPitest makes the following mutations of our code to test if they really do meaningful work\n#1 Mutation \u2013 Changed conditional boundary (mutator)\n\u00a0\u00a0\u00a0 public boolean isPositive(int number) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 boolean result = false;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// mutator - changed conditional boundary\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (number > 0) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 result = true;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return result;\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0 }\n#2 Mutation \u2013 Negated conditional (mutator)\npublic boolean isPositive(int number) {\n\u00a0 \u00a0 boolean result = false;\n\u00a0 \u00a0 // mutator - negated conditional\n\u00a0 \u00a0 if (false) {\n\u00a0 \u00a0 \u00a0 \u00a0 result = true;\n\u00a0 \u00a0 }\n\u00a0 \u00a0 return result;\n}\nA Good unit test should fail (kill) all the mutations #1,#2,#3.\n@Test\npublic void testPositive() {\n\u00a0 \u00a0 CalculatorService obj = new CalculatorService();\n\u00a0 \u00a0 assertEquals(true, obj.isPositive(10));\n}\nThe above unit test will kill the mutation #2 (unit test is failed), but the mutation #1 is survived (unit test is passed).\nReview the mutation #1 again. To fail (kill) this test (mutation), we should test the conditional boundary, the number zero.\n\u00a0\u00a0\u00a0 public boolean isPositive(int number) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 boolean result = false;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// mutator - changed conditional boundary\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (number > 0) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 result = true;\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return result;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0 }\nImproving the unit test by testing the number zero.\n\u00a0\u00a0\u00a0 @Test\n\u00a0\u00a0\u00a0 public void testPositive() {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CalculatorService obj = new CalculatorService();\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 assertEquals(true, obj.isPositive(10));\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//kill mutation #1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 assertEquals(true, obj.isPositive(0));\n\u00a0\u00a0\u00a0 }\nDone, 100% mutation coverage now.\nThere are other mutation testing systems for Java, but they are not widely used. They are mostly slow, difficult to use and written to meet the needs of academic research rather than real development teams.\nPIT is different. It\u2019s fast\u00a0\u2013 can analyse in\u00a0minutes\u00a0what would take earlier systems\u00a0days; and is actively developed & supported.\nThe reports produced by PIT are in an easy to read format combining\u00a0line coverage\u00a0and\u00a0mutation coverage\u00a0information.\n\n\nAnd it is quite easy to integrate with Ant, Maven, Gradle and others. There are eclipse and IntelliJ idea plugins available for use too. So, there is something new I learned at work.\n", "tags": [], "categories": ["Blog", "Development", "Resilience"]}
{"post_id": 27882, "title": "Gazar about Luminis; \u201cNo empty promises\u201d", "url": "https://www.luminis.eu/blog/gazar-over-luminis-geen-loze-beloftes/", "updated_at": "2021-08-10T16:10:01", "body": "Gazar Ajroumjan worked for another company for a while after college, dabled in midlancing, but eventually decided he could learn more in a permanent position. That is how he ended up at Luminis.\nTell us Gazar, what was your job interview at Luminis like?\nI almost canceled my job interview with Luminis. Two more companies were interested in me and I thought I would just choose one of them. In the end I went to see Luminis, and until this day I do not regret this decision. The conversation was fun, something instantly clicked. What particularly attracted me to Luminis is that there is a lot of room for personal development. At first I thought this was just a sales pitch, after all, every company makes new IT-ers those kinds of promises. But at Luminis things where different: there is no fixed training buget, if you can explain why you need a certain course, you will be able to partake.\n\nWhat kind of projects are you currently working on?\nThe first project I worked on was really fun: developing a lead motion controller. I had no idea that Luminis had such interesting, innovative projects, in that respect the website does not do the organisation justice. Although it was my first project, I was given a lot of freedom. I was allowed to choose which techniques I wanted to use and how I would implement everything with the help of a designer.\nAt the moment I work at VGZ, where we are designing a chatbot, which is also a very interesting project. In the future I want to focus on the business side. That is why I am doing an MA in Management at Nyenrode. Software is very interesting, but I do not want to limit myself by specialising in just one area of expertise. In addition to my work, I have been running my own company for about eight years, which is currently on the back burner, but I have always had an interest in entrepreneurship.\nLuminis and its future, how do you think we will evolve?\nI see a bright future for Luminis: the technical sector is growing rapidly and I expect that it will continue to grow in the coming years. Luminis is doing very well, but to keep up you also have to keep asking yourself critical questions and taking on new challenges.\nBesides all that, Luminis is a pleasant workplace. I get along well with my colleagues and we have a great relationship as a team!\n\n\n\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 27852, "title": "Introducing Luminis", "url": "https://www.luminis.eu/blog/introducing-luminis/", "updated_at": "2021-08-10T16:08:59", "body": "I regularly conduct job interviews. Sometimes it only lasts an hour and sometimes it is the beginning of a friendship that continues for many years. At Luminis we leave every meeting in a positive way. I think it is important that you always look back at Luminis with a good feeling, whether it regards a short introduction or years of collaboration.\nOrdinary or not quite?\nThe aim of a first meeting is to get to know each other, have an open conversation, not just firing away way too many questions. When a candidate talks about his life, I pay special attention to the choices someone has made. Of course the \u201cwhat\u201d is important: what did you study? But more importantly is the \u201cwhy\u201d. How did you make the choices you made and what are the things that drive you daily?\nObviously such an introduction goes both ways. I also tell my interlocutor about working at Luminis and explain them why I love working here. I do not think much about the things I talk about during a first meeting, but every time I notice candidates think it\u2019s quite extraordinary. The time we take for colleagues, how we work together, our collective motivation, our passion for craftsmanship, the importance of humour, knowledge sharing and of course our focus to learn new things and develop every day. These are all topics we talk about a lot at Luminis, and that we have come to find normal conversation topics. These introductory meetings have made me realise that talking about such topics is actually not so common.\nWelcome!\nThat is why I started writing down said topics at the end of last year. Intended to introduce interested parties to our culture, and also to introduce colleagues who have just started at Luminis to our culture. When you start working at Luminis there are probably things that will surprise you. This special booklet will help you understand why your colleagues do what they do and say what they say. It helps you feel welcome at our company and dares you to be yourself.\n\nCommitting our culture to writing made me see a pitfall. Implicit manners become explicit by writing them down. In addition, it can be taken as a standard, something that new employees need to meet. I wanted to stay away from that. A corporate culture develops over time, it is not something that should be imposed. The goal instead is to record our experiences so that they can be shared. And of course I hope it will inspire everyone, new colleagues as well as ones who have worked at Luminis for years. There are always opportunities to further develop!\nWorking on the booklet together\nWhat started during my Christmas break with an empty Word document has now become a large stack of booklets, which were delivered to the office last week. In the months in between, many colleagues co-wrote and helped make my texts readable. We hired a cartoonist and a colleague, Laura designed the booklet. Many thanks to all! As far as I am concerned this joint effort was a good example of what our company culture can lead to.\nWould you like to receive the \u201cwelcome to Luminis booklet\u201d?\nWould you like to receive the \u201cwelcome to Luminis booklet? You can! Enter your details here and we will send it to you. Of course we would like to hear your reaction after reading it, because at Luminis we believe we have new things to learn every day.\nGreeting,\nRen\u00e9.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 27527, "title": "What does a Business Consultant do?", "url": "https://www.luminis.eu/blog/news-en/what-does-a-business-consultant-do/", "updated_at": "2021-08-10T16:09:05", "body": "Meet Robin! Robin van Kaathoven is business consultant at Luminis. Nice job title, but what do you actually do? What does being a business consultant at Luminis entail? We would like to give you an idea through an interview with Robin. However, if you end up a Business Consultant at Luminis, your workload could look different. At Luminis, we focus on people, not a pre-determined profile. We think it is important that you fit in with us. Only then will your qualities be able to shine and will you enjoy your work to the fullest.\nBelow you can read the interview with Robin. Do you recognise yourself in the things a Business Consultant does at Luminis? Do not hesitate to contact us!\nRobin, why did you choose Luminis?\nI chose Luminis because I am enthusiastic about innovations in IT, Cloud, and Data. The speed at which new services and digital products are introduced is increasing, as is the variety of revenue models associated with them. Modern software technology opens doors to create value faster, but how do you go about this? What product design do your customers like the most? Which business model fits and what is feasible in terms of software technology? Precisely these questions are at the core of the way Luminis helps customers take the next step.\n\u00a0\n\n\u00a0\nHow do you start with a new customer?\nWhile I also have my own network as a business consultant, I am usually introduced to customers by my commercial colleagues. My task is to gain the trust of clients and position Luminis at board-room level. A business design workshop, for example, is a great tool to establish the relationship between a company\u2019s strategy and the digital products that can contribute to it. After a few follow-up interviews, I usually start in an advisory role, rolling up my sleeves and getting to work. For example, in my last assignment I started as Product Owner. This was a great way to get to know the people and the domain better, and of course to set an example of how such a role can be fulfilled.\nSo as Business Owner you are a Product Owner as well?\nI only fill such roles temporarily. I prefer to work during the phase in which business concepts and project definitions have not yet been fully formed, that\u2019s where my strength lies. My focus is therefore not delivering projects, but transforming companies into agile organisations that are able to translate the possibilities of software technology into business benefits. I realise that I cannot do this alone and I consider myself fortunate to have my Luminis colleagues who can join me in various areas of expertise and who can help fill parts of the path that I envision for the customer. Such as, for example, a business analyst who is much stronger in concretising a concept to system requirements and a project definition.\n\nWhat do you do when the project is on its way?\nWhen we have reached that state, I am often already broadening my network within the customer organisation to see how Luminis can contribute more besides the original assignment. It is my job to strengthen the relationship between customers and Luminis in this way, and not fall into the trap of continuing to work full-time on one project. However, I will continue to monitor the relationship and make adjustments because I can, like no other, establish the relationship between where the customer wants to go and the role Luminis can play in this.\nDo you also have a commercial role as Business Consultant?\nWith regards to the long-term strategy for a customer, I work with my commercial colleague. For large accounts we make a long-term plan where the strategy of the company and their business model play an important role. Of course we share this strategy with said customer and adjust it based on their feedback. These are the moments when a customer notices that we are a true partner, not just a supplier.\nWhat really makes you happy?\nTransforming an organisation is not easy. I often work with people who are less willing or able to contribute to the necessary changes. That is part of my job. For me, digital leadership is also an ambassador\u2019s role, and I know that in addition to drive, patience and soft skills are also very important. I always keep the bigger goal and long term vision in mind. What I really enjoy is when people gradually become more enthusiastic about new technology and start using and applying it themselves.\nDo you also have an internal role at Luminis?\nEvery week is different for me, as I work for various clients at the same time. I am also working internally besides my work for our clients. For example: a brainstorm with colleagues, help with a quotation, to prepare a presentation or have a meeting with all colleagues who work for one of my clients. We then discuss developments and see whether we are on track. I am not only concerned with the development of our customers, but also with that of Luminis.\n", "tags": [], "categories": ["Blog", "News"]}
{"post_id": 27764, "title": "New business models for the installation industry", "url": "https://www.luminis.eu/blog/new-business-models-for-the-installation-industry/", "updated_at": "2021-08-10T16:09:16", "body": "In recent years, I have regularly attended technology fairs. I still remember a booth, where visitors with VR glasses gazed at large gears: products that, as far as I could judge, were no longer distinctive decades ago. I missed the link with the VR application, which seemed to have originated more from a marketing standpoint than from product management scope. Innovations can contribute to brand experience, but the road to adding business value is often very long. The reason for writing this article is that I have in recent years seen a shift that I would like to share with you.\nAt Luminis, we are passionate about digital products that matter: products that are valuable to users and that enable companies to make great strides. We are currently working for a number of customers in the installation industry that are seeing their business grow through digitisation because their innovations meet the real challenges their customers face.\nThe Challenges\nSpecialist knowledge\nThe biggest challenge for the installation industry is knowledge. Attracting well-trained technical personnel is not easy to say the least. As a result, installers need more guidance to properly perform their work at customer locations. Dutch language proficiency is sometimes another factor that increases this challenge. Suppliers often offer support in the form of third-line employees and notice that this increases the pressure on specialist knowledge. In addition, this is not a scalable solution, and there is usually no business model for third-line support, which increases costs. Furthermore, the lower level of available knowledge increases the amount of mistakes that are made, for example when a new installation is taken into use, and reduces the appreciation of end customers. Scenarios like this result in multiple service visits by the installer, which increases costs for the installation company, and makes them less inclined to recommend such an installation in the future.\nEase of operation\nSuppliers are responding to challenges by making the operation of devices simpler and less prone to errors. Often devices are provided with a user interface and software. Although this certainly increases the ease of use, the costs of devices also increase significantly, and it is a challenge to keep software up to date.\nLack of data\nA second challenge in the installation industry is that device suppliers usually do not have the required information about the use of their devices: who use them, where, when and most importantly: how? The installations of devices are often carried out by independent companies and the devices in fact disappear out of reach for the supplier. This is particularly problematic for R&D departments of suppliers, as this data would be invaluable for determining the product roadmap for these devices.\nThe Solutions\nLuminis works with various companies in the installation industry, such as BDR Thermea, Flamco and Bronkorst High-Teach. These companies all recognise the challenges described above. Through our collaboration we have been able to find fitting solutions for these challenges. As is often the case, the implementation of solutions differ for each company. For example, for Bronkhorst we started an online Design Sprint to arrive at a validated prototype for their specific problem. Yet the solutions also have some similarities.\nMobile\nIn all projects for clients in the installation industry, we have developed an app that communicates with the device via Bluetooth or WIFI. The screen of a smartphone or tablet opens up completely new possibilities for user support compared to hardware operation on the device. In addition, the smartphone is of course connected to the internet. This may not be the case when the installer is working, but even in that case the smartphone or tablet can download information at another time to help during installation. This information can be tailor-made. After all, the app communicates with the device and therefore only information is shown that relates to the specific device and the exact situation indicated by the device. By means of language settings, the instruction can be offered in the correct language and provided with images or videos to reduce the risk of errors.\nCloud environment\nAs is becoming clear, it is not just about an app that needs to be developed. A central content server is also necessary, and of course the content (instructions) for the installer must be up to date and structured in a way that allows it to be offered in accordance with the situation. We often see that the introduction of a solution like is the trigger that ensures that content is improved. The content thus grows and is enriched by the experiences gained through usage. This content does not only consist of instructions. We see that this is also used for the distribution of software on the various devices. An app is the perfect means to download and install this software on the device in a safe way.\nOne or two directions?\nThe data flow does not only have to go from a central cloud to the mobile device. If the user consents to this, the device can also send usage data to the cloud. In this way, insight is gained into where devices are located and which malfunctions occurred. This information helps suppliers to further improve devices and help the installer help customers faster and more focused, thereby reducing downtime.\nJifeline\nAnother Luminis customer who has taken these types of solutions one step further is Jifeline. Dealer and garage companies see that vehicles are increasingly equipped with complex software. Reading and installing this requires specialist knowledge. Consider resetting a reversing sensor after installing a towbar. Jifeline supports garages by performing these activities remotely. This means that garages no longer have to invest in acquiring this type of specialist knowledge, while still being able to offer solutions for customers. At Jifeline, this support service is not a service that is added to the delivery of a device, but the service is a business model in itself. A good example of how technological innovation creates value for our customers\u2019 customers.\nIntroducing digital products that really matter starts with user experience design. Only when the end customer immediately recognises the added value will the use of an app or web application increase explosively. The combination of cloud, data and connected devices make these types of solutions possible for many companies. Fortunately, nowadays it is no longer necessary to develop these solutions from scratch yourself. There are more and more semi-finished products that enable companies to experiment with this in an accessible and economically attractive way.\n", "tags": [], "categories": ["Blog", "Cloud", "Strategy &amp; Innovation"]}
{"post_id": 25604, "title": "Gradle: behind the basics", "url": "https://www.luminis.eu/blog/development-en/gradle-insights-for-when-you-want-more-than-defining-dependencies/", "updated_at": "2021-08-10T16:09:29", "body": "If you know the basics of Gradle but want to have a better understanding of how it works and what else is possible keep on reading. In this post I will talk about:\n\nThe Gradle build lifecycle\nGradle plugins\nUse of properties\nKotlin as DSL\n\n\u00a0\nUp until recently I only used Gradle when I needed to add a dependency or some basic plugin. I never really took the time to dig deeper into Gradle. I knew that Gradle had way more to offer than what I was using and recently I had some spare time and decided that this was the moment for me to go past the basics. I took a course on Gradle (https://www.udemy.com/course/gradle-masterclass/) and did some practicing and this blog post is the result of the most important bits that I\u2019ve learned.\n\u00a0\nThe Gradle build lifecycle\n\nBuild phases\n\n\nA Gradle build has three distinct phases.\n\n\n\nInitialization\nGradle supports single and multi-project builds. During the initialization phase, Gradle determines which projects are going to take part in the build, and creates a\u00a0Project\u00a0instance for each of these projects.\nConfiguration\nDuring this phase the project objects are configured. The build scripts of\u00a0all\u00a0projects which are part of the build are executed.\nExecution\nGradle determines the subset of the tasks, created and configured during the configuration phase, to be executed. The subset is determined by the task name arguments passed to the\u00a0gradle\u00a0command and the current directory. Gradle then executes each of the selected tasks.\n\n\n\n\nsource: https://docs.gradle.org/current/userguide/build_lifecycle.html\n\u00a0\nAs explained above the Initialization phase is responsible for determining what projects are going to be used. This is exactly where the settings.gradle file is for, telling Gradle what different projects/modules there are.\n\nBesides the settings.gradle file there\u2019s another file that\u2019s part of the Initialization phase: the init scripts or init.gradle file.\nThe init.gradle file (https://docs.gradle.org/current/userguide/init_scripts.html) allows you to define logic before the Gradle build has actually started. This allows you to for example change the way Gradle logs or add a mandatory company provided repository for dependencies.\nOne of the strange things about the init script is that the location of this file is inside of the .gradle folder which is excluded from git. An alternative would be to provide the script as an argument to Gradle using \u2013init-script init.gradle. However that is bound to go wrong at some point.\nThe most common approach that I\u2019ve seen is creating a custom distribution of Gradle with the init script included. This feels finicky for something that\u00a0 seems a bit of a niche. But apparently, people have had a proper use case for this for example: https://blog.mrhaki.com/2012/10/gradle-goodness-distribute-custom.html\n\u00a0\nAs for the Configuration and Execution phase, both phases make use of build.gradle files of which there can be multiple as opposed to the settings.gradle file of which there can only be one.\nin the Configuration phase Gradle walks through the script to find out what tasks there are. In a way this can be compared to asking people around you at work what their plans are for today, what input they need to get started and who is waiting for their work.\nThen in the Execution phase all of the work is executed in the right order starting with the tasks which have no dependencies on others etc.. until all work is done. As you can imagine if 2 tasks are in need of each other albeit directly or through other tasks, a circular dependency occurs. Gradle detects this part in the configuration phase and then refuses to perform the Execution phase.\nIn order to illustrate what I\u2019ve explained above, I created an example.\n\nHere I\u2019ve created a Gradle task \u2018HelloThere\u2019 which performs some setup on the newly created task such as setting the description and then defines what logic needs to be executed once the \u2018HelloThere\u2019 task is executed in the doLast part of the task (more about the doLast part in the next section).\n\nAs you see above there is a distinction between what is executed in the configuration phase vs the execution phase. This matters because the code in the configuration phase is executed for every task you execute(!). While the code in the doLast is only executed if you execute the specific task.\n\u00a0\nExtending the Gradle lifecycle\u00a0\nIn the example above I created a new task that I had to explicitly start by executing the command: ./gradlew HelloThere. However, ideally your new task is part of some existing lifecycle and started automatically whenever it is ready to perform its job.\nWhen a task is not dependent on any other task you can make the task part of the \u2018defaultTasks\u2019 method.\nHowever most of the time your task is dependent on some other task. In this situation, a better approach would be to make your new task directly depent on the other task.\n\nHere you can see an example where I\u2019ve introduced a new task \u2018goodbye\u2019 which is the defaultTask which means we can start this task by simply executing ./gradlew. However the \u2018goodbye\u2019 task first needs the \u2018HelloThere\u2019 task the be executed.\n\u00a0\nPlugins\nPlugins are basically nothing more than a set of tasks that hook into the existing Gradle tasks (using the same dependsOn logic as above) and/or provide standalone tasks. The java plugin for example (https://docs.gradle.org/current/userguide/java_plugin.html) extends the existing Gradle tasks with logic to compile java code, run tests, build a jar etc..\n\nIn the image above you can see what tasks the java plugins adds and their interdependency on each other\n\u00a0\nNoteworthy plugins\nBesides mandatory plugins to run your project like the Java plugin, there are also utility plugins that help make a developers life easier.\nAn example of this is the scan plugin built by Gradle itself (https://docs.gradle.com/enterprise/gradle-plugin/). It adds the option to scan your Gradle build process for all sorts of metadata such as how long tests took to run but also suggestions about how to improve the speed of you build and way more.\nNow that it is clear that plugins are nothing more than a bundle of new tasks or an extension of existing tasks it\u2019s easy to create your own. I won\u2019t go into the details here but if you want to dive deeper check out https://docs.gradle.org/current/userguide/custom_plugins.html.\n\u00a0\nThe use of properties\nEvery build script has some sensitive information that you don\u2019t want to share. Think of for example credentials for your repository manager, Git or if you\u2019re working with Android, the play store.\nIdeally you automate your entire build process so you don\u2019t have to type in passwords manually or define them hardcoded in your project ever. But at the same time you don\u2019t want to check your sensitive passwords into Git as part of your buildscript or code.\nThis is where the gradle.properties file comes into play. In this file you can define properties in a key=value manner and their contents can be used in your build.gradle and settings.gradle file. You can either create a gradle.properties file in the root of your project (by default part of your versioning system) or in your $GRADLE_USER_HOME. The advantage to the latter is that you credentials which you use over different projects in a single location.\n\nexample gradle.properties file\n\nHere\u2019s an example of how to use a Gradle property combined with the hasProperty() function. The hasProperty() does what the name implies checks if the property is there and is useful to find out why something isn\u2019t working.\n\u00a0\nKotlin as DSL\nInstead of Groovy which can take some time to get used to you can also use Kotlin (https://docs.gradle.org/current/userguide/kotlin_dsl.html)! What\u2019s better than using the same language for your project and build scripts. Besides, Kotlin is statically typed whereas Groovy is not making it easier to create build scripts!\nOne other advantage is that Intellij offers better support through things like autocomplete and fancy colors. Below you can see a side-by-side comparison of the two.\n\n\u00a0\nClosing thoughts\nLearning something is always fun I had fun in this process of doing so. As for how to keep up-to-date with Gradle in the future and continue learning I\u2019ve found the official documentation (https://docs.gradle.org/current/userguide/userguide.html) really useful. It\u2019s clear explicit and has lot\u2019s of examples and if you really don\u2019t know you can always try stackoverflow \ud83d\ude09\n\u00a0\nThanks for reading, I hope I\u2019ve helped you gain some new insights into Gradle and its possibilities!\n", "tags": [], "categories": ["Blog", "Development"]}
{"post_id": 26604, "title": "Java with Lambda does not have to be slow", "url": "https://www.luminis.eu/blog/cloud-en/java-with-lambda-does-not-have-to-be-slow/", "updated_at": "2021-08-10T16:09:42", "body": "As a seasoned Java programmer, usually with Spring Boot, it hurts to learn that most lambdas use JavaScript/TypeScript. What if you want to keep writing your Lambda in Java? A query in google teaches us that Quarkus is the way to go. This blog post introduces Quarkus, GraalVM, and we top it off with AWS CDK to deploy the sample.\nIntroducing Quarkus\nThe goal for Quarkus is to create a framework that makes Java applications ready for the cloud. It uses GraalVM to build Docker images running Java as native applications. The improvements to first response times are impressive, as is the memory consumption. This procedure does limit the libraries that you can use. However, more and more extensions become available. I got inspired to look into Quarkus (again) after reading this very cool blog post about running Lucene as a Lambda.\nThe trick with Quarkus is creating a native image with as little as possible reflection and, as much as possible build metadata processing upfront. That way, less processing is required at the start time. Classes only required during setup can be removed. That way, the image and the used resources are lower. Read more about it on the Quarkus page Container First.\nGenerating the Lambda\nIf there is one thing you have to like about Quarkus besides blazing-fast java applications, it is their documentation and tutorials. The tutorial Quarkus \u2013 Amazon Lambda\u00a0gives you a jump start to create your own Lambda. Some aspects that I like is that you can include multiple Lambdas in one package. Use an environment variable to choose the Lambda to run. The generated code also comes with a few scripts that help you deploy and interact with your Lambda. I don\u2019t use them; I prefer the CDK approach. Still, it is good to know they are available. The following code block shows the maven command to generate our project using the archetype provided by Quarkus.\n\r\nmvn archetype:generate \\\r\n       -DarchetypeGroupId=io.quarkus \\\r\n       -DarchetypeArtifactId=quarkus-amazon-lambda-archetype \\\r\n       -DarchetypeVersion=1.12.2.Final \\\r\n       -DgroupId=eu.luminis.aws \\\r\n       -DartifactId=aws-quarkus-lambda\r\n\nAfter an \u201cmvn clean package\u201d, the target folder is available with the mentioned shell scripts and a function.zip file. This function.zip file contains the optimized code for the Lambda. Later in this blog, we use this zip file in our CDK stack to upload the Lambda. We need to add one property to the maven config. For the native profile, we add the property \u201cquarkus.native.container-build\u201d. I removed the generated Lambdas and created my own SendMessageLambda and HelloLambda. If you have only one Lambda available, this is automatically selected. I like to experiment with two Lambdas in one distribution.\nUsing Java for a Lambda does require a specific interface. AWS does provide this. The Java class that receives the events had to implement the interface \u201cRequestHandler\u201d. You can specify the InputObject and OutputObject yourself. The following code block shows the java code for the Lambda-class.\n\r\n@Named(\"message\")\r\npublic class SendMessageLambda implements RequestHandler<InputObject, OutputObject> {\r\n    @Inject\r\n    ProcessingService service;\r\n\r\n    @Override\r\n    public OutputObject handleRequest(InputObject input, Context context) {\r\n        String process = service.process(input.getGreeting(), input.getName());\r\n        OutputObject out = new OutputObject();\r\n        out.setResult(process);\r\n        out.setRequestId(context.getAwsRequestId());\r\n\r\n        return out;\r\n    }\r\n}\r\n\nNotice the \u201c@Named\u201d annotation; you used this annotation to specify the Lambda currently in use. Use the application.properties file or a system environment parameter. The \u201c@Inject\u201d annotation configures the dependency injection.\nDeploying the Lambda using CDK\nWith the Lambda available as a zip file in the maven target folder, we can use CDK to deploy the Lambda to AWS. Within the project, we create a CDK folder and initialize the CDK project using TypeScript.\n$ cdk init app --language typescript\r\n\nThe configuration of the stack to include the Lambda looks like this.\n\r\nconst quarkusLambda = new lambda.Function(this, \"QuarkusLambda\", {\r\n  runtime: lambda.Runtime.PROVIDED_AL2,\r\n  handler: \"io.quarkus.amazon.lambda.runtime.QuarkusStreamHandler::handleRequest\",\r\n  code: lambda.Code.fromAsset('../target/function.zip'),\r\n});\r\n\nDo not change the handler; Quarkus provide this class. Notice the runtime that we use. As we create a native image, we use the provided runtime.\u00a0Before we run cdk deploy, we first have to build the java project using maven and the native profile.\n$ mvn clean package -Pnative\r\n$ cd cdk\r\n$ cdk deploy\r\n\nUsing the AWS console, we can test the Lambda. Open the Lambda, click on the test tab, and enter the following JSON document as input.\n\r\n{\r\n  \"name\": \"Hello\",\r\n  \"greeting\": \"readers\"\r\n}\r\n\nAfter the invocation, you see the following output.\n\nNotice the init duration (196.37 ms) and the duration (4.69 ms). On a second run, the init duration is gone, and the duration becomes 1 ms.\nUse environment variables to select the right Lambda\nUsing the console, we provide the following environment variable \u201cquarkus_lambda_handler\u201d. Go to the \u201cConfiguration\u201d tab, environment variables and add the one mentioned above. In our case, we give it the value \u201chello\u201d, now the output becomes \u201cHello World!\u201d, no matter the input. As we change the configuration, the Lambda requires a cold restart again.\nOf course, we also want to be able to configure this parameter using CDK. This configuration is straightforward; add an environment block to the lambda specification.\n\r\nconst quarkusMessageLambda = new lambda.Function(this, \"QuarkusMessageLambda\", {\r\n  runtime: lambda.Runtime.PROVIDED_AL2,\r\n  handler: \"io.quarkus.amazon.lambda.runtime.QuarkusStreamHandler::handleRequest\",\r\n  code: lambda.Code.fromAsset('../target/function.zip'),\r\n  environment: {\r\n    quarkus_lambda_handler: \"message\"\r\n  }\r\n});\r\n\nConcluding\nYes, we still can use our java knowledge to create high performing lambdas. Of course, we created a basic Lambda, but more advanced lambdas are possible as well. Quarkus is perfect for making these java Lambdas. I want to look at Quarkus as a modern rest endpoint combined with the AWS API Gateway in the next blog. So stay tuned for more Quarkus.\nReferences\n\nhttps://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/\nhttps://quarkus.io\nhttps://quarkus.io/guides/amazon-lambda\nhttps://github.com/luminis-ams/aws-cdk-examples\n\n\u00a0\n", "tags": ["cdk", "cloud", "java", "Quarkus"], "categories": ["Blog", "Cloud"]}
{"post_id": 25679, "title": "From legacy to the cloud: how modern software development speeds you up", "url": "https://www.luminis.eu/blog/naar-moderne-softwareontwikkeling/", "updated_at": "2022-02-04T11:47:11", "body": "These days, software is indispensable for most organizations. When talking to our customers, we often hear that although their existing software is well-tested and has been running fine for years, their system has reached its maximum lifespan. Moreover, a large chunk of their IT budgets is needed to keep existing systems up and running. Now, modernizing a running system is a complex process. At a certain point, it is more efficient to switch to a new generation of technology, and to slowly migrate legacy software. However, what does \u201cat a certain point\u201d mean for your organization? In this article, I\u2019ll show you when to switch to a new generation of technology, and how to reduce risks during a migration.\nModern software: what\u2019s in it for me?\nLet\u2019s start by understanding when a software system is outdated. This obsolescence is partly technical: software grows more complex over time by definition. Over time, it becomes too expensive to adapt software. Sometimes software becomes so complex that trying to understand it causes headaches. What we also notice is that in the rapidly changing digital world, there are fewer and fewer people on the market who still master older technology. Technical amortisation is not exclusive to the digital world, of course, but if you\u2019re not careful, software will be outdated quickly.\nAging can also be economic: new services are coming onto the market that you can no longer compete with. The world never stops turning; people expect new generations of digital technology that work well on mobile phones, or that just respond to their voice commands. The cloud accelerates innovation incredibly fast, which means that customers expect software to be always accessible, to work smoothly and to be fast even during peak traffic. Cloud technology Cloud vendors now have a proven track record and can guarantee the desired degree of availability.\u00a0\nWouldn\u2019t it be nice if your software could be easily adapted? If you can quickly develop an idea and implement a new feature? If said new feature can be immediately distributed to your customers to validate the new idea? With the evolution of the cloud, we now look very differently at software development than we did with technology of previous generations. Using cloud technology requires a smaller capital investment, as well. If it turns out that there\u2019s a better idea, your organization will have learned this with a small investment.\nHow does this work? Faster innovation with cloud technology is made possible by a lower capital investment, where you only pay for what you actually use. Developers do not have to build their own components, but can reuse proven, safe and efficient components other developers have made. Think of technology that you had to acquire knowledge for until not so long ago, such as load balancers, web servers and DNS changes. In addition, with cloud technology you can deploy updates to software several times a day and quickly roll back if and when problems arise. You no longer have to design a complex solution far in advance, so you can start validation quickly.\nFinally, cloud services make life easier for developers in your organization. Cloud vendors supply tools for every task you can think of. Example: with AWS, if you build apps for the Apple ecosystem, you can compile software in the cloud. Traditionally, developers had to buy Apple computers and build their software on those. Having computers around like this introduces headaches of its own. Think people tripping over power cords. To solve this, AWS has racked up actual Apple Minis, so developers no longer have these problems. Sometimes, developers spend an extra hour to get something to run by themselves. I know I\u2019m guilty. Developer time is precious, though, and might be better spent elsewhere.\nHow do you notice that your organization is at risk?\nThere are several signs that indicate you need to update software on the short term. For example, software sometimes only works on a company network. By now, people expect that they can access the their stuff at any time, on their own mobiles, without messing around with VPNs. Store confidential information securely in the cloud, so it can be distributed. In addition, privacy legislation made giant strides by now. In legacy systems, the disposal of data is not always thorough enough by current standards. Furthermore, we increasingly see organizations making their own system choices outside the all-powerful IT department. People want to buy their software tools directly.\nKeeping existing systems running is expensive, not only due to the growing complexity of this changing world, but also due to the scarcity of knowledge. It is therefore important to find out what the costs and benefits of the old system are. Finally, there are other reasons to switch, for example the end of support from a supplier or expiring maintenance contracts.\nFrom legacy to modern software: start small, start now\nWe believe that the best way to rebuild software is with an incremental way of working. To reduce the impact on the current business of the organization, we start small and gradually transfer parts of the technology landscape. After all, the more experience about cloud within the organization, the more projects can be started that help you benefit from cloud technology.\nIn the meantime, keep the legacy system running. This often means that data is stored in two places for a while, so that business-critical data is not lost. Existing systems are more often monolithic and, before things can be migrated, need changes to make chunks of data available. Define in your organization which data is leading: that of the new cloud systems or of the legacy systems. The larger the portion that resides in the cloud, the easier it will be to start new experiments. This way your organization can start doing things that were previously possible for tech giants only.\nWhen building software you have to choose what you want to be good at. You can choose to build everything yourself, including maintaining your own servers, for example. However, your organization is not going to make the difference with having their own servers; you would be better of just renting rackspace. Not so long ago, organizations had to manage their own server fleet, know everything about the management of operating systems and have all kinds of network knowledge, before they could only start building useful things. Only then, people could \u00a0see your organization. The cloud vendor does that work for you now, so you can focus on your strengths.\n\nNow your organization can focus on writing the application and lean on the cloud vendor for more technical matters. This requires less knowledge within the organization, which allows your development group to focus on what makes your organization unique.\u00a0\nHow do we do this?\nThere is a range of flavours between running in the cloud completely and maintaining a system on your own metal. Wherever you currently are on this spectrum, it is always important to reduce risks with a gradual transition. The first step of the transition is to determine the migration strategy. The first strategy is just keeping the existing technology; this way there is little risk, but also little reward. The second strategy, moving existing technology to the cloud, brings more rewards. Determine which parts of your technology landscape can be moved and which parts can be phased out. Finally, the strategy that can have the most impact is to rebuild using managed cloud services. Read more about this spectrum of strategies in the whitepaper by Piet van Dongen, our colleague who has the remarkable talent to give serious subjects such as cloud migrations a funny twist.\nOnce you have taken the first steps in the cloud, it is important to get to know the hundreds of managed cloud services better. There is such a wide range of services that finding the right service is a competence in itself. This is precisely where Luminis can offer a helping hand: as a partner of the major cloud providers, we can find the right services and proactively propose them if we spot possible improvements for the organization. Bert Ertman tells more about how these cloud services work in his blogpost. In this day and age where building and maintaining software happen at the same time, we stay in the loop after building.\u00a0\nWithout Luminis to help guide you, it is difficult to know what you can do with a cloud provider like AWS. AWS is the market leader because of their reliability and enormous range of product, but not because their cloud services are easy to grasp. Their cloud services have rather mysterious names and are introduced at such a pace that it is a day job to keep up with new developments. For example, say you have a lot of location data. How do you choose between AWS Kinesis, Timestream or just using Lambdas? In addition, the cloud bill is the only place where your organization gains complete insight into the costs, which can only be viewed after using services. We analyze which services entail excessive costs beforehand and, for example, migrate data to a different type of storage or use a different type of database if this can help save costs. This way we help your organization regains control of cloud costs. Finally, we analyse which usage we can predict: smart purchasing makes smart deals possible and the costs of the IT landscape will go down even further. Yet the ultimate goal is not to think cost-driven, but to be agile through technology. This makes IT fun again and we can make people happy with robust systems that make it possible to do what was previously impossible.\n", "tags": [], "categories": ["Blog", "Strategy &amp; Innovation"]}
