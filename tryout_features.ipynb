{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to this notebook explaining the functionality of the [SearchArray](https://github.com/softwaredoug/searcharray) libray from [Doug Turnbull](https://softwaredoug.com). The goal of *SearchArray* is to bring Lexical search to the Pythonic way of working. It is a library that extends Python Pandas with an inverted index, just like Lucene would. No installation of tools like Elasticsearch, OpenSearch or Solr. Just Pandas and other data related tools that just work for Python. \n",
    "\n",
    "For the demo, I'll use the content from the [Luminis blog](https://www.luminis.eu/blog/), available as a multiline json file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58490284255346f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data into a dataframe\n",
    "The data contains a 100 blogs taken from Luminis. The result is a jsonl document with a complete json document per line. You can choose between the few_documents and all_documents files. The few_documents file is a subset of the all_documents file. The all_documents file contains 100 documents. The few_documents file contains 6 documents."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d7151d89032a409"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "file_path = 'data/all_documents.jsonl'\n",
    "\n",
    "# Read the JSON file into a pandas DataFrame\n",
    "blogs = pd.read_json(file_path, orient='records', lines=True)\n",
    "blogs.head()"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing SearchArray   ",
   "id": "ae33b61c39ea6424"
  },
  {
   "cell_type": "code",
   "source": [
    "from searcharray import SearchArray"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a31ebb22ef40630a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Tokenizer\n",
    "The tokenizer is a function that takes a string and returns a list of tokens. The tokens are the words that are used to build the index. The tokenizer is used to split the text into words and remove punctuation. The default tokenizer is a simple whitespace tokenizer. Test the tokenizer using a few sample texts.\n",
    "\n",
    "```python\n",
    "print(ws_punc_tokenizer('Hello, World!'))\n",
    "```"
   ],
   "id": "ce6d2bc0cceb6232"
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def ws_punc_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text by splitting on whitespace and removing punctuation.\n",
    "    :param text: String to tokenize.\n",
    "    :return: Array of tokens.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'(\\w)-(\\w)', r'\\1 \\2', text.lower())\n",
    "    split = text.split()\n",
    "    return [token.translate(str.maketrans('', '', string.punctuation.replace('-', '')))\n",
    "            for token in split]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a35fc2509afebfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(ws_punc_tokenizer('Hello, World!'))",
   "id": "aebb563e0ff0a0df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Write the index with the tokens\n",
    "Next we can use the tokenizer to add a special column to the pandas dataframe. This column will contain the tokens of the title. This step also creates the index for the searcharray."
   ],
   "id": "3de221412a97c945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "title_index = SearchArray.index(blogs['title'], tokenizer=ws_punc_tokenizer)\n",
    "blogs['title_index'] = title_index"
   ],
   "id": "778671279f3250d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\"\"The type of the response: {type(title_index)}\n",
    "\n",
    "The result is a column or a list type, shape is the row count: {title_index.shape}\n",
    "\n",
    "The dictionary translates the available terms into numbers.\n",
    "{title_index.term_dict}\n",
    "\n",
    "Sentences translated into arrays of numbers using the dictionary.\n",
    "{title_index.term_mat}\"\"\")\n"
   ],
   "id": "9dbaebea4814ae96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the method _index_ from the SearchArray class creates everything you need to exeute lexical search on your content. Comparing a query to a lot of documents works best with numbers. It requires less memory and is faster. The _index_ method creates a dictionary that translates the terms into numbers. The _term_mat_ is a matrix with the documents as rows and the terms as columns. The values are the term numbers taken from the dictionary.",
   "id": "b012d0942c141082"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Search the index\n",
    "The index is now ready to be used for searching. The search method returns a boolean array with the same length as the original data. The boolean array indicates if the document contains the search term. The search method uses the same tokenizer as the index."
   ],
   "id": "ab9875da2d478ab6"
  },
  {
   "cell_type": "code",
   "source": "title_index.match(ws_punc_tokenizer('quarkus'))",
   "metadata": {
    "collapsed": false
   },
   "id": "54e08f2fbef501e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What just happened here? How did SearchArray determine if a document is a match to our query?\n",
    "1. Obtain the number representing the token 'Quarkus' from the dictionary. To make lookups faster, the dictionary is a two-way dictionary. It can translate the token to a number and the number to a token.\n",
    "2. Find those documents that have a higher than zero term positions for the specific term.\n",
    "\n",
    "In the next code-block we have a closer look at those term positions. This is the part where Doug created something fast using roaring bitmaps."
   ],
   "id": "bee4c37520e5d17a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def token_frequencies(token: str):\n",
    "    \"\"\"\n",
    "    Prints a summary of important information about the token and its frequency of occurrence in the documents.\n",
    "    :param token: The token to find positions for.\n",
    "    \"\"\"\n",
    "    term_id = title_index.term_dict.get_term_id(token)    \n",
    "    doc_ids, term_frequencies = title_index.posns.termfreqs(term_id)\n",
    "\n",
    "    print(f\"\"\"\n",
    "Type of the object storing the positions of the terms in docs '{type(title_index.posns).__name__}'\n",
    "The number of a single token 'openai': {term_id}\n",
    "For fun, convert the id back to the token: {title_index.term_dict.get_term(term_id)}\n",
    "\n",
    "The document ids where the token is found: \"\"\")\n",
    "    for i in range(len(doc_ids)):\n",
    "        print(f\"Document id: {doc_ids[i]} - Term frequency: {term_frequencies[i]}\")\n"
   ],
   "id": "3c37638e7c0ca0e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "token_frequencies('quarkus')",
   "id": "8c4bfe7a710424c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The method above makes use of the method _termfreqs_ of the object _PosnBitArray_. This is the focus for another section. ",
   "id": "2f4ec225175b46da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Search for multiple terms\n",
    "The search method can also be used to search for multiple terms. The default behavior is to search for documents that contain all the terms. The search method returns a boolean array with the same length as the original data. The boolean array indicates if the document contains all the search terms. The search method uses the same tokenizer as the index."
   ],
   "id": "56a0b29ce230e7da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "title_index.match(ws_punc_tokenizer('debug Quarkus'))",
   "id": "cfd0ee24046006d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we change the order of the terms, the result will be different. The order of the terms is used for matching. The only method to support that is to keep the positions of the terms in a document. First the proof that the reverse order of tokens does not result in a match.",
   "id": "f194f0916d4cceec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "title_index.match(ws_punc_tokenizer('Quarkus debug'))",
   "id": "be7ce58b4405f5db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scoring the search results\n",
    "Next, calculate a score using BM25. With this score we can rank the documents based on the relevance of the search terms. The BM25 score is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. It is based on the probabilistic information retrieval model. The BM25 score is the sum of the scores of each term in the query. The score of a term is calculated using the following formula:"
   ],
   "id": "1b6f9c5a9ebd80c1"
  },
  {
   "cell_type": "code",
   "source": [
    "bm25 = blogs['title_index'].array.score(ws_punc_tokenizer('quarkus'))\n",
    "bm25"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3f23c046ba5394d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "blogs['bm25'] = bm25\n",
    "blogs[['title', 'bm25']].sort_values('bm25', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a94355df59c961a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The BM25 score is influenced by the length of the document (or field) compared to the average length of the document. The impact of this difference is controlled with the *b* parameter. The *k1* parameter controls the impact of the term frequency on the score. The default values are *k1=1.5* and *b=0.75*.",
   "id": "7bb42b4fdb4915f4"
  },
  {
   "cell_type": "code",
   "source": [
    "from searcharray.similarity import bm25_legacy_similarity\n",
    "\n",
    "custom_bm25 = bm25_legacy_similarity(k1=1, b=0.1)\n",
    "blogs['custom_bm25'] = blogs['title_index'].array.score(ws_punc_tokenizer('quarkus'), similarity=custom_bm25)\n",
    "blogs[['title', 'bm25', 'custom_bm25']].sort_values('custom_bm25', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "909cd07f588676a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Filter rows using the tags\n",
    "Next we use the tags field to filter on those blogs that are tagged with *java*. We start showing the tags for the top matching rows from the previous query."
   ],
   "id": "9d425d8af39e452d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "blogs[['title', 'bm25', 'tags']].sort_values('bm25', ascending=False)",
   "id": "fd2aaf1a04cd1cbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter rows where 'tags' field contains 'java'\n",
    "java_blogs = blogs[blogs['tags'].apply(lambda tags: 'java' in tags)]\n",
    "java_blogs[['title', 'bm25', 'tags']].sort_values('bm25', ascending=False)"
   ],
   "id": "6ee42877eb7535fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
